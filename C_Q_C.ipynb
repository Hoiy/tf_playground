{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline\n",
    "import time\n",
    "from DataLoader import GloVe\n",
    "from TextPreprocess.sequences import Sequences\n",
    "from TextPreprocess.Tokenizer.RegExp import tokenize\n",
    "import Utils.pandas_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "WORD_DIM = 300\n",
    "WORD_COUNT = 400000+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Loading Glove Model\n",
      "End: Loaded 400000 rows.\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe.load2('./data/GloVe/glove.6B.{}d.txt'.format(WORD_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: 300\n",
      "Embedding Symbols: 400003\n",
      "Index to symbol: [(0, 'matakana'), (1, 'tortona'), (2, '223.2'), (3, 'txi'), (4, 'ferreres'), (5, 'sfpd'), (6, 'jomhuri'), (7, 'state-supported'), (8, 'yam'), (9, 'reconciliation')]\n"
     ]
    }
   ],
   "source": [
    "# emb: Symbol to float32 of fixed DIMENSION\n",
    "# Create an index mapping, index to symbol, symbol to index\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, emb, verbose = False):\n",
    "        # assert emb is dictionary and each entry has same dimension\n",
    "        self.emb = emb\n",
    "        self.dim = len(self.emb[list(self.emb.keys())[0]])\n",
    "        self.emb['<UNK>'] = [0. for i in range(self.dim)]\n",
    "        self.emb['<PAD>'] = [1. for i in range(self.dim)]\n",
    "        self.emb['<GO>'] = [-1. for i in range(self.dim)]\n",
    "        \n",
    "        self.build_dicts()\n",
    "        \n",
    "        if verbose:\n",
    "            self.describe()\n",
    "        \n",
    "    def describe(self):\n",
    "        print('Embedding Dimension: {}'.format(self.dim))\n",
    "        print('Embedding Symbols: {}'.format(len(self.emb)))\n",
    "        print('Index to symbol: {}'.format([(i, self.idx2Sym[i]) for i in range(10)]))\n",
    "        \n",
    "    def getIndex(self, symbol):\n",
    "        if symbol in self.sym2Idx:\n",
    "            return self.sym2Idx[symbol]\n",
    "        else:\n",
    "            return self.sym2Idx['<UNK>']\n",
    "\n",
    "    def getEmb(self, symbol):\n",
    "        return self.emb[self.idx2Sym[self.getIndex(symbol)]]\n",
    "    \n",
    "    def getSymbols(self, indices):\n",
    "        return [self.idx2Sym[idx] for idx in indices]\n",
    "\n",
    "    def getNumpyArray(self):\n",
    "        return np.array([self.emb[self.idx2Sym[idx]] for idx in range(len(self.emb))])\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        self.sym2Idx = {}\n",
    "        index = 0\n",
    "        for key in self.emb.keys():\n",
    "            self.sym2Idx[key] = index\n",
    "            index += 1\n",
    "            \n",
    "        self.idx2Sym = { v:k for k, v in self.sym2Idx.items()}\n",
    "\n",
    "glove_emb = Embedding(glove, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = file.read('data/Quora/train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.question1 = df.question1.astype(str)\n",
    "df.question2 = df.question2.astype(str)\n",
    "df.is_duplicate = df.is_duplicate.astype(float)\n",
    "\n",
    "df = df.as_matrix(['question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "data = {}\n",
    "data['train'], data['test'] = train_test_split(df, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessQuestion(string):\n",
    "    try:\n",
    "        return [glove_emb.getIndex(token.lower()) for token in tokenize(string)]\n",
    "    except:\n",
    "        print(string)\n",
    "\n",
    "\n",
    "def preprocessData(data):\n",
    "    return [[preprocessQuestion(rec[0]), preprocessQuestion(rec[1]), float(rec[2])] for rec in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ['train', 'test']:\n",
    "    data[i] = preprocessData(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Turns iteratable of symbols into padded batch\n",
    "from functools import lru_cache\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.verbose = verbose\n",
    "        self.size = len(self.seqs)\n",
    "        self.seq_lens = [len(seq) for seq in self.seqs]\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.describe()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def max_length(self):\n",
    "        return max(self.seq_lens)\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def longgest_sequence(self):\n",
    "        for seq in self.seqs:\n",
    "            if len(seq) == self.max_length():\n",
    "                return seq\n",
    "    \n",
    "    def describe(self):\n",
    "        print('Size: {}'.format(self.size))\n",
    "        print(\"Longest sequence length: {}\".format(self.max_length()))\n",
    "        bin_width = max(1, self.max_length() // 30)\n",
    "        plt.hist(self.seq_lens, range(0, self.max_length() + bin_width, bin_width))\n",
    "        plt.title('Sequence length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def batchPadding(self, batch, padding_symbol):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), padding_symbol)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = batch[i]\n",
    "        return result\n",
    "\n",
    "    def batchMask(self, batch):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), 0.0)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = 1.0\n",
    "        return result\n",
    "        \n",
    "    # Same length within the batch, stuffed with padding symbol\n",
    "    def generator(self, padding_symbol, batch_size=None, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        length = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for seq in self.seqs:\n",
    "                train.append([sym for sym in seq])\n",
    "                length.append(len(seq))\n",
    "                if(len(train) == batch_size):\n",
    "                    yield self.batchPadding(train, padding_symbol), length, self.batchMask(train)\n",
    "                    train = []\n",
    "                    length = []\n",
    "            epouch -= 1\n",
    "            if self.verbose:\n",
    "                print('epouch done...')\n",
    "                \n",
    "class Batcher2:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.size = len(self.seqs)\n",
    "\n",
    "    def generator(self, batch_size=32, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for sym in self.seqs:\n",
    "                train.append([sym])\n",
    "                if(len(train) == batch_size):\n",
    "                    yield train\n",
    "                    train = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')\n",
    "            \n",
    "            \n",
    "# Turn data into batch, where data is iterable over records, record is iterable over fields\n",
    "class Batcher3:\n",
    "    def __init__(self, data):\n",
    "        #assert it is doubly iterable\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        \n",
    "    def generator(self, batch_size = 32, epouch = -1):\n",
    "        batch = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for record in self.data:\n",
    "                batch.append(record)\n",
    "                if(len(batch) == batch_size):\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batcher = {}\n",
    "for i in ['train', 'test']:\n",
    "    batcher[i] = Batcher3(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LV1_DIM = 10\n",
    "LV2_STEP = 1\n",
    "LV2_DIM = 150\n",
    "\n",
    "DROPOUT_RNN = 0.1\n",
    "DROP_DENSE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embeddings_initializer(shape):\n",
    "    with tf.variable_scope(\"Embeddings_Initializer\"):\n",
    "        in_emb = tf.placeholder(\n",
    "            dtype = tf.float32, \n",
    "            shape = shape, \n",
    "            name = \"Placeholder\"\n",
    "        )\n",
    "        \n",
    "        emb = tf.Variable(\n",
    "            tf.constant(0.0, shape = shape), \n",
    "            trainable=False, \n",
    "            name = 'Embeddings', \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        init_emb = emb.assign(in_emb)\n",
    "    return in_emb, init_emb, emb\n",
    "\n",
    "def cells_initializer(num_units, reuse):\n",
    "    with tf.variable_scope(\"Cells_Initializer\"):\n",
    "        cells = tf.contrib.rnn.GRUCell(\n",
    "            num_units = num_units,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "    return cells\n",
    "\n",
    "\n",
    "#IN (batch, time, 1)\n",
    "def simple_embedding(inputs, embeddings):\n",
    "    with tf.variable_scope(\"Simple_Embedding\"):\n",
    "        lookup = tf.nn.embedding_lookup(\n",
    "            params = embeddings,\n",
    "            ids = inputs,\n",
    "            partition_strategy='mod',\n",
    "            name='Embedding_Lookup',\n",
    "            validate_indices=True,\n",
    "            max_norm=1\n",
    "        )\n",
    "\n",
    "    return lookup\n",
    "\n",
    "#OUT: (batch, time, dim) float32\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_dynamic_rnn(cell, inputs, lengths):\n",
    "    with tf.variable_scope(\"Simple_Dynamic_RNN\"):        \n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        step_size = tf.shape(inputs)[1]\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell, \n",
    "            inputs, \n",
    "            dtype = tf.float32, \n",
    "            sequence_length = lengths,\n",
    "            initial_state = cell.zero_state(batch_size)\n",
    "        )\n",
    "\n",
    "        indices = tf.range(0, batch_size) * step_size + (lengths - 1)\n",
    "        gather = tf.reshape(\n",
    "            tf.gather(\n",
    "                tf.reshape(outputs, [-1, cell.output_size]), indices\n",
    "            ), \n",
    "            [-1, cell.output_size]\n",
    "         )\n",
    "        \n",
    "    return gather\n",
    "#OUT (batch, dim)\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_encoder(inputs, input_lengths, embeddings, dropout = 0.0, reuse = None):\n",
    "    with tf.variable_scope('Simple_Encoder'):\n",
    "        \n",
    "        emb = simple_embedding(inputs, embeddings)\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell,\n",
    "            input_keep_prob = 1. - dropout,\n",
    "            output_keep_prob = 1. - dropout,\n",
    "            state_keep_prob = 1. - dropout,\n",
    "            variational_recurrent=False,\n",
    "            input_size=None,\n",
    "            dtype=None,\n",
    "            seed=None\n",
    "        )\n",
    "\n",
    "        rnn = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = emb,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn\n",
    "            \n",
    "        #\n",
    "        # Conv layer does not support dynamic length ;/\n",
    "        #\n",
    "    \"\"\"\n",
    "        filter_2 = tf.Variable(\n",
    "            tf.random_uniform([2, WORD_DIM, LV1_DIM], -1, 1), \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        #IN (batch, time, dim)\n",
    "        conv_2 = tf.nn.conv1d(\n",
    "            value = inputs,\n",
    "            filters = filter_2,\n",
    "            stride = 1,\n",
    "            padding = 'VALID',\n",
    "            use_cudnn_on_gpu=True,\n",
    "            data_format=None,\n",
    "            name='Conv_Witdh_2'\n",
    "        )\n",
    "        #OUT (batch, time-1, dim)\n",
    "\n",
    "    with tf.variable_scope('Level_2_RNN'):\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size=None,\n",
    "            activation=tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        rnn_output_2 = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = inputs,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn_output_2\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#OUT (batch, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zero_state() missing 1 required positional argument: 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d18f516ec192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder_Layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0menc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_word_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0menc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_word_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-abc02a2ce8bc>\u001b[0m in \u001b[0;36msimple_encoder\u001b[0;34m(inputs, input_lengths, embeddings, dropout, reuse)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-abc02a2ce8bc>\u001b[0m in \u001b[0;36msimple_dynamic_rnn\u001b[0;34m(cell, inputs, lengths)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         )\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zero_state() missing 1 required positional argument: 'dtype'"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.variable_scope(\"Inputs_Layer\"):\n",
    "    \n",
    "        #IN\n",
    "        in_word_indices = [tf.placeholder(tf.int32, (None, None), name = \"Q{}_Word_Indices\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        #batch_size = [tf.shape(inputs[i], name= \"Q{}_Batch_Size\".format(i+1))[0] for i in range(2)]\n",
    "        #steps = [tf.shape(inputs[i], name= \"Q{}_Steps\".format(i+1))[1] for i in range(2)]\n",
    "        \n",
    "        #IN\n",
    "        in_lengths = [tf.placeholder(tf.int32, (None), name = \"Q{}_Lengths\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch) int32\n",
    "        \n",
    "        #in_word_indices = tf.placeholder(tf.int32, (None, None), name = \"Q_Word_Indices\")\n",
    "        \n",
    "        #in_lengths = tf.placeholder(tf.int32, (None), name = \"Q_Lengths\")\n",
    "        \n",
    "        in_truth = tf.placeholder(tf.float32, (None, 1), name = \"Truth\")\n",
    "\n",
    "    with tf.variable_scope(\"Embeddings_Layer\"):\n",
    "        in_emb, init_emb, emb = embeddings_initializer((WORD_COUNT, WORD_DIM))\n",
    "        \n",
    "    with tf.variable_scope(\"Encoder_Layer\"):\n",
    "        enc_0 = simple_encoder(in_word_indices[0], in_lengths[0], emb, dropout=DROPOUT_RNN)\n",
    "        enc_1 = simple_encoder(in_word_indices[1], in_lengths[1], emb, dropout=DROPOUT_RNN, reuse=True)\n",
    "\n",
    "    #IN: (batch, dim) x2\n",
    "    with tf.variable_scope(\"Prediction_Layer\"):\n",
    "        #pred_in = [tf.placeholder(tf.float32, (None, LV2_DIM)) for i in range(2)]\n",
    "        #pred_input = tf.concat([pred_in[0], pred_in[1]], 1)\n",
    "        pred_input = tf.concat([enc_0, enc_1], 1)\n",
    "        \n",
    "        pred_weights = tf.Variable(tf.random_uniform([LV2_DIM * 2, 1], -1, 1), name='weights')\n",
    "        pred_bias = tf.Variable(tf.constant(0.0, shape=[1]), name=\"bias\")\n",
    "        \n",
    "        pred = tf.nn.sigmoid(pred_input @ pred_weights + pred_bias)\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(in_truth, pred))\n",
    "    acc = tf.reduce_mean(tf.contrib.keras.metrics.binary_accuracy(in_truth, pred))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.7899467542476092&quot;).pbtxt = 'node {\\n  name: &quot;Inputs_Layer/Q1_Word_Indices&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Inputs_Layer/Q2_Word_Indices&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Inputs_Layer/Q1_Lengths&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Inputs_Layer/Q2_Lengths&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Inputs_Layer/Truth&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 400003\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 400003\\n          }\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 400003\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Embeddings_Layer/Embeddings_Initializer/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Placeholder&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;Embeddings_Layer/Embeddings_Initializer/Embeddings/read&quot;\\n  input: &quot;Inputs_Layer/Q1_Word_Indices&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Rsqrt&quot;\\n  op: &quot;Rsqrt&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/truediv/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Const&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/truediv/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Rsqrt&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_1&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/Minimum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Embeddings_Layer/Embeddings_Initializer/Embeddings&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/DropoutWrapperInit/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.8999999761581421\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/DropoutWrapperInit/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.8999999761581421\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/DropoutWrapperInit/Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.8999999761581421\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/Shape&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack_1&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Embedding/Embedding_Lookup/clip_by_norm&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/Shape_1&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack_1&quot;\\n  input: &quot;Encoder_Layer/Simple_Encoder/Simple_Dynamic_RNN/strided_slice_1/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.7899467542476092&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tvars_vals = sess.run(tvars)\n",
    "\n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name, val)  # Prints the name of the variable alongside its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 2000000\n",
    "MODEL = './model/q.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def batchPadding(batch, padding_symbol):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result\n",
    "\n",
    "def proc_batch(batch):\n",
    "    q1 = batchPadding([record[0] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q2 = batchPadding([record[1] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q1_lens = [len(record[0]) for record in batch]\n",
    "    q2_lens = [len(record[1]) for record in batch]\n",
    "    truth = [[record[2]] for record in batch]\n",
    "    \n",
    "    return q1, q2, q1_lens, q2_lens, truth\n",
    "\n",
    "gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    gen[i] = batcher[i].generator(batch_size=BATCH_SIZE, epouch = 1 if i == 'test' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 1000\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    try:\n",
    "        saver.restore(session, MODEL)\n",
    "        print('Restored training...')\n",
    "        #session.run(tf.global_variables_initializer())\n",
    "        #print('Restarting training...')\n",
    "    except:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "        \n",
    "    session.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    #run_metadata = tf.RunMetadata()\n",
    "    \n",
    "    #tvars_vals = session.run(tvars)\n",
    "    #for var, val in zip(tvars, tvars_vals):\n",
    "    #    print(var.name, val)  # Prints the name of the variable alongside its value.\n",
    "\n",
    "    #for name in session.run( tf.report_uninitialized_variables( tf.global_variables( ) ) ):\n",
    "    #    print(name)\n",
    "    \n",
    "    \n",
    "    average_loss = 0\n",
    "    average_max_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        train_q1, train_q2, train_q1_lengths, train_q2_lengths, train_label = proc_batch(next(gen['train']))\n",
    "        \n",
    "        feed_dict = {\n",
    "            in_word_indices[0]: train_q1,\n",
    "            in_word_indices[1]: train_q2,\n",
    "            in_lengths[0]: train_q1_lengths,\n",
    "            in_lengths[1]: train_q2_lengths,\n",
    "            in_truth: train_label\n",
    "        }\n",
    "        \n",
    "        #_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "                avg_loss_val = 0\n",
    "                avg_acc_val = 0\n",
    "                \n",
    "                gen['test'] = Batcher3(data['test']).generator(epouch = 1, batch_size=1024)\n",
    "                count = 0\n",
    "                for batch in gen['test']:\n",
    "                    test_q1, test_q2, test_q1_lengths, test_q2_lengths, test_label = proc_batch(batch)\n",
    "                    feed_dict = {\n",
    "                        in_word_indices[0]: test_q1,\n",
    "                        in_word_indices[1]: test_q2,\n",
    "                        in_lengths[0]: test_q1_lengths,\n",
    "                        in_lengths[1]: test_q2_lengths,\n",
    "                        in_truth: test_label\n",
    "                    }\n",
    "\n",
    "                    loss_val, acc_val = session.run([loss, acc], feed_dict=feed_dict)\n",
    "                    avg_loss_val += loss_val\n",
    "                    avg_acc_val += acc_val\n",
    "                    count += 1\n",
    "                \n",
    "                print('Testing Set batch loss: {0}, acc {1}:'.format(avg_loss_val/count, avg_acc_val/count))\n",
    "                \n",
    "                start = time.time()\n",
    "                \n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            # Create the Timeline object, and write it to a json\n",
    "            #tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            #ctf = tl.generate_chrome_trace_format()\n",
    "            #with open('timeline.json', 'w') as f:\n",
    "            #    f.write(ctf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 100\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    print('Restored model...')\n",
    "    \n",
    "    average_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "        test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "        test_label = next(label_gen['test'])\n",
    "        \n",
    "        feed_dict = {\n",
    "            inputs[0]: test_q1,\n",
    "            inputs[1]: test_q2,\n",
    "            input_lengths[0]: test_q1_lengths,\n",
    "            input_lengths[1]: test_q2_lengths,\n",
    "            truth: test_label\n",
    "        }\n",
    "        \n",
    "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "q1_gen = {}\n",
    "q2_gen = {}\n",
    "label_gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    q1_gen[i] = q1_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    q2_gen[i] = q2_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    label_gen[i] = label_batcher[i].generator(batch_size=BATCH_SIZE)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "    test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "    test_label = next(label_gen['test'])\n",
    "        \n",
    "    feed_dict = {\n",
    "        in_word_indices[0]: test_q1,\n",
    "        in_word_indices[1]: test_q2,\n",
    "        in_lengths[0]: test_q1_lengths,\n",
    "        in_lengths[1]: test_q2_lengths,\n",
    "        in_truth: test_label\n",
    "    }\n",
    "    \n",
    "    e0_val, e1_val = sess.run([e0, e1], feed_dict=feed_dict)\n",
    "    print(e0_val)\n",
    "    print(e1_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
