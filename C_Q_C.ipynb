{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline\n",
    "import time\n",
    "from DataLoader import GloVe\n",
    "from TextPreprocess.sequences import Sequences\n",
    "from TextPreprocess.Tokenizer.RegExp import tokenize\n",
    "import Utils.pandas_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "WORD_DIM = 300\n",
    "WORD_COUNT = 400000+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe.load2('./data/GloVe/glove.6B.{}d.txt'.format(WORD_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# emb: Symbol to float32 of fixed DIMENSION\n",
    "# Create an index mapping, index to symbol, symbol to index\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, emb, verbose = False):\n",
    "        # assert emb is dictionary and each entry has same dimension\n",
    "        self.emb = emb\n",
    "        self.dim = len(self.emb[list(self.emb.keys())[0]])\n",
    "        self.emb['<UNK>'] = [0. for i in range(self.dim)]\n",
    "        self.emb['<PAD>'] = [1. for i in range(self.dim)]\n",
    "        self.emb['<GO>'] = [-1. for i in range(self.dim)]\n",
    "        \n",
    "        self.build_dicts()\n",
    "        \n",
    "        if verbose:\n",
    "            self.describe()\n",
    "        \n",
    "    def describe(self):\n",
    "        print('Embedding Dimension: {}'.format(self.dim))\n",
    "        print('Embedding Symbols: {}'.format(len(self.emb)))\n",
    "        print('Index to symbol: {}'.format([(i, self.idx2Sym[i]) for i in range(10)]))\n",
    "        \n",
    "    def getIndex(self, symbol):\n",
    "        if symbol in self.sym2Idx:\n",
    "            return self.sym2Idx[symbol]\n",
    "        else:\n",
    "            return self.sym2Idx['<UNK>']\n",
    "\n",
    "    def getEmb(self, symbol):\n",
    "        return self.emb[self.idx2Sym[self.getIndex(symbol)]]\n",
    "    \n",
    "    def getSymbols(self, indices):\n",
    "        return [self.idx2Sym[idx] for idx in indices]\n",
    "\n",
    "    def getNumpyArray(self):\n",
    "        return np.array([self.emb[self.idx2Sym[idx]] for idx in range(len(self.emb))])\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        self.sym2Idx = {}\n",
    "        index = 0\n",
    "        for key in self.emb.keys():\n",
    "            self.sym2Idx[key] = index\n",
    "            index += 1\n",
    "            \n",
    "        self.idx2Sym = { v:k for k, v in self.sym2Idx.items()}\n",
    "\n",
    "glove_emb = Embedding(glove, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = file.read('data/Quora/train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.question1 = df.question1.astype(str)\n",
    "df.question2 = df.question2.astype(str)\n",
    "df.is_duplicate = df.is_duplicate.astype(float)\n",
    "\n",
    "df = df.as_matrix(['question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "data = {}\n",
    "data['train'], data['test'] = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessQuestion(string):\n",
    "    try:\n",
    "        return [glove_emb.getIndex(token.lower()) for token in tokenize(string)]\n",
    "    except:\n",
    "        print(string)\n",
    "\n",
    "\n",
    "def preprocessData(data):\n",
    "    return [[preprocessQuestion(rec[0]), preprocessQuestion(rec[1]), float(rec[2])] for rec in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ['train', 'test']:\n",
    "    data[i] = preprocessData(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Turns iteratable of symbols into padded batch\n",
    "from functools import lru_cache\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.verbose = verbose\n",
    "        self.size = len(self.seqs)\n",
    "        self.seq_lens = [len(seq) for seq in self.seqs]\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.describe()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def max_length(self):\n",
    "        return max(self.seq_lens)\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def longgest_sequence(self):\n",
    "        for seq in self.seqs:\n",
    "            if len(seq) == self.max_length():\n",
    "                return seq\n",
    "    \n",
    "    def describe(self):\n",
    "        print('Size: {}'.format(self.size))\n",
    "        print(\"Longest sequence length: {}\".format(self.max_length()))\n",
    "        bin_width = max(1, self.max_length() // 30)\n",
    "        plt.hist(self.seq_lens, range(0, self.max_length() + bin_width, bin_width))\n",
    "        plt.title('Sequence length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def batchPadding(self, batch, padding_symbol):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), padding_symbol)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = batch[i]\n",
    "        return result\n",
    "\n",
    "    def batchMask(self, batch):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), 0.0)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = 1.0\n",
    "        return result\n",
    "        \n",
    "    # Same length within the batch, stuffed with padding symbol\n",
    "    def generator(self, padding_symbol, batch_size=None, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        length = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for seq in self.seqs:\n",
    "                train.append([sym for sym in seq])\n",
    "                length.append(len(seq))\n",
    "                if(len(train) == batch_size):\n",
    "                    yield self.batchPadding(train, padding_symbol), length, self.batchMask(train)\n",
    "                    train = []\n",
    "                    length = []\n",
    "            epouch -= 1\n",
    "            if self.verbose:\n",
    "                print('epouch done...')\n",
    "                \n",
    "class Batcher2:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.size = len(self.seqs)\n",
    "\n",
    "    def generator(self, batch_size=32, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for sym in self.seqs:\n",
    "                train.append([sym])\n",
    "                if(len(train) == batch_size):\n",
    "                    yield train\n",
    "                    train = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')\n",
    "            \n",
    "            \n",
    "# Turn data into batch, where data is iterable over records, record is iterable over fields\n",
    "class Batcher3:\n",
    "    def __init__(self, data):\n",
    "        #assert it is doubly iterable\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        \n",
    "    def generator(self, batch_size = 32, epouch = -1):\n",
    "        batch = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for record in self.data:\n",
    "                batch.append(record)\n",
    "                if(len(batch) == batch_size):\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batcher = {}\n",
    "for i in ['train', 'test']:\n",
    "    batcher[i] = Batcher3(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LV1_DIM = 10\n",
    "LV2_STEP = 1\n",
    "LV2_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embeddings_initializer(shape):\n",
    "    with tf.variable_scope(\"Embeddings_Initializer\"):\n",
    "        in_emb = tf.placeholder(\n",
    "            dtype = tf.float32, \n",
    "            shape = shape, \n",
    "            name = \"Placeholder\"\n",
    "        )\n",
    "        \n",
    "        emb = tf.Variable(\n",
    "            tf.constant(0.0, shape = shape), \n",
    "            trainable=False, \n",
    "            name = 'Embeddings', \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        init_emb = emb.assign(in_emb)\n",
    "    return in_emb, init_emb, emb\n",
    "\n",
    "def cells_initializer(num_units, reuse):\n",
    "    with tf.variable_scope(\"Cells_Initializer\"):\n",
    "        cells = tf.contrib.rnn.GRUCell(\n",
    "            num_units = num_units,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "    return cells\n",
    "\n",
    "\n",
    "#IN (batch, time, 1)\n",
    "def simple_embedding(inputs, embeddings):\n",
    "    with tf.variable_scope(\"Simple_Embedding\"):\n",
    "        lookup = tf.nn.embedding_lookup(\n",
    "            params = embeddings,\n",
    "            ids = inputs,\n",
    "            partition_strategy='mod',\n",
    "            name='Embedding_Lookup',\n",
    "            validate_indices=True,\n",
    "            max_norm=1\n",
    "        )\n",
    "\n",
    "    return lookup\n",
    "\n",
    "#OUT: (batch, time, dim) float32\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_dynamic_rnn(cell, inputs, lengths):\n",
    "    with tf.variable_scope(\"Simple_Dynamic_RNN\"):        \n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell, \n",
    "            inputs, \n",
    "            dtype = tf.float32, \n",
    "            sequence_length = lengths\n",
    "        )\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        step_size = tf.shape(inputs)[1]\n",
    "        indices = tf.range(0, batch_size) * step_size + (lengths - 1)\n",
    "        gather = tf.reshape(\n",
    "            tf.gather(\n",
    "                tf.reshape(outputs, [-1, cell.output_size]), indices\n",
    "            ), \n",
    "            [-1, cell.output_size]\n",
    "         )\n",
    "        \n",
    "    return gather\n",
    "#OUT (batch, dim)\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_encoder(inputs, input_lengths, embeddings):\n",
    "    with tf.variable_scope('Simple_Encoder'):\n",
    "        \n",
    "        emb = simple_embedding(inputs, embeddings)\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = None\n",
    "        )\n",
    "\n",
    "        rnn = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = emb,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn\n",
    "            \n",
    "        #\n",
    "        # Conv layer does not support dynamic length ;/\n",
    "        #\n",
    "    \"\"\"\n",
    "        filter_2 = tf.Variable(\n",
    "            tf.random_uniform([2, WORD_DIM, LV1_DIM], -1, 1), \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        #IN (batch, time, dim)\n",
    "        conv_2 = tf.nn.conv1d(\n",
    "            value = inputs,\n",
    "            filters = filter_2,\n",
    "            stride = 1,\n",
    "            padding = 'VALID',\n",
    "            use_cudnn_on_gpu=True,\n",
    "            data_format=None,\n",
    "            name='Conv_Witdh_2'\n",
    "        )\n",
    "        #OUT (batch, time-1, dim)\n",
    "\n",
    "    with tf.variable_scope('Level_2_RNN'):\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size=None,\n",
    "            activation=tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        rnn_output_2 = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = inputs,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn_output_2\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#OUT (batch, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.variable_scope(\"Inputs_Layer\"):\n",
    "    \n",
    "        #IN\n",
    "        in_word_indices = [tf.placeholder(tf.int32, (None, None), name = \"Q{}_Word_Indices\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        #batch_size = [tf.shape(inputs[i], name= \"Q{}_Batch_Size\".format(i+1))[0] for i in range(2)]\n",
    "        #steps = [tf.shape(inputs[i], name= \"Q{}_Steps\".format(i+1))[1] for i in range(2)]\n",
    "        \n",
    "        #IN\n",
    "        in_lengths = [tf.placeholder(tf.int32, (None), name = \"Q{}_Lengths\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch) int32\n",
    "        \n",
    "        #in_word_indices = tf.placeholder(tf.int32, (None, None), name = \"Q_Word_Indices\")\n",
    "        \n",
    "        #in_lengths = tf.placeholder(tf.int32, (None), name = \"Q_Lengths\")\n",
    "        \n",
    "        in_truth = tf.placeholder(tf.float32, (None, 1), name = \"Truth\")\n",
    "\n",
    "    with tf.variable_scope(\"Embeddings_Layer\"):\n",
    "        in_emb, init_emb, emb = embeddings_initializer((WORD_COUNT, WORD_DIM))\n",
    "        \n",
    "    with tf.variable_scope(\"Encoder_0\"):\n",
    "        enc_0 = simple_encoder(in_word_indices[0], in_lengths[0], emb)\n",
    "        \n",
    "    with tf.variable_scope(\"Encoder_1\"):\n",
    "        enc_1 = simple_encoder(in_word_indices[1], in_lengths[1], emb)\n",
    "\n",
    "    #IN: (batch, dim) x2\n",
    "    with tf.variable_scope(\"Prediction_Layer\"):\n",
    "        #pred_in = [tf.placeholder(tf.float32, (None, LV2_DIM)) for i in range(2)]\n",
    "        #pred_input = tf.concat([pred_in[0], pred_in[1]], 1)\n",
    "        pred_input = tf.concat([enc_0, enc_1], 1)\n",
    "        \n",
    "        pred_weights = tf.Variable(tf.random_uniform([LV2_DIM * 2, 1], -1, 1), name='weights')\n",
    "        pred_bias = tf.Variable(tf.constant(0.0, shape=[1]), name=\"bias\")\n",
    "        \n",
    "        pred = tf.nn.sigmoid(pred_input @ pred_weights + pred_bias)\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(in_truth, pred))\n",
    "    acc = tf.reduce_mean(tf.contrib.keras.metrics.binary_accuracy(in_truth, pred))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 2000000\n",
    "MODEL = './model/q.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def batchPadding(batch, padding_symbol):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result\n",
    "\n",
    "def proc_batch(batch):\n",
    "    q1 = batchPadding([record[0] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q2 = batchPadding([record[1] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q1_lens = [len(record[0]) for record in batch]\n",
    "    q2_lens = [len(record[1]) for record in batch]\n",
    "    truth = [[record[2]] for record in batch]\n",
    "    \n",
    "    return q1, q2, q1_lens, q2_lens, truth\n",
    "\n",
    "gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    gen[i] = batcher[i].generator(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 200\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    try:\n",
    "        #saver.restore(session, MODEL)\n",
    "        #print('Restored training...')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "    except:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "        \n",
    "    session.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    #run_metadata = tf.RunMetadata()\n",
    "    \n",
    "    #tvars_vals = session.run(tvars)\n",
    "    #for var, val in zip(tvars, tvars_vals):\n",
    "    #    print(var.name, val)  # Prints the name of the variable alongside its value.\n",
    "\n",
    "    #for name in session.run( tf.report_uninitialized_variables( tf.global_variables( ) ) ):\n",
    "    #    print(name)\n",
    "    \n",
    "    \n",
    "    average_loss = 0\n",
    "    average_max_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        train_q1, train_q2, train_q1_lengths, train_q2_lengths, train_label = proc_batch(next(gen['train']))\n",
    "        \n",
    "        feed_dict = {\n",
    "            in_word_indices[0]: train_q1,\n",
    "            in_word_indices[1]: train_q2,\n",
    "            in_lengths[0]: train_q1_lengths,\n",
    "            in_lengths[1]: train_q2_lengths,\n",
    "            in_truth: train_label\n",
    "        }\n",
    "        \n",
    "        #_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()\n",
    "                \n",
    "                avg_loss_val = 0\n",
    "                avg_acc_val = 0\n",
    "                for i in range(10):\n",
    "                    \n",
    "                    test_q1, test_q2, test_q1_lengths, test_q2_lengths, test_label = proc_batch(next(gen['test']))\n",
    "    \n",
    "                    feed_dict = {\n",
    "                        in_word_indices[0]: test_q1,\n",
    "                        in_word_indices[1]: test_q2,\n",
    "                        in_lengths[0]: test_q1_lengths,\n",
    "                        in_lengths[1]: test_q2_lengths,\n",
    "                        in_truth: test_label\n",
    "                    }\n",
    "\n",
    "                    loss_val, acc_val = session.run([loss, acc], feed_dict=feed_dict)\n",
    "                    avg_loss_val+=loss_val\n",
    "                    avg_acc_val += acc_val\n",
    "                \n",
    "                print('Testing Set 10 batch loss: {0}, acc {1}:'.format(avg_loss_val/10.0, avg_acc_val / 10.0))\n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            # Create the Timeline object, and write it to a json\n",
    "            #tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            #ctf = tl.generate_chrome_trace_format()\n",
    "            #with open('timeline.json', 'w') as f:\n",
    "            #    f.write(ctf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 100\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    print('Restored model...')\n",
    "    \n",
    "    average_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "        test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "        test_label = next(label_gen['test'])\n",
    "        \n",
    "        feed_dict = {\n",
    "            inputs[0]: test_q1,\n",
    "            inputs[1]: test_q2,\n",
    "            input_lengths[0]: test_q1_lengths,\n",
    "            input_lengths[1]: test_q2_lengths,\n",
    "            truth: test_label\n",
    "        }\n",
    "        \n",
    "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "q1_gen = {}\n",
    "q2_gen = {}\n",
    "label_gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    q1_gen[i] = q1_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    q2_gen[i] = q2_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    label_gen[i] = label_batcher[i].generator(batch_size=BATCH_SIZE)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "    test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "    test_label = next(label_gen['test'])\n",
    "        \n",
    "    feed_dict = {\n",
    "        in_word_indices[0]: test_q1,\n",
    "        in_word_indices[1]: test_q2,\n",
    "        in_lengths[0]: test_q1_lengths,\n",
    "        in_lengths[1]: test_q2_lengths,\n",
    "        in_truth: test_label\n",
    "    }\n",
    "    \n",
    "    e0_val, e1_val = sess.run([e0, e1], feed_dict=feed_dict)\n",
    "    print(e0_val)\n",
    "    print(e1_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
