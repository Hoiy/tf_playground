{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline\n",
    "import time\n",
    "from DataLoader import GloVe\n",
    "from TextPreprocess.sequences import Sequences\n",
    "from TextPreprocess.Tokenizer.RegExp import tokenize\n",
    "import Utils.pandas_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "WORD_DIM = 300\n",
    "WORD_COUNT = 400000+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = file.read('data/Quora/train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {}\n",
    "data['train'], data['test'] = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Loading Glove Model\n",
      "End: Loaded 400000 rows.\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe.load2('./data/GloVe/glove.6B.{}d.txt'.format(WORD_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: 300\n",
      "Embedding Symbols: 400003\n",
      "Index to symbol: [(0, 'belediyespor'), (1, 'bunchhay'), (2, '109.2'), (3, 'stefon'), (4, 'name'), (5, 'rhynie'), (6, 'malita'), (7, '120.52'), (8, '5-under'), (9, 'otv')]\n"
     ]
    }
   ],
   "source": [
    "# emb: Symbol to float32 of fixed DIMENSION\n",
    "# Create an index mapping, index to symbol, symbol to index\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, emb, verbose = False):\n",
    "        # assert emb is dictionary and each entry has same dimension\n",
    "        self.emb = emb\n",
    "        self.dim = len(self.emb[list(self.emb.keys())[0]])\n",
    "        self.emb['<UNK>'] = [0. for i in range(self.dim)]\n",
    "        self.emb['<PAD>'] = [1. for i in range(self.dim)]\n",
    "        self.emb['<GO>'] = [-1. for i in range(self.dim)]\n",
    "        \n",
    "        self.build_dicts()\n",
    "        \n",
    "        if verbose:\n",
    "            self.describe()\n",
    "        \n",
    "    def describe(self):\n",
    "        print('Embedding Dimension: {}'.format(self.dim))\n",
    "        print('Embedding Symbols: {}'.format(len(self.emb)))\n",
    "        print('Index to symbol: {}'.format([(i, self.idx2Sym[i]) for i in range(10)]))\n",
    "        \n",
    "    def getIndex(self, symbol):\n",
    "        if symbol in self.sym2Idx:\n",
    "            return self.sym2Idx[symbol]\n",
    "        else:\n",
    "            return self.sym2Idx['<UNK>']\n",
    "\n",
    "    def getEmb(self, symbol):\n",
    "        return self.emb[self.idx2Sym[self.getIndex(symbol)]]\n",
    "    \n",
    "    def getSymbols(self, indices):\n",
    "        return [self.idx2Sym[idx] for idx in indices]\n",
    "\n",
    "    def getNumpyArray(self):\n",
    "        return np.array([self.emb[self.idx2Sym[idx]] for idx in range(len(self.emb))])\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        self.sym2Idx = {}\n",
    "        index = 0\n",
    "        for key in self.emb.keys():\n",
    "            self.sym2Idx[key] = index\n",
    "            index += 1\n",
    "            \n",
    "        self.idx2Sym = { v:k for k, v in self.sym2Idx.items()}\n",
    "\n",
    "glove_emb = Embedding(glove, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    return [glove_emb.getIndex(token.lower()) for token in tokenize(string)]\n",
    "\n",
    "def preprocessLabels(val):\n",
    "    return [1., 0.] if val == 0 else [0., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turns iteratable of symbols into padded batch\n",
    "from functools import lru_cache\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.verbose = verbose\n",
    "        self.size = len(self.seqs)\n",
    "        self.seq_lens = [len(seq) for seq in self.seqs]\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.describe()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def max_length(self):\n",
    "        return max(self.seq_lens)\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def longgest_sequence(self):\n",
    "        for seq in self.seqs:\n",
    "            if len(seq) == self.max_length():\n",
    "                return seq\n",
    "    \n",
    "    def describe(self):\n",
    "        print('Size: {}'.format(self.size))\n",
    "        print(\"Longest sequence length: {}\".format(self.max_length()))\n",
    "        bin_width = max(1, self.max_length() // 30)\n",
    "        plt.hist(self.seq_lens, range(0, self.max_length() + bin_width, bin_width))\n",
    "        plt.title('Sequence length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def batchPadding(self, batch, padding_symbol):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), padding_symbol)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = batch[i]\n",
    "        return result\n",
    "\n",
    "    def batchMask(self, batch):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), 0.0)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = 1.0\n",
    "        return result\n",
    "        \n",
    "    # Same length within the batch, stuffed with padding symbol\n",
    "    def generator(self, padding_symbol, batch_size=None, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        length = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for seq in self.seqs:\n",
    "                train.append([sym for sym in seq])\n",
    "                length.append(len(seq))\n",
    "                if(len(train) == batch_size):\n",
    "                    yield self.batchPadding(train, padding_symbol), length, self.batchMask(train)\n",
    "                    train = []\n",
    "                    length = []\n",
    "            epouch -= 1\n",
    "            if self.verbose:\n",
    "                print('epouch done...')\n",
    "                \n",
    "                \n",
    "\n",
    "class Batcher2:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.size = len(self.seqs)\n",
    "\n",
    "    def generator(self, batch_size=32, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for sym in self.seqs:\n",
    "                train.append([sym])\n",
    "                if(len(train) == batch_size):\n",
    "                    yield train\n",
    "                    train = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1={}\n",
    "q2={}\n",
    "label={}\n",
    "for i in ['train', 'test']:\n",
    "    q1[i] = data[i]['question1'].astype(str).apply(preprocess)\n",
    "    q2[i] = data[i]['question2'].astype(str).apply(preprocess)\n",
    "    label[i] = data[i]['is_duplicate'].astype('float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_batcher = {}\n",
    "q2_batcher = {}\n",
    "label_batcher = {}\n",
    "for i in ['train', 'test']:\n",
    "    q1_batcher[i] = Batcher(q1[i])\n",
    "    q2_batcher[i] = Batcher(q2[i])\n",
    "    label_batcher[i] = Batcher2(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING = glove_emb.getNumpyArray()\n",
    "\n",
    "LV1_DIM = 10\n",
    "LV2_STEP = 1\n",
    "LV2_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IN (batch, time, dim)\n",
    "def simple_dynamic_rnn(cell, inputs, lengths, scope):\n",
    "     with tf.variable_scope(scope):\n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell, \n",
    "            inputs, \n",
    "            initial_state = cell.zero_state(tf.shape(inputs)[0], dtype=tf.float32),\n",
    "            dtype = tf.float32, \n",
    "            sequence_length = lengths\n",
    "        )\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        step_size = tf.shape(inputs)[1]\n",
    "        indices = tf.range(0, batch_size) * step_size + (lengths - 1)\n",
    "        gather = tf.reshape(tf.gather(tf.reshape(outputs, [-1, cell.output_size]), indices), [-1, cell.output_size])\n",
    "        return gather\n",
    "#OUT (batch, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Level_2_RNN/L2/rnn/gru_cell/gates/weights:0' shape=(320, 40) dtype=float32_ref>, <tf.Variable 'Level_2_RNN/L2/rnn/gru_cell/gates/biases:0' shape=(40,) dtype=float32_ref>, <tf.Variable 'Level_2_RNN/L2/rnn/gru_cell/candidate/weights:0' shape=(320, 20) dtype=float32_ref>, <tf.Variable 'Level_2_RNN/L2/rnn/gru_cell/candidate/biases:0' shape=(20,) dtype=float32_ref>, <tf.Variable 'Final_Prediction/weights:0' shape=(40, 1) dtype=float32_ref>, <tf.Variable 'Final_Prediction/bias:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hoiy\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.variable_scope(\"Inputs\"):\n",
    "    \n",
    "        #IN\n",
    "        inputs = [tf.placeholder(tf.int32, (None, None), name = \"Q{}_Word_Indices\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        batch_size = [tf.shape(inputs[i], name= \"Q{}_Batch_Size\".format(i+1))[0] for i in range(2)]\n",
    "        steps = [tf.shape(inputs[i], name= \"Q{}_Steps\".format(i+1))[1] for i in range(2)]\n",
    "        \n",
    "        #IN\n",
    "        input_lengths = [tf.placeholder(tf.int32, (None), name = \"Q{}_Lengths\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch) int32\n",
    "        \n",
    "        truth = tf.placeholder(tf.float32, (None, 1), name = \"labels\")\n",
    "        \n",
    "    with tf.variable_scope(\"Embedding\"):\n",
    "        \n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[WORD_COUNT, WORD_DIM]), trainable=False, name='embeddings', dtype=tf.float32)\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [WORD_COUNT, WORD_DIM])\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "\n",
    "        encoder_inputs = [tf.nn.embedding_lookup(embeddings, inputs[i]) for i in range(2)] \n",
    "        #OUT: (batch, time, dim) float32\n",
    "        \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Level_1_RNN\"):\n",
    "        \n",
    "        rnn_output_1 = [[simple_dynamic_rnn(\n",
    "                    cell = tf.contrib.rnn.GRUCell(\n",
    "                        LV1_DIM, \n",
    "                        reuse = True if i == 1 else None\n",
    "                    ),\n",
    "                    inputs = encoder_inputs[i],\n",
    "                    lengths = input_lengths[i],\n",
    "                    scope = '{}'.format(step)\n",
    "                ) for step in range(LV2_STEP)] for i in range(2)]\n",
    "        # OUT: rnn_output[LV2_STEP][LV1_DIM]\n",
    "        # (time, batch, dim) float32\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"Level_2_RNN\"):\n",
    "        \n",
    "        #rnn_input_2 = [tf.transpose(rnn_output_1[i], [1, 0, 2]) for i in range(2)]\n",
    "        # (batch, time, dim)\n",
    "        \n",
    "        cell = [tf.contrib.rnn.GRUCell(\n",
    "                        LV2_DIM,\n",
    "                        reuse = True if i == 1 else None\n",
    "                ) for i in range(2)]\n",
    "        \n",
    "        rnn_output_2 = [simple_dynamic_rnn(\n",
    "                    cell = cell[i],\n",
    "                    #inputs = rnn_input_2[i],\n",
    "                    #lengths = tf.fill(tf.shape(input_lengths[i]), LV2_STEP),\n",
    "                    inputs = encoder_inputs[i],\n",
    "                    lengths = input_lengths[i],\n",
    "                    scope = 'L2',\n",
    "                ) for i in range(2)]\n",
    "        # OUT: [2](batch, LV2_DIM)\n",
    "        \n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Level_2_RNN')\n",
    "\n",
    "    with tf.variable_scope(\"Final_Prediction\"):\n",
    "        final_input = tf.concat([rnn_output_2[0], rnn_output_2[1]], 1)\n",
    "        \n",
    "        final_weights = tf.Variable(tf.random_uniform([LV2_DIM * 2, 1], -1, 1), name='weights')\n",
    "        final_bias = tf.Variable(tf.constant(0.0, shape=[1]), name=\"bias\")\n",
    "        \n",
    "        predict = tf.nn.sigmoid(final_input @ final_weights + final_bias)\n",
    "        \n",
    "        variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Final_Prediction')\n",
    "        \n",
    "        print(variables)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(truth, predict))\n",
    "    acc = tf.reduce_mean(tf.contrib.keras.metrics.binary_accuracy(truth, predict))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 2000000\n",
    "MODEL = './model/q.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "q1_gen = {}\n",
    "q2_gen = {}\n",
    "label_gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    q1_gen[i] = q1_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    q2_gen[i] = q2_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    label_gen[i] = label_batcher[i].generator(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting training...\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 71.12s, each step: 0.07s\n",
      "Average mean loss at step  1000 :  0.57800303492\n",
      "Testing Set 10 batch loss: 0.5304929494857789, acc 0.74453125:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 69.59s, each step: 0.07s\n",
      "Average mean loss at step  2000 :  0.536945110917\n",
      "Testing Set 10 batch loss: 0.5374695062637329, acc 0.72265625:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "epouch done...\n",
      "Total time for 1000 steps: 68.65s, each step: 0.07s\n",
      "Average mean loss at step  3000 :  0.522771584481\n",
      "Testing Set 10 batch loss: 0.5204206705093384, acc 0.76015625:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 70.74s, each step: 0.07s\n",
      "Average mean loss at step  4000 :  0.51573110646\n",
      "Testing Set 10 batch loss: 0.5183598637580872, acc 0.740625:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 70.64s, each step: 0.07s\n",
      "Average mean loss at step  5000 :  0.507175723791\n",
      "Testing Set 10 batch loss: 0.5043836742639541, acc 0.75546875:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "epouch done...\n",
      "Total time for 1000 steps: 70.41s, each step: 0.07s\n",
      "Average mean loss at step  6000 :  0.504048728138\n",
      "Testing Set 10 batch loss: 0.5231233030557633, acc 0.7375:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 69.86s, each step: 0.07s\n",
      "Average mean loss at step  7000 :  0.499102885127\n",
      "Testing Set 10 batch loss: 0.5219950556755066, acc 0.753125:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "epouch done...\n",
      "Total time for 1000 steps: 68.63s, each step: 0.07s\n",
      "Average mean loss at step  8000 :  0.495786408693\n",
      "Testing Set 10 batch loss: 0.5146391153335571, acc 0.7515625:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 71.18s, each step: 0.07s\n",
      "Average mean loss at step  9000 :  0.494674029082\n",
      "Testing Set 10 batch loss: 0.5298146545886994, acc 0.73359375:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "Total time for 1000 steps: 69.50s, each step: 0.07s\n",
      "Average mean loss at step  10000 :  0.488102026671\n",
      "Testing Set 10 batch loss: 0.522747865319252, acc 0.73984375:\n",
      "Model saved in file: ./model/q.ckpt\n",
      "epouch done...\n"
     ]
    }
   ],
   "source": [
    "DEBUG_SIZE = 1000\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    try:\n",
    "        #saver.restore(session, MODEL)\n",
    "        #print('Restored training...')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "    except:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "        \n",
    "    session.run(embedding_init, feed_dict={embedding_placeholder: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    #run_metadata = tf.RunMetadata()\n",
    "    \n",
    "    #tvars_vals = session.run(tvars)\n",
    "    #for var, val in zip(tvars, tvars_vals):\n",
    "    #    print(var.name, val)  # Prints the name of the variable alongside its value.\n",
    "\n",
    "    #for name in session.run( tf.report_uninitialized_variables( tf.global_variables( ) ) ):\n",
    "    #    print(name)\n",
    "    \n",
    "    \n",
    "    average_loss = 0\n",
    "    average_max_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        train_q1, train_q1_lengths, _ = next(q1_gen['train'])\n",
    "        train_q2, train_q2_lengths, _ = next(q2_gen['train'])\n",
    "        train_label = next(label_gen['train'])\n",
    "        \n",
    "        feed_dict = {\n",
    "            inputs[0]: train_q1,\n",
    "            inputs[1]: train_q2,\n",
    "            input_lengths[0]: train_q1_lengths,\n",
    "            input_lengths[1]: train_q2_lengths,\n",
    "            truth: train_label\n",
    "        }\n",
    "        \n",
    "        #_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()\n",
    "                \n",
    "                avg_loss_val = 0\n",
    "                avg_acc_val = 0\n",
    "                for i in range(10):\n",
    "                \n",
    "                    test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "                    test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "                    test_label = next(label_gen['test'])\n",
    "        \n",
    "                    feed_dict = {\n",
    "                        inputs[0]: test_q1,\n",
    "                        inputs[1]: test_q2,\n",
    "                        input_lengths[0]: test_q1_lengths,\n",
    "                        input_lengths[1]: test_q2_lengths,\n",
    "                        truth: test_label\n",
    "                    }\n",
    "\n",
    "                    loss_val, acc_val = session.run([loss, acc], feed_dict=feed_dict)\n",
    "                    avg_loss_val+=loss_val\n",
    "                    avg_acc_val += acc_val\n",
    "                \n",
    "                print('Testing Set 10 batch loss: {0}, acc {1}:'.format(avg_loss_val/10.0, avg_acc_val / 10.0))\n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            # Create the Timeline object, and write it to a json\n",
    "            #tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            #ctf = tl.generate_chrome_trace_format()\n",
    "            #with open('timeline.json', 'w') as f:\n",
    "            #    f.write(ctf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 100\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    print('Restored model...')\n",
    "    \n",
    "    average_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "        test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "        test_label = next(label_gen['test'])\n",
    "        \n",
    "        feed_dict = {\n",
    "            inputs[0]: test_q1,\n",
    "            inputs[1]: test_q2,\n",
    "            input_lengths[0]: test_q1_lengths,\n",
    "            input_lengths[1]: test_q2_lengths,\n",
    "            truth: test_label\n",
    "        }\n",
    "        \n",
    "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
