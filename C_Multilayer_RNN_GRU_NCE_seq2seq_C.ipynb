{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sents = brown.sents()\n",
    "sents = [[token.lower() for token in sent] for sent in sents]\n",
    "words = brown.words()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1161192\n",
      "Number of sentences: 57340\n",
      "Longest sentences length: 180\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens: {}\".format(len(words)))\n",
    "print(\"Number of sentences: {}\".format(len(sents)))\n",
    "print(\"Longest sentences length: {}\".format(max([len(sent) for sent in sents])))\n",
    "MAX_SENTENCE_LENGTH = max([len(sent) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "five\n",
      "Number of unique tokens: 49815\n"
     ]
    }
   ],
   "source": [
    "words_dict, inv_words_dict = words2dict.convert(words)\n",
    "print(words_dict['five'])\n",
    "print(inv_words_dict[334])\n",
    "\n",
    "words_size = len(words_dict)\n",
    "print(\"Number of unique tokens: {}\".format(words_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VOCABULAY_SIZE = len(words_dict)\n",
    "GO_SYMBOL = VOCABULAY_SIZE - 1\n",
    "PADDING_SYMBOL = VOCABULAY_SIZE - 2\n",
    "UNK_SYMBOL = VOCABULAY_SIZE - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batchPadding(batch, padding_symbol=PADDING_SYMBOL):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result\n",
    "\n",
    "def batchMask(batch):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), 0.0)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef dataGenerator(sents, words_dict, window_size = 2, batch_size=32, train_length=2, epouch=-1, padding_symbol=PADDING_SYMBOL):\\n    train = []\\n    label = []\\n    length = []\\n    while(epouch < 0 or epouch > 0):\\n        left_window = [padding_symbol for i in range(window_size)]\\n        target = [padding_symbol for i in range(train_length)]\\n        right_window = [padding_symbol for i in range(window_size)]\\n        for sent in sents:\\n            for word in sent:\\n                right_window.append(words_dict[word])\\n                target.append(right_window.pop(0))\\n                left_window.append(target.pop(0))\\n                left_window.pop(0)\\n                \\n                for context in left_window + right_window:\\n                    train.append(list(target))\\n                    label.append(list([context]))\\n                    length.append(len(target))\\n                    if(len(train) == batch_size):\\n                        yield train, label, length\\n                        train = []\\n                        label = []\\n                        length = []\\n        epouch -= 1\\n        print('epouch done...')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def dataGenerator(sents, words_dict, window_size = 2, batch_size=32, train_length=2, epouch=-1, padding_symbol=PADDING_SYMBOL):\n",
    "    train = []\n",
    "    label = []\n",
    "    length = []\n",
    "    while(epouch < 0 or epouch > 0):\n",
    "        left_window = [padding_symbol for i in range(window_size)]\n",
    "        target = [padding_symbol for i in range(train_length)]\n",
    "        right_window = [padding_symbol for i in range(window_size)]\n",
    "        for sent in sents:\n",
    "            for word in sent:\n",
    "                right_window.append(words_dict[word])\n",
    "                target.append(right_window.pop(0))\n",
    "                left_window.append(target.pop(0))\n",
    "                left_window.pop(0)\n",
    "                \n",
    "                for context in left_window + right_window:\n",
    "                    train.append(list(target))\n",
    "                    label.append(list([context]))\n",
    "                    length.append(len(target))\n",
    "                    if(len(train) == batch_size):\n",
    "                        yield train, label, length\n",
    "                        train = []\n",
    "                        label = []\n",
    "                        length = []\n",
    "        epouch -= 1\n",
    "        print('epouch done...')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentenceGenerator(sents, words_dict, batch_size=32, epouch=-1, padding_symbol=PADDING_SYMBOL):\n",
    "    train = []\n",
    "    length = []\n",
    "    while(epouch < 0 or epouch > 0):\n",
    "        for sent in sents:\n",
    "            train.append([words_dict[word] for word in sent])\n",
    "            length.append(len(sent))\n",
    "            if(len(train) == batch_size):\n",
    "                yield batchPadding(train), length, batchMask(train)\n",
    "                train = []\n",
    "                length = []\n",
    "        epouch -= 1\n",
    "        print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TRAIN_LENGTH = 4\n",
    "#WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generator = dataGenerator(sents, words_dict, window_size = WINDOW_SIZE, batch_size=BATCH_SIZE, train_length=TRAIN_LENGTH, epouch=1)\n",
    "#bigram_list = []\n",
    "#for batch_inputs, _, _ in generator:\n",
    "#    bigram_list += ['_'.join([inv_words_dict[idx] for idx in batch_input]) for batch_input in batch_inputs]\n",
    "        \n",
    "#bigrams_dict, inv_bigrams_dict = words2dict.convert(bigram_list)\n",
    "#generator = sentenceGenerator(sents, words_dict)\n",
    "#batch, lengths, mask = next(generator)\n",
    "#print(batch[1])\n",
    "#print(lengths)\n",
    "#print(mask[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef visualizeData(generator):\\n    train, label, length = next(generator)\\n    for i in range(len(train)):\\n        print([inv_words_dict[word] for word in train[i]], [inv_words_dict[word] for word in label[i]], length[i])\\n\\ngenerator = dataGenerator(sents[:1], words_dict, window_size = 1, batch_size=64, train_length=2)\\n#print(sents[0])\\n#visualizeData(generator)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def visualizeData(generator):\n",
    "    train, label, length = next(generator)\n",
    "    for i in range(len(train)):\n",
    "        print([inv_words_dict[word] for word in train[i]], [inv_words_dict[word] for word in label[i]], length[i])\n",
    "\n",
    "generator = dataGenerator(sents[:1], words_dict, window_size = 1, batch_size=64, train_length=2)\n",
    "#print(sents[0])\n",
    "#visualizeData(generator)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generator = dataGenerator(sents, words_dict, window_size = WINDOW_SIZE, batch_size=BATCH_SIZE, train_length=TRAIN_LENGTH)\n",
    "generator = sentenceGenerator(sents, words_dict, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RNN_DIMENSION = [100]\n",
    "RNN_LAYERS = len(RNN_DIMENSION)\n",
    "DIMENSION = 50\n",
    "NEGATIVE_SAMPLE = 128\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Encoder/rnn/while/Exit_3:0\", shape=(?, 100), dtype=float32) must be from the same graph as Tensor(\"Decoder/decoder/Const:0\", shape=(), dtype=int32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3c818b875b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             )\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\u001b[0m in \u001b[0;36mdynamic_decode\u001b[0;34m(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\u001b[0m\n\u001b[1;32m    276\u001b[0m         ],\n\u001b[1;32m    277\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mfinal_outputs_ta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m   \"\"\"\n\u001b[0;32m-> 2608\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"while\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No loop variables provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values)\u001b[0m\n\u001b[1;32m   4165\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4166\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4167\u001b[0;31m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4168\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4169\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Encoder/rnn/while/Exit_3:0\", shape=(?, 100), dtype=float32) must be from the same graph as Tensor(\"Decoder/decoder/Const:0\", shape=(), dtype=int32)."
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "        #IN\n",
    "        inputs = tf.placeholder(tf.int32, (None, None), name = \"Input_Sentence_Word_Index\")\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        #IN\n",
    "        input_lengths = tf.placeholder(tf.int32, (None), name = \"Input_Sentence_Length\")\n",
    "        #OUT: (batch) int32\n",
    "                \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        encoder_inputs = inputs\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        #decoder_inputs = tf.pad(tf.slice(encoder_inputs, [0,1], [batch_size, steps-1]), [[1, 0], [0, 0]])\n",
    "        decoder_inputs = tf.pad(\n",
    "            tf.slice(encoder_inputs, [0,0], [batch_size, steps-1]) - GO_SYMBOL, \n",
    "            [[0, 0], [1, 0]]\n",
    "        ) + GO_SYMBOL\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        decoder_input_lengths = tf.reshape(input_lengths, [batch_size])\n",
    "        #OUT: (batch) int32\n",
    "\n",
    "        # assume same input length\n",
    "        decoder_masks = tf.placeholder(tf.float32, (None, None), name = \"Input_Sentence_Mask\")\n",
    "        #OUT: (batch, time)\n",
    "        \n",
    "        #labels = tf.placeholder(tf.int32, (None, 1), name = \"Context_Word_Index\")\n",
    "        #OUT: (batch, 1) int32\n",
    "        \n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([VOCABULAY_SIZE, DIMENSION], -1.0, 1.0), trainable=False, name=\"Word2Vec\")\n",
    "        \n",
    "        #IN: (batch, time) int32\n",
    "        inputs_embed = tf.nn.embedding_lookup(embeddings, inputs, max_norm=1)\n",
    "        #OUT: (batch, time, dim) float32\n",
    "        decoder_embed = tf.nn.embedding_lookup(embeddings, decoder_inputs, max_norm=1)\n",
    "\n",
    "        # OUT: [time [batch_size, DIMENSION]]\n",
    "        \n",
    "        \n",
    "        ###### IN: (batch, time, DIMENSION) float32 ######\n",
    "        \n",
    "        with tf.variable_scope(\"Encoder\") as encoder_scope:\n",
    "            #encoder_cell = tf.contrib.rnn.LSTMCell(RNN_DIMENSION[-1])\n",
    "            #cell = tf.contrib.rnn.GRUCell(RNN_DIMENSION[-1])\n",
    "            stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "            #stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "            #bw_stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "\n",
    "            #rnn_tuple_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(l[i][0], l[i][1]) for i in range(RNN_LAYERS)])\n",
    "\n",
    "            #cell = tf.contrib.rnn.LSTMCell(DIMENSION, state_is_tuple=True)        \n",
    "            #cell = tf.contrib.rnn.GRUCell(DIMENSION)\n",
    "            #initial_state = stack.zero_state(batch_size, tf.float32)\n",
    "            #bw_initial_state = bw_stack.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            rnn_outputs, rnn_states = tf.nn.dynamic_rnn(stack, inputs_embed, initial_state=None, dtype=tf.float32, sequence_length=input_lengths)\n",
    "            \n",
    "            #IN: (batch, time, RNN_DIMENSION[-1]) float32\n",
    "            index = tf.range(0, batch_size) * tf.shape(inputs)[1] + (input_lengths - 1)\n",
    "            rnn_final_state = tf.gather(tf.reshape(rnn_outputs, [-1, RNN_DIMENSION[-1]]), index)\n",
    "            #rnn_final_state = tf.clip_by_norm(rnn_final_state, 1, axes=[1])\n",
    "            ###rnn_final_state = tf.gather(tf.reshape(rnn_outputs, [-1, DIMENSION]), index)\n",
    "            #OUT: (batch, RNN_DIMENSION[-1])\n",
    "                    \n",
    "        ###### OUT: (batch, time, RNN_DIMENSION[-1]) float32 ######\n",
    "        \n",
    "        ###### IN: (batch, time, RNN_DIMENSION[-1]) float32 ######\n",
    "        \n",
    "        with tf.variable_scope(\"Decoder\") as decoder_scope:\n",
    "            decoder_cell = tf.contrib.rnn.GRUCell(RNN_DIMENSION[-1])\n",
    "            #decoder_cell = tf.contrib.rnn.LSTMCell(RNN_DIMENSION[-1])\n",
    "            #decoder_stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "            #decoder_stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "            \n",
    "            if MODE == \"train\":\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs=decoder_embed,\n",
    "                    sequence_length=decoder_input_lengths)\n",
    "                \n",
    "            elif MODE == \"infer\":\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=embedding,\n",
    "                    start_tokens=tf.tile([GO_SYMBOL], [batch_size]),\n",
    "                    end_token=END_SYMBOL)\n",
    "\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell,\n",
    "                helper=helper,\n",
    "                initial_state=rnn_states[-1],\n",
    "                #output_layer=layers_core.Dense(VOCABULAY_SIZE, use_bias=True, activation=None))\n",
    "                output_layer=None)\n",
    "            #sequence_loss has softmax already\n",
    "\n",
    "            decoder_outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=False,\n",
    "                maximum_iterations=None,\n",
    "                parallel_iterations=32,\n",
    "                swap_memory=False,\n",
    "                scope=None\n",
    "            )\n",
    "            \n",
    "            sent_nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([VOCABULAY_SIZE, RNN_DIMENSION[-1]],\n",
    "                                stddev=1.0 / math.sqrt(RNN_DIMENSION[-1])), trainable=True)\n",
    "\n",
    "            sent_nce_biases = tf.Variable(tf.zeros([VOCABULAY_SIZE]), trainable=True)\n",
    "        \n",
    "            def nce_loss(labels, inputs):\n",
    "                return tf.nn.nce_loss(\n",
    "                    weights=sent_nce_weights,\n",
    "                    biases=sent_nce_biases,\n",
    "                    labels=tf.reshape(labels, [-1, 1]),\n",
    "                    inputs=inputs,\n",
    "                    num_sampled=NEGATIVE_SAMPLE,\n",
    "                    num_classes=VOCABULAY_SIZE)\n",
    "            \n",
    "            seq_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=decoder_outputs.rnn_output,\n",
    "                targets=inputs,\n",
    "                weights=decoder_masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True,\n",
    "                softmax_loss_function=nce_loss,\n",
    "                name=None\n",
    "            )\n",
    "        \n",
    "            loss = tf.reduce_mean(seq_loss)\n",
    "\n",
    "            \"\"\"\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs, decoder_input_lengths)\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, rnn_final_state)\n",
    "        \n",
    "            decoder_outputs, decoder_states = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n",
    "            print(decoder_outputs)\n",
    "            #rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs_embed, initial_state=rnn_final_state, sequence_length=input_lengths)\n",
    "            \n",
    "            #weights: A 2D Tensor of shape [batch_size x sequence_length] and dtype float. Weights constitutes the weighting of each prediction in the sequence. When using weights as masking set all valid timesteps to 1 and all padded timesteps to 0.\n",
    "            loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=decoder_outputs,\n",
    "                targets=labels,\n",
    "                weights=decoder_masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True,\n",
    "                softmax_loss_function=None,\n",
    "                name=None\n",
    "            ))\n",
    "            \"\"\"\n",
    "\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        optimizer = tf.train.MomentumOptimizer(1.0, 0.5).minimize(loss)\n",
    "        #optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        embeddings_saver = tf.train.Saver({'Words2Vec': embeddings})\n",
    "        #context = tf.nn.softmax(tf.matmul(rnn_final_state, tf.transpose(nce_weights)) + nce_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "MODEL = './model/seq2seq-autoencoder.ckpt'\n",
    "WORDS2VEC_MODEL = './model/brown-Words2Vec-{}.ckpt'.format(DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cloestWord(vec, words_vec, count=10, method='cos'):\n",
    "    if method == 'cos':\n",
    "        dist = np.array([ sum(vec * words_vec[key]) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[::-1][:10]\n",
    "    else:\n",
    "        dist = np.array([ sum(np.square(np.array(vec) - np.array(words_vec[key]))) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[:10]\n",
    "    return [list(words_vec.keys())[i] for i in top_ten]\n",
    "\n",
    "def cloestWord2(word, emb, count=10, method='cos'):\n",
    "    return cloestWord3(emb[words_dict[word]], emb, count, method)\n",
    "\n",
    "def cloestWord3(vec, emb, count=10, method='cos'):\n",
    "    if method == 'cos':\n",
    "        dist = np.array([ sum(vec * emb[i]) for i in range(emb.shape[0])])\n",
    "        # dist: word index -> dist\n",
    "        \n",
    "        top = dist.argsort()[::-1][:count]\n",
    "        # top: ranking -> word index\n",
    "        \n",
    "    return [(inv_words_dict[i], \"%.2f\" % dist[i])  for i in top]\n",
    "\n",
    "def to_word_indices(words):\n",
    "    return [words_dict[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/brown-Words2Vec-50.ckpt\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  200 :  321.986343002\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('hardship', '0.89'), ('theology', '0.88'), ('lynn', '0.88'), ('masculine', '0.88'), ('bluntly', '0.88'), ('insecticide', '0.88'), ('worldly', '0.88'), ('nails', '0.88'), ('mileage', '0.88'), ('isotopic', '0.88')]\n",
      "decoder_outputs:  ['his', '--', 'by', 'by']\n",
      "decoder_outputs:  ['his', '--', 'by', 'two']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  400 :  211.412623253\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('few', '1.00'), ('seven', '1.00'), ('fifty', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.86'), ('iocsixg', '0.86'), ('hardship', '0.86'), ('danny', '0.86'), ('dirge', '0.86'), ('lynn', '0.86'), ('originally', '0.86'), ('sensibility', '0.86'), (\"burlington's\", '0.86'), ('judith', '0.86')]\n",
      "decoder_outputs:  ['--', 'his', 'his', 'in']\n",
      "decoder_outputs:  ['--', 'his', '--', 'in']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  600 :  164.953567162\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('few', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('darkly', '0.87'), ('purvis', '0.87'), ('outreach', '0.87'), (\"year's\", '0.87'), ('traveling', '0.87'), ('calendars', '0.87'), ('year', '0.87'), ('rejects', '0.87'), ('lalaurie', '0.87'), ('sneaked', '0.87')]\n",
      "decoder_outputs:  ['--', '--', 'out', 'out']\n",
      "decoder_outputs:  ['--', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  800 :  134.351025772\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('few', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('purvis', '0.84'), ('moment', '0.84'), ('freshmen', '0.84'), ('whereof', '0.84'), ('lesson', '0.84'), ('till', '0.84'), ('angie', '0.84'), ('snake', '0.84'), ('studio', '0.84'), ('phrases', '0.84')]\n",
      "decoder_outputs:  ['been', '--', '--', 'his']\n",
      "decoder_outputs:  ['been', '--', 'out', 'his']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  1000 :  115.376979561\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('few', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('moment', '0.84'), ('purvis', '0.84'), ('deathbed', '0.84'), ('freshmen', '0.84'), ('carving', '0.84'), ('till', '0.84'), ('lesson', '0.84'), ('although', '0.84'), ('whereof', '0.84'), ('unless', '0.84')]\n",
      "decoder_outputs:  ['been', '--', '--', '--']\n",
      "decoder_outputs:  ['been', '--', '--', '--']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  1200 :  99.5678835869\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('few', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('moment', '0.86'), ('unless', '0.86'), ('carving', '0.86'), ('till', '0.86'), ('deathbed', '0.86'), ('although', '0.86'), ('dive', '0.86'), ('lesson', '0.86'), ('word', '0.86'), ('highway', '0.86')]\n",
      "decoder_outputs:  ['were', '--', '--', '--']\n",
      "decoder_outputs:  ['were', '--', '--', '--']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  1400 :  85.7550196075\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('few', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('moment', '0.86'), ('unless', '0.86'), ('lesson', '0.86'), ('till', '0.86'), ('dawn', '0.86'), ('although', '0.86'), ('knew', '0.86'), ('rector', '0.86'), ('gibby', '0.86'), ('carving', '0.86')]\n",
      "decoder_outputs:  ['were', 'his', 'his', 'out']\n",
      "decoder_outputs:  ['were', 'his', '--', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  1600 :  75.1049230385\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('few', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('indicated', '0.86'), ('dawn', '0.86'), ('moment', '0.86'), ('although', '0.86'), ('track', '0.86'), ('unless', '0.86'), ('highway', '0.86'), ('victory', '0.86'), ('relaxed', '0.86'), ('listed', '0.86')]\n",
      "decoder_outputs:  ['as', '--', '--', '--']\n",
      "decoder_outputs:  ['as', '--', '--', '--']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  1800 :  68.3928683281\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('four', '1.00'), ('three', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('ten', '1.00'), ('few', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('moment', '0.88'), ('responsibility', '0.88'), ('although', '0.87'), ('executives', '0.87'), ('nowhere', '0.87'), ('while', '0.87'), ('dreams', '0.87'), ('indicated', '0.87'), ('participated', '0.87'), ('synthesis', '0.87')]\n",
      "decoder_outputs:  ['--', '--', '--', 'out']\n",
      "decoder_outputs:  ['--', '--', '--', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  2000 :  62.1889664173\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.87'), ('responsibility', '0.87'), ('but', '0.87'), ('wrongs', '0.87'), ('although', '0.87'), ('helplessness', '0.87'), ('representative', '0.87'), ('except', '0.86'), ('buys', '0.86'), ('eloquent', '0.86')]\n",
      "decoder_outputs:  ['him', '--', 'out', 'out']\n",
      "decoder_outputs:  ['him', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  2200 :  55.5795786428\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.87'), ('wrongs', '0.87'), ('ruptured', '0.87'), ('responsibility', '0.87'), ('convent', '0.87'), ('while', '0.87'), ('shimmy', '0.86'), ('but', '0.86'), ('originated', '0.86'), ('convocation', '0.86')]\n",
      "decoder_outputs:  ['as', '--', 'his', 'out']\n",
      "decoder_outputs:  ['as', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  2400 :  47.2373401785\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.88'), ('but', '0.88'), ('wrongs', '0.88'), ('responsibility', '0.88'), ('seafood', '0.88'), ('tri-state', '0.88'), ('takeoffs', '0.88'), ('originated', '0.88'), ('ruptured', '0.88'), ('shimmy', '0.88')]\n",
      "decoder_outputs:  ['his', '--', 'his', 'out']\n",
      "decoder_outputs:  ['his', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  2600 :  44.4065364838\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('seven', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.90'), ('but', '0.90'), ('wrongs', '0.90'), ('although', '0.90'), ('even', '0.90'), ('while', '0.90'), ('ruptured', '0.90'), ('except', '0.90'), ('entry-limiting', '0.90'), ('exports', '0.90')]\n",
      "decoder_outputs:  ['his', '--', 'his', 'out']\n",
      "decoder_outputs:  ['his', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  2800 :  40.8659912014\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('seven', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.89'), ('but', '0.89'), ('wrongs', '0.89'), ('exports', '0.89'), ('ruptured', '0.89'), ('seafood', '0.89'), ('while', '0.89'), ('entry-limiting', '0.89'), ('girdle', '0.89'), ('although', '0.89')]\n",
      "decoder_outputs:  ['him', '--', 'his', 'out']\n",
      "decoder_outputs:  ['him', '--', 'his', 'his']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  3000 :  38.2152070117\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.88'), ('while', '0.88'), ('wrongs', '0.88'), ('councils', '0.88'), ('exports', '0.88'), ('ruptured', '0.88'), ('israeli', '0.88'), ('but', '0.88'), ('convent', '0.88'), ('delightful', '0.88')]\n",
      "decoder_outputs:  ['--', '--', 'his', 'out']\n",
      "decoder_outputs:  ['--', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  3200 :  33.5086877418\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('seven', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.88'), ('wrongs', '0.88'), ('while', '0.88'), ('ruptured', '0.88'), ('carraway', '0.88'), ('but', '0.88'), ('convent', '0.88'), ('exports', '0.88'), ('awaited', '0.88'), ('delightful', '0.88')]\n",
      "decoder_outputs:  ['--', '--', 'his', 'out']\n",
      "decoder_outputs:  ['--', '--', '--', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  3400 :  29.6218385148\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('seven', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('.', '0.91'), ('wrongs', '0.91'), ('while', '0.91'), ('moodily', '0.90'), ('although', '0.90'), ('awaited', '0.90'), ('but', '0.90'), ('dismay', '0.90'), ('since', '0.90'), ('debut', '0.90')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', '--', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  3600 :  28.1665732455\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.91'), ('while', '0.91'), ('despite', '0.91'), ('although', '0.91'), ('since', '0.91'), ('.', '0.91'), ('dismay', '0.91'), ('conclave', '0.91'), ('time-temperature', '0.91'), ('trembled', '0.91')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  3800 :  27.9410513592\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('five', '1.00'), ('several', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('last', '1.00'), ('many', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.91'), ('while', '0.91'), ('despite', '0.91'), ('stiffened', '0.91'), ('since', '0.91'), ('darkly', '0.91'), ('annapolis', '0.91'), ('moliere', '0.91'), ('arabic', '0.91'), ('trembled', '0.91')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  4000 :  26.0936566448\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.93'), ('.', '0.93'), ('while', '0.93'), ('since', '0.93'), ('although', '0.93'), ('langer', '0.93'), ('telescoped', '0.93'), ('multi-year', '0.93'), ('despite', '0.93'), ('plummer', '0.93')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  4200 :  23.2025393939\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.93'), ('since', '0.93'), ('although', '0.93'), ('while', '0.93'), ('larkin', '0.93'), ('despite', '0.93'), ('.', '0.93'), ('plummer', '0.93'), ('telescoped', '0.93'), ('nor', '0.93')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  4400 :  21.2377338696\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('ten', '1.00'), ('many', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.94'), ('since', '0.94'), ('larkin', '0.94'), ('although', '0.94'), ('during', '0.94'), ('langer', '0.94'), ('judith', '0.94'), ('telescoped', '0.94'), ('multi-year', '0.94'), ('excursions', '0.94')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  4600 :  19.2364352417\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('many', '1.00'), ('last', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('wrongs', '0.94'), ('during', '0.94'), ('on', '0.94'), ('despite', '0.93'), ('conclave', '0.93'), ('vincent', '0.93'), ('initiated', '0.93'), ('deafened', '0.93'), ('caused', '0.93'), ('larkin', '0.93')]\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  4800 :  18.3367944551\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('many', '1.00'), ('other', '1.00'), ('early', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('since', '0.96'), ('apologized', '0.96'), ('until', '0.96'), ('langer', '0.96'), ('tantalizing', '0.96'), ('despite', '0.96'), ('when', '0.96'), ('initiated', '0.96'), ('wrongs', '0.96'), ('hostelries', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  5000 :  17.9147643423\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('many', '1.00'), ('early', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('since', '0.96'), ('despite', '0.96'), ('during', '0.96'), ('tantalizing', '0.96'), ('excursions', '0.96'), ('apologized', '0.96'), ('until', '0.96'), ('initiated', '0.96'), ('on', '0.96'), ('two-fold', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  5200 :  17.3941295934\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('many', '1.00'), ('early', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('during', '0.95'), ('on', '0.95'), ('despite', '0.95'), ('langer', '0.95'), ('two-fold', '0.95'), ('since', '0.95'), ('wrongs', '0.95'), ('extremes', '0.95'), ('larkin', '0.95'), ('initiated', '0.95')]\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  5400 :  15.9036895323\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('early', '1.00'), ('many', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('during', '0.95'), ('extremes', '0.95'), ('on', '0.95'), ('langer', '0.95'), ('despite', '0.95'), ('awfully', '0.95'), ('vividly', '0.95'), ('initiated', '0.95'), ('since', '0.95'), ('pre-war', '0.95')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  5600 :  14.0918527818\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('four', '1.00'), ('several', '1.00'), ('five', '1.00'), ('six', '1.00'), ('many', '1.00'), ('few', '1.00'), ('early', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('awfully', '0.96'), ('superbly', '0.96'), ('langer', '0.96'), ('vividly', '0.96'), ('extremes', '0.96'), ('tantalizing', '0.96'), ('completes', '0.96'), ('maids', '0.96'), ('during', '0.96'), ('loathed', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  5800 :  13.2820977259\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('five', '1.00'), ('many', '1.00'), ('six', '1.00'), ('few', '1.00'), ('early', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('awfully', '0.97'), ('hidden', '0.97'), ('completes', '0.97'), ('vividly', '0.97'), ('superbly', '0.97'), ('extremes', '0.97'), ('stilts', '0.96'), ('langer', '0.96'), ('leyte', '0.96'), ('maids', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "decoder_outputs:  ['out', '--', 'his', 'his']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  6000 :  14.0838781261\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('five', '1.00'), ('six', '1.00'), ('many', '1.00'), ('early', '1.00'), ('few', '1.00'), ('recent', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('extremes', '0.95'), ('completes', '0.95'), ('awfully', '0.95'), ('vividly', '0.95'), ('springs', '0.95'), ('superbly', '0.95'), ('langer', '0.95'), ('tenant', '0.95'), ('bodyguard', '0.95'), ('between', '0.95')]\n",
      "decoder_outputs:  ['his', '--', 'his', 'out']\n",
      "decoder_outputs:  ['his', '--', 'his', 'his']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  6200 :  12.1444535208\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('five', '1.00'), ('early', '1.00'), ('many', '1.00'), ('six', '1.00'), ('few', '1.00'), ('recent', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.96'), ('extremes', '0.96'), ('vividly', '0.96'), ('outdo', '0.96'), ('bodyguard', '0.96'), ('superbly', '0.96'), ('between', '0.96'), ('awfully', '0.96'), ('langer', '0.96'), ('helen', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'his']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  6400 :  11.1810670161\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('five', '1.00'), ('early', '1.00'), ('many', '1.00'), ('six', '1.00'), ('few', '1.00'), ('recent', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('bodyguard', '0.95'), ('completes', '0.95'), ('awfully', '0.95'), ('extremes', '0.95'), ('springs', '0.95'), ('vividly', '0.95'), ('between', '0.95'), (\"marine's\", '0.95'), ('concerning', '0.95'), ('langer', '0.95')]\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "decoder_outputs:  ['out', '--', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  6600 :  12.3260719514\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('early', '1.00'), ('five', '1.00'), ('many', '1.00'), ('six', '1.00'), ('recent', '1.00'), ('few', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.96'), ('awfully', '0.95'), ('extremes', '0.95'), ('bodyguard', '0.95'), ('springs', '0.95'), (\"marine's\", '0.95'), ('vividly', '0.95'), ('tenant', '0.95'), ('rudely', '0.95'), ('superbly', '0.95')]\n",
      "decoder_outputs:  ['out', '--', 'his', 'his']\n",
      "decoder_outputs:  ['out', '--', 'out', 'his']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  6800 :  10.887150898\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('many', '1.00'), ('early', '1.00'), ('five', '1.00'), ('six', '1.00'), ('few', '1.00'), ('recent', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.96'), ('awfully', '0.96'), ('extremes', '0.96'), ('bodyguard', '0.96'), ('superbly', '0.96'), ('vividly', '0.96'), ('springs', '0.96'), ('gloated', '0.96'), (\"marine's\", '0.96'), ('anorexia', '0.96')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  7000 :  10.7554256821\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('six', '1.00'), ('recent', '1.00'), ('global', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.95'), ('bodyguard', '0.95'), ('disarranged', '0.95'), ('seekonk', '0.95'), ('place-kicking', '0.95'), ('springs', '0.95'), ('yang', '0.95'), ('scholarly', '0.95'), ('extremes', '0.95'), ('diamond', '0.95')]\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'his', 'his', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  7200 :  9.91482954979\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('recent', '1.00'), ('six', '1.00'), ('last', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.96'), ('bodyguard', '0.96'), ('disarranged', '0.96'), ('healthily', '0.96'), ('scholarly', '0.96'), ('9-1/2', '0.95'), ('springs', '0.95'), ('golfers', '0.95'), ('place-kicking', '0.95'), ('extremes', '0.95')]\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "decoder_outputs:  ['out', '--', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  7400 :  10.2907693672\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('many', '1.00'), ('early', '1.00'), ('five', '1.00'), ('recent', '1.00'), ('six', '1.00'), ('number', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('bodyguard', '0.96'), ('completes', '0.95'), ('healthily', '0.95'), ('trans-political', '0.95'), ('scholarly', '0.95'), ('9-1/2', '0.95'), ('undoing', '0.95'), ('seekonk', '0.95'), (\"marine's\", '0.95'), ('disarranged', '0.95')]\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  7600 :  9.1333642745\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('recent', '1.00'), ('six', '1.00'), ('140', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('disarranged', '0.95'), ('completes', '0.95'), ('bodyguard', '0.95'), ('healthily', '0.95'), ('undoing', '0.95'), ('scholarly', '0.95'), ('africans', '0.95'), ('9-1/2', '0.95'), ('foundling', '0.95'), ('golfers', '0.95')]\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  7800 :  8.76165156126\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('many', '1.00'), ('early', '1.00'), ('recent', '1.00'), ('five', '1.00'), ('six', '1.00'), ('140', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('disarranged', '0.95'), ('completes', '0.95'), ('healthily', '0.95'), ('scholarly', '0.95'), (\"horse's\", '0.95'), ('deteriorating', '0.95'), ('undoing', '0.95'), ('africans', '0.95'), ('golfers', '0.95'), ('foundling', '0.95')]\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'his', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  8000 :  8.4482821703\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('six', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('disarranged', '0.95'), ('deteriorating', '0.95'), (\"horse's\", '0.95'), ('stub', '0.95'), ('foundling', '0.95'), ('scholarly', '0.95'), ('completes', '0.95'), ('ignoring', '0.95'), ('healthily', '0.95'), ('9-1/2', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  8200 :  8.1869813776\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('140', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('trans-political', '0.95'), ('healthily', '0.95'), ('scholarly', '0.95'), ('undoing', '0.95'), ('completes', '0.95'), ('bodyguard', '0.95'), ('9-1/2', '0.95'), ('laude', '0.95'), ('township', '0.95'), ('geo-political', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  8400 :  7.81377252579\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('many', '1.00'), ('early', '1.00'), ('140', '1.00'), ('other', '1.00'), ('methuselah', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('completes', '0.95'), ('trans-political', '0.95'), ('scholarly', '0.95'), ('healthily', '0.95'), ('disarranged', '0.95'), ('undoing', '0.95'), ('bodyguard', '0.95'), ('laude', '0.95'), ('deteriorating', '0.95'), ('africans', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  8600 :  7.95111898899\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('five', '1.00'), ('many', '1.00'), ('other', '1.00'), ('early', '1.00'), ('six', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('bodyguard', '0.96'), ('trans-political', '0.96'), ('9-1/2', '0.96'), ('deteriorating', '0.96'), ('completes', '0.96'), ('healthily', '0.96'), ('seekonk', '0.96'), ('neo-classicists', '0.96'), ('gallants', '0.96'), ('scholarly', '0.96')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  8800 :  8.25560695887\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('140', '1.00'), ('early', '1.00'), ('many', '1.00'), ('five', '1.00'), ('six', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('healthily', '0.95'), ('9-1/2', '0.94'), ('trans-political', '0.94'), ('scholarly', '0.94'), ('bodyguard', '0.94'), ('threesome', '0.94'), ('township', '0.94'), ('undoing', '0.94'), ('stans', '0.94'), ('disarranged', '0.94')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  9000 :  7.61856283903\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('six', '1.00'), ('early', '1.00'), ('many', '1.00'), ('other', '1.00'), ('140', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('9-1/2', '0.95'), ('trans-political', '0.95'), ('healthily', '0.95'), ('undoing', '0.95'), ('conquete', '0.95'), ('stans', '0.95'), ('deteriorating', '0.95'), ('township', '0.95'), ('bodyguard', '0.95'), ('laude', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  9200 :  7.55270636082\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('other', '1.00'), ('140', '1.00'), ('many', '1.00'), ('early', '1.00'), ('six', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('9-1/2', '0.96'), ('trans-political', '0.96'), ('healthily', '0.96'), ('neo-classicists', '0.96'), ('deteriorating', '0.96'), ('undoing', '0.96'), ('laude', '0.95'), ('township', '0.95'), ('conquete', '0.95'), ('self-consciously', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  9400 :  7.43185105324\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('other', '1.00'), ('140', '1.00'), ('many', '1.00'), ('easiest', '1.00'), ('six', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('9-1/2', '0.96'), ('trans-political', '0.96'), ('deteriorating', '0.96'), ('healthily', '0.96'), ('neo-classicists', '0.96'), ('self-consciously', '0.96'), ('undoing', '0.96'), ('conquete', '0.96'), ('potentiometer', '0.96'), ('laude', '0.96')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  9600 :  6.69416588306\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('140', '1.00'), ('methuselah', '1.00'), ('many', '1.00'), ('easiest', '1.00'), ('other', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('deteriorating', '0.96'), ('9-1/2', '0.96'), ('healthily', '0.96'), ('disarranged', '0.96'), ('parallelism', '0.96'), ('seekonk', '0.96'), ('township', '0.96'), ('scholarly', '0.95'), ('trans-political', '0.95'), ('reflector', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  9800 :  6.40021986008\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('140', '1.00'), ('other', '1.00'), ('many', '1.00'), ('easiest', '1.00'), ('tenderly', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('9-1/2', '0.96'), ('healthily', '0.96'), ('deteriorating', '0.96'), ('trans-political', '0.95'), ('advisability', '0.95'), ('geo-political', '0.95'), ('self-consciously', '0.95'), ('potentiometer', '0.95'), (\"hogan's\", '0.95'), ('respecting', '0.95')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "epouch done...\n",
      "Average loss at step  10000 :  6.80221025944\n",
      "word2vec:  [('two', '1.00'), ('four', '0.94'), ('three', '0.94'), ('five', '0.92'), ('several', '0.90'), ('six', '0.88'), ('ten', '0.88'), ('few', '0.86'), ('seven', '0.83'), ('fifty', '0.82')]\n",
      "encoder output cloest words of \"two\":  [('two', '1.00'), ('three', '1.00'), ('several', '1.00'), ('four', '1.00'), ('recent', '1.00'), ('140', '1.00'), ('other', '1.00'), ('easiest', '1.00'), ('many', '1.00'), ('tenderly', '1.00')]\n",
      "encoder output cloest words of \"good morning\":  [('9-1/2', '0.96'), ('deteriorating', '0.96'), ('healthily', '0.96'), (\"hogan's\", '0.96'), ('trans-political', '0.96'), ('colloquium', '0.96'), ('parroting', '0.96'), ('self-consciously', '0.96'), ('disagreements', '0.96'), ('potentiometer', '0.96')]\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "decoder_outputs:  ['out', 'out', 'out', 'out']\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-cd83a7da43b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEBUG_SIZE = 200\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    #saver.restore(session, MODEL)\n",
    "    \n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "      \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_input_lengths, batch_masks = next(generator)\n",
    "        feed_dict = {inputs: batch_inputs, input_lengths: batch_input_lengths, decoder_masks: batch_masks}\n",
    "\n",
    "        #_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "\n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "                emb = embeddings.eval()\n",
    "                normalize(emb, norm='l2', axis=1, copy=False)\n",
    "                print('word2vec: ', cloestWord2('two', emb))\n",
    "                \n",
    "                dict_list = [[i] for i in range(len(words_dict))]\n",
    "                dict_list_lengths = [1 for i in range(len(words_dict))]\n",
    "                emb = rnn_final_state.eval(feed_dict={inputs: dict_list, input_lengths: dict_list_lengths})\n",
    "                normalize(emb, norm='l2', axis=1, copy=False)\n",
    "                print('encoder output cloest words of \"two\": ', cloestWord2('two', emb))\n",
    "                \n",
    "                vec = rnn_final_state.eval(feed_dict={inputs: [to_word_indices(['good', 'morning'])], input_lengths: [2]})\n",
    "                normalize(vec, norm='l2', axis=1, copy=False)\n",
    "                print('encoder output cloest words of \"good morning\": ', cloestWord3(vec[0], emb))\n",
    "                \n",
    "                feed_dict = {inputs: [\n",
    "                    to_word_indices(['once', 'upon', 'a', 'time']), \n",
    "                    to_word_indices(['this', 'cat', 'is', 'cute']),\n",
    "                ], input_lengths:[4, 4], decoder_masks: [[1,1,1,1], [1,1,1,1]]}\n",
    "\n",
    "                #print('encoder_inputs:', encoder_inputs.eval(feed_dict))\n",
    "                #print('decoder_inputs:', decoder_inputs.eval(feed_dict))\n",
    "                #print('decoder_masks: ', decoder_masks.eval(feed_dict))\n",
    "                d_out = decoder_outputs.rnn_output.eval(feed_dict)\n",
    "                print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[0]])\n",
    "                print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[1]])\n",
    "\n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            # Create the Timeline object, and write it to a json\n",
    "            tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            ctf = tl.generate_chrome_trace_format()\n",
    "            with open('timeline.json', 'w') as f:\n",
    "                f.write(ctf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/brown-Words2Vec-50.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq-autoencoder.ckpt\n",
      "3.92263\n",
      "decoder_outputs:  ['may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may']\n",
      "decoder_outputs:  ['may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may']\n",
      "decoder_outputs:  ['may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may']\n",
      "decoder_outputs:  ['may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may', 'may']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    saver.restore(session, MODEL)\n",
    "    \n",
    "    input_sents = [\n",
    "        to_word_indices(['once', 'upon', 'a', 'time']), \n",
    "        to_word_indices(['this', 'cat', 'is', 'cute']),\n",
    "        to_word_indices(sents[0]),\n",
    "        to_word_indices(sents[1])\n",
    "    ]\n",
    "    \n",
    "    input_sents = batchPadding(input_sents)\n",
    "                      \n",
    "    feed_dict = {inputs: input_sents, input_lengths:[input_sents.shape[1] for i in range(input_sents.shape[0])], decoder_masks: batchMask(input_sents)}\n",
    "    d_out = decoder_outputs.rnn_output.eval(feed_dict)\n",
    "    print(seq_loss.eval(feed_dict))\n",
    "\n",
    "    print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[0]])\n",
    "    print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[1]])\n",
    "    print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[2]])\n",
    "    print('decoder_outputs: ', [inv_words_dict[out.argsort()[::-1][0]] for out in d_out[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
