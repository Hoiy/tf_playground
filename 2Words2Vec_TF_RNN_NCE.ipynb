{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sents = brown.sents()\n",
    "sents = [[token.lower() for token in sent] for sent in sents]\n",
    "words = brown.words()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1161192\n",
      "Number of sentences: 57340\n",
      "Longest sentences length: 180\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens: {}\".format(len(words)))\n",
    "print(\"Number of sentences: {}\".format(len(sents)))\n",
    "print(\"Longest sentences length: {}\".format(max([len(sent) for sent in sents])))\n",
    "MAX_SENTENCE_LENGTH = max([len(sent) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "five\n",
      "Number of unique tokens: 49815\n"
     ]
    }
   ],
   "source": [
    "words_dict, inv_words_dict = words2dict.convert(words)\n",
    "print(words_dict['five'])\n",
    "print(inv_words_dict[334])\n",
    "\n",
    "words_size = len(words_dict)\n",
    "print(\"Number of unique tokens: {}\".format(words_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batchPadding(batch, padding_symbol=words_dict['--']):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dataGenerator(sents, words_dict, window_size = 2, batch_size=32, train_length=2):\n",
    "    train = []\n",
    "    label = []\n",
    "    length = []\n",
    "    while(True):\n",
    "        left_window = [words_dict['--'] for i in range(window_size)]\n",
    "        target = [words_dict['--'] for i in range(train_length)]\n",
    "        right_window = [words_dict['--'] for i in range(window_size)]\n",
    "        for sent in sents:\n",
    "            for word in sent:\n",
    "                right_window.append(words_dict[word])\n",
    "                target.append(right_window.pop(0))\n",
    "                left_window.append(target.pop(0))\n",
    "                left_window.pop(0)\n",
    "                \n",
    "                for context in left_window + right_window:\n",
    "                    train.append(list(target))\n",
    "                    label.append(list([context]))\n",
    "                    length.append(len(target))\n",
    "                    if(len(train) == batch_size):\n",
    "                        yield train, label, length\n",
    "                        train = []\n",
    "                        label = []\n",
    "                        length = []\n",
    "\n",
    "        print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 2\n",
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def visualizeData(generator):\n",
    "    train, label, length = next(generator)\n",
    "    for i in range(len(train)):\n",
    "        print([inv_words_dict[word] for word in train[i]], [inv_words_dict[word] for word in label[i]], length[i])\n",
    "\n",
    "generator = dataGenerator(sents[:1], words_dict, window_size = 1, batch_size=64, train_length=2)\n",
    "#print(sents[0])\n",
    "#visualizeData(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generator = dataGenerator(sents, words_dict, window_size = WINDOW_SIZE, batch_size=BATCH_SIZE, train_length=TRAIN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DIMENSION = 50\n",
    "VOCABULAY_SIZE = len(words_dict)\n",
    "NEGATIVE_SAMPLE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "        inputs = tf.placeholder(tf.int32, (None, None), name = \"Input_Sentence_Word_Index\")\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        input_lengths = tf.placeholder(tf.int32, (None), name = \"Input_Sentence_Length\")\n",
    "        #OUT: (batch) int32\n",
    "        \n",
    "        labels = tf.placeholder(tf.int32, (None, 1), name = \"Context_Word_Index\")\n",
    "        #OUT: (batch, 1) int32\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        embeddings = tf.Variable(tf.zeros([VOCABULAY_SIZE, DIMENSION], tf.float32), trainable=False, name=\"Word2Vec\")\n",
    "        \n",
    "        #IN: (batch, time) int32\n",
    "        inputs_embed = tf.nn.embedding_lookup(embeddings, inputs, max_norm=1)\n",
    "        #OUT: (batch, time, dim) float32\n",
    "\n",
    "        #IN: (batch, time, dim) float32\n",
    "        rnn_inputs = tf.transpose(inputs_embed, [1, 0, 2])\n",
    "        ###rnn_inputs = inputs_embed\n",
    "        #OUT: (time, batch, dim) float32\n",
    "\n",
    "        cell = tf.contrib.rnn.LSTMCell(DIMENSION, state_is_tuple=True)        \n",
    "        #cell = tf.contrib.rnn.GRUCell(DIMENSION)\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        #IN: (time, batch, DIMENSION) float32\n",
    "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=initial_state, sequence_length=input_lengths, time_major=True)\n",
    "        #OUT: (time, batch, DIMENSION) float32\n",
    "        \n",
    "        #IN: (time, batch, DIMENSION) float32\n",
    "        index = tf.range(0, batch_size) * tf.shape(inputs)[1] + (input_lengths - 1)\n",
    "        rnn_final_state = tf.gather(tf.reshape(tf.transpose(rnn_outputs, (1,0,2)), [-1, DIMENSION]), index)\n",
    "        rnn_final_state = tf.clip_by_norm(rnn_final_state, 1, axes=[1])\n",
    "        ###rnn_final_state = tf.gather(tf.reshape(rnn_outputs, [-1, DIMENSION]), index)\n",
    "        #OUT: (batch, DIMENSION)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([VOCABULAY_SIZE, DIMENSION],\n",
    "                                stddev=1.0 / math.sqrt(DIMENSION)), name=\"NCE_Weights\")\n",
    "\n",
    "        nce_biases = tf.Variable(tf.zeros([VOCABULAY_SIZE]), name=\"NCE_Bias\")\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=labels,\n",
    "                     inputs=rnn_final_state,\n",
    "                     num_sampled=NEGATIVE_SAMPLE,\n",
    "                     num_classes=VOCABULAY_SIZE))\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        #optimizer = tf.train.AdamOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        embeddings_saver = tf.train.Saver({'Words2Vec': embeddings})\n",
    "        context = tf.nn.softmax(tf.matmul(rnn_final_state, tf.transpose(nce_weights)) + nce_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "MODEL = './model/2Words2Vec-tf-rnn-nce.ckpt'\n",
    "WORDS2VEC_MODEL = './model/brown-Words2Vec.ckpt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  2000 :  4.96578600144\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  4000 :  4.93373045754\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  6000 :  4.94054861641\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  8000 :  4.91869240141\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  10000 :  4.88599774981\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  12000 :  4.85607321215\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  14000 :  4.89905647182\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  16000 :  4.84841774702\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  18000 :  4.85298632336\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  20000 :  4.93955073524\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  22000 :  4.92474759626\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  24000 :  4.93189296317\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  26000 :  4.90804334211\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  28000 :  4.88174906969\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  30000 :  4.84795752168\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  32000 :  4.88390768147\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  34000 :  4.83274778891\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  36000 :  4.82561535144\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  38000 :  4.93083721519\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  40000 :  4.91624572253\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  42000 :  4.92216085625\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  44000 :  4.89523500824\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  46000 :  4.87330791783\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  48000 :  4.83762719226\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  50000 :  4.86586399436\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  52000 :  4.82246921015\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  54000 :  4.79919913435\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  56000 :  4.93028294039\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  58000 :  4.90931976819\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  60000 :  4.91083092165\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  62000 :  4.8865650115\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  64000 :  4.86930151248\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  66000 :  4.82626918864\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  68000 :  4.85188629127\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  70000 :  4.8169651382\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  72000 :  4.78540005159\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  74000 :  4.91315263939\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  76000 :  4.90140052414\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  78000 :  4.90577457583\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  80000 :  4.87924902868\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  82000 :  4.86899909067\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  84000 :  4.81411607504\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  86000 :  4.83887427139\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  88000 :  4.80958392978\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  90000 :  4.76958044839\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  92000 :  4.90123962855\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  94000 :  4.8949869895\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  96000 :  4.89415567589\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  98000 :  4.87589425993\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  100000 :  4.86891853476\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  102000 :  4.79634371877\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  104000 :  4.82769371867\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  106000 :  4.80570243549\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  108000 :  4.75545870697\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  110000 :  4.88571933174\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  112000 :  4.8938876493\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  114000 :  4.88571957016\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  116000 :  4.87093772912\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  118000 :  4.85692891479\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  120000 :  4.7873805964\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  122000 :  4.82436583686\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  124000 :  4.80108325148\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  126000 :  4.75276468849\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  128000 :  4.8633233217\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  130000 :  4.89012832928\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  132000 :  4.87512711143\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  134000 :  4.8676958828\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  136000 :  4.85145121694\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  138000 :  4.78441615605\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  140000 :  4.81008838582\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  142000 :  4.79328483653\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  144000 :  4.74857837915\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  146000 :  4.84524388599\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  148000 :  4.88569398451\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  150000 :  4.86723840737\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  152000 :  4.86395501471\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  154000 :  4.84531665826\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  156000 :  4.78120102239\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  158000 :  4.80738103199\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  160000 :  4.77206404018\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  162000 :  4.74421244216\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  164000 :  4.8269865669\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  166000 :  4.88116627407\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  168000 :  4.85981742477\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  170000 :  4.86209843159\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  172000 :  4.84183561254\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  174000 :  4.77969254541\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  176000 :  4.80253257465\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  178000 :  4.76469177437\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  180000 :  4.74255009913\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n",
      "Average loss at step  182000 :  4.79997486115\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  184000 :  4.87600194216\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  186000 :  4.8543614893\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  188000 :  4.85855927181\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  190000 :  4.83680589461\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  192000 :  4.77597327006\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  194000 :  4.7935855763\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  196000 :  4.75209643483\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "Average loss at step  198000 :  4.73877091455\n",
      "Model saved in file: ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "epouch done...\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    #init.run()\n",
    "    #embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    saver.restore(session, MODEL)\n",
    "      \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels, batch_input_lengths = next(generator)\n",
    "        feed_dict = {inputs: batch_inputs, labels: batch_labels, input_lengths: batch_input_lengths}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "        if step % 2000 == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/brown-Words2Vec.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/2Words2Vec-tf-rnn-nce.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    #saver.restore(session, MODEL)\n",
    "    embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    final_embeddings = embeddings.eval()\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize(final_embeddings, norm='l2', axis=1, copy=False)\n",
    "\n",
    "words_vec = {}\n",
    "for i in range(final_embeddings.shape[0]):\n",
    "    words_vec[inv_words_dict[i]] = final_embeddings[i]\n",
    "  \n",
    "words_vec2 = {}\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    \n",
    "    for key in words_vec.keys():\n",
    "        feed_dict = {inputs: [[words_dict[key]]], input_lengths: [1]}\n",
    "        words_vec2[key] = normalize(rnn_final_state.eval(feed_dict), norm='l2', copy=True)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sent2Context(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        context_pred = context.eval(feed_dict)\n",
    "        print(rnn_final_state.eval(feed_dict))\n",
    "        return [inv_words_dict[i] for i in context_pred.argsort()[0][::-1][:10]]\n",
    "    \n",
    "def twoWords2Vec(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        return normalize(rnn_final_state.eval(feed_dict), norm='l2', copy=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/2Words2Vec-tf-rnn-nce.ckpt\n",
      "[-0.05923763  0.2136212  -0.14718555  0.06965707  0.00332109  0.07057834\n",
      "  0.05668808 -0.02834846  0.23397775  0.14585446  0.13615161 -0.13666223\n",
      " -0.10390599 -0.1941687  -0.04024365  0.02982403  0.19603655  0.27186435\n",
      " -0.01047328  0.19310986 -0.0632597  -0.16442876 -0.00136202 -0.18368034\n",
      " -0.17697984  0.06736191 -0.04950423  0.21467388 -0.08571327  0.07059266\n",
      "  0.14630686  0.20384529  0.17110957  0.15202926 -0.17488046 -0.18977112\n",
      " -0.11714847  0.0124442   0.15827432 -0.19323823  0.24799895 -0.18233418\n",
      " -0.00799689 -0.09322709  0.0574378  -0.02100265  0.00937227  0.14544383\n",
      "  0.21380867  0.08619152]\n"
     ]
    }
   ],
   "source": [
    "print(twoWords2Vec(['run', 'faster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cloestWord(vec, words_vec, count=10, method='cos'):\n",
    "    if method == 'cos':\n",
    "        dist = np.array([ sum(vec * words_vec[key]) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[::-1][:10]\n",
    "    else:\n",
    "        dist = np.array([ sum(np.square(np.array(vec) - np.array(words_vec[key]))) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[:10]\n",
    "    return [list(words_vec.keys())[i] for i in top_ten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'six', 'st.', 'four', 'three', 'five', 'last', 'other', 'du', 'rhode']\n",
      "['however', 'moreover', 'redder', 'etc.', 'sinful', 'irritability', 'fantasies', 'obstinate', 'sohn', 'isles']\n",
      "['man', 'desired', 'interesting', 'plain', 'identified', 'applying', 'mother', 'long', 'officially', 'wonderful']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndef rnn_out(sent):\\n    with tf.Session(graph=graph) as session:\\n        saver.restore(session, MODEL)\\n        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\\n        print('rnn_inputs: ', rnn_inputs.eval(feed_dict))\\n        print('rnn_outputs: ', rnn_outputs.eval(feed_dict))\\n        print('rnn_final_state:', rnn_final_state.eval(feed_dict))\\n\\nrnn_out(['two'])\\nrnn_out(['three'])\\nwords_vec['two']\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(cloestWord(twoWords2Vec(['run', 'faster'])))\n",
    "#print(cloestWord(twoWords2Vec(['two', 'idiot'])))\n",
    "#print(cloestWord(words_vec['two']))\n",
    "#print(cloestWord(words_vec['but']))\n",
    "#print(cloestWord(words_vec['man']))\n",
    "\n",
    "\n",
    "#print(cloestWord(words_vec2['two'], words_vec2))\n",
    "#print(cloestWord(words_vec2['but'], words_vec2))\n",
    "#print(cloestWord(words_vec2['man'], words_vec2))\n",
    "\n",
    "print(cloestWord(words_vec2['two'], words_vec2))\n",
    "print(cloestWord(words_vec2['however'], words_vec2))\n",
    "print(cloestWord(words_vec2['man'], words_vec2))\n",
    "\n",
    "#print(cloestWord(twoWords2Vec(['but']), words_vec2))\n",
    "#print(cloestWord(twoWords2Vec(['man']), words_vec2))\n",
    "#print(cloestWord(twoWords2Vec(['two'])))\n",
    "\"\"\"\n",
    "def rnn_out(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        print('rnn_inputs: ', rnn_inputs.eval(feed_dict))\n",
    "        print('rnn_outputs: ', rnn_outputs.eval(feed_dict))\n",
    "        print('rnn_final_state:', rnn_final_state.eval(feed_dict))\n",
    "\n",
    "rnn_out(['two'])\n",
    "rnn_out(['three'])\n",
    "words_vec['two']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WORDS2VEC_MODEL = './model/brown-Words2Vec.ckpt'\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "\n",
    "with graph2.as_default():\n",
    "    embeddings2 = tf.Variable(\n",
    "            tf.random_uniform([VOCABULAY_SIZE, DIMENSION], -1.0, 1.0), name='Words2Vec')\n",
    "    embeddings_saver2 = tf.train.Saver({'Words2Vec': embeddings2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/brown-Words2Vec.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph2) as session2:\n",
    "    embeddings_saver2.restore(session2, WORDS2VEC_MODEL)\n",
    "    final_embeddings2 = embeddings2.eval()\n",
    "    \n",
    "final_embeddings2 = normalize(final_embeddings2, norm='l2', axis=1, copy=True)\n",
    "\n",
    "words_vec3 = {}\n",
    "for i in range(final_embeddings2.shape[0]):\n",
    "    words_vec3[inv_words_dict[i]] = final_embeddings2[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'three', 'several', 'four', 'other', 'five', 'six', 'ten', 'each', 'types']\n",
      "['however', 'therefore', 'moreover', 'indeed', 'nevertheless', 'jr.', 'especially', 'etc.', 'finally', 'instance']\n",
      "['man', 'woman', 'person', 'child', 'boy', 'killed', 'girl', 'married', 'shot', 'further']\n"
     ]
    }
   ],
   "source": [
    "print(cloestWord(words_vec3['two'], words_vec3))\n",
    "print(cloestWord(words_vec3['however'], words_vec3))\n",
    "print(cloestWord(words_vec3['man'], words_vec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "five\n"
     ]
    }
   ],
   "source": [
    "print(words_dict['five'])\n",
    "print(inv_words_dict[334])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
