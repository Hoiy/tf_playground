{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"/Users/Shared/data/HN_posts_year_to_Sep_26_2016.csv\"\n",
    "\n",
    "data = pd.read_csv(FILE)\n",
    "data = data[[\"id\", \"title\", \"num_points\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87282 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "title = data[\"title\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(title)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    293119.000000\n",
      "mean         15.025324\n",
      "std          58.504103\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           2.000000\n",
      "75%           4.000000\n",
      "max        5771.000000\n",
      "Name: num_points, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "num_points = data['num_points'].values\n",
    "print(data['num_points'].describe())\n",
    "cur_dict = {}\n",
    "for i in range(0, 6000):\n",
    "    cur_dict[i] = len(num_points[num_points < i]) / len(num_points)\n",
    "\n",
    "data['cur_num_points'] = data['num_points'].apply(lambda x: cur_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    293119.000000\n",
       "mean          0.402988\n",
       "std           0.344124\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.328904\n",
       "75%           0.707586\n",
       "max           0.999997\n",
       "Name: cur_num_points, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cur_num_points'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOOD_THRESHOLD = 100\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "train = data.sample(frac=0.8)\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(df):  \n",
    "    good = df[df[\"num_points\"] >= GOOD_THRESHOLD]\n",
    "    bad = df[df[\"num_points\"] < GOOD_THRESHOLD]\n",
    "    bad = bad.sample(n=good.shape[0])\n",
    "    dt = good.append(bad)\n",
    "    dt = dt.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    num_points = dt[\"num_points\"].values\n",
    "    cur_num_points = dt[\"cur_num_points\"].values\n",
    "\n",
    "    y_train = np.zeros((len(num_points), 2), dtype=int)\n",
    "    y_original = np.zeros((len(num_points)), dtype=int)\n",
    "    for i in range(0, len(num_points)):\n",
    "        y_train[i, 1] = int(num_points[i] >= GOOD_THRESHOLD)\n",
    "        y_train[i, 0] = int(num_points[i] < GOOD_THRESHOLD)\n",
    "        y_original[i] = int(num_points[i] >= GOOD_THRESHOLD)\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(dt[\"title\"])\n",
    "    x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return x_train, y_train, y_original, cur_num_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_full, y_full, y2_full, y_cur = prepareData(data)\n",
    "x_train, y_train, _, _ = prepareData(train)\n",
    "x_test, y_test, _, _ = prepareData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79786025,  0.83267547,  0.99638713, ...,  0.92781771,\n",
       "        0.97819998,  0.70758634])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('/Users/Shared/data/glove.6B/', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Convolution1D, MaxPooling1D, Dense, Flatten, Dropout, Embedding, LSTM,BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, activity_l2\n",
    "\n",
    "def create_baseline():\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    #x = LSTM(64)(embedded_sequences)\n",
    "    x = Convolution1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = MaxPooling1D()(x)\n",
    "    #x = Convolution1D(16, 5, activation='relu')(embedded_sequences)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = MaxPooling1D()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32, init='uniform', activation='relu')(x)\n",
    "    x = Dense(32, init='uniform', activation='relu')(x)\n",
    "    preds = Dense(1)(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='mse',\n",
    "              optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from scipy.stats import describe\n",
    "\n",
    "def validate(model, x_test, y_test):\n",
    "    test_truth = y_test\n",
    "    test_pred = model.predict(x_test)\n",
    "    print(describe(test_truth))\n",
    "    print(describe(test_pred))\n",
    "    print(cur_dict[GOOD_THRESHOLD])\n",
    "    test_pred[test_pred >= cur_dict[GOOD_THRESHOLD]] = 1\n",
    "    test_pred[test_pred < cur_dict[GOOD_THRESHOLD]] = 0\n",
    "    print(describe(test_pred))\n",
    "    precision = precision_score(test_truth, test_pred)\n",
    "    recall = recall_score(test_truth, test_pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping('val_loss', patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    1     2     3 ..., 23074 23076 23077] TEST: [    0     9    10 ..., 23075 23078 23079]\n",
      "Train on 18464 samples, validate on 4616 samples\n",
      "Epoch 1/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1782 - val_loss: 0.1437\n",
      "Epoch 2/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1428 - val_loss: 0.1435\n",
      "Epoch 3/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1413 - val_loss: 0.1473\n",
      "Epoch 4/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1385 - val_loss: 0.1457\n",
      "Epoch 5/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1360 - val_loss: 0.1487\n",
      "Epoch 6/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1329 - val_loss: 0.1508\n",
      "Epoch 7/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1273 - val_loss: 0.1537\n",
      "Epoch 8/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1207 - val_loss: 0.1590\n",
      "Epoch 9/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1137 - val_loss: 0.1646\n",
      "Epoch 10/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1056 - val_loss: 0.1768\n",
      "DescribeResult(nobs=4616, minmax=(0, 1), mean=0.5, variance=0.25005417118093176, skewness=0.0, kurtosis=-2.0)\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.12903117], dtype=float32), array([ 1.50590515], dtype=float32)), mean=array([ 0.75892794], dtype=float32), variance=array([ 0.03249393], dtype=float32), skewness=array([ 0.10973765], dtype=float32), kurtosis=array([ 0.62314844], dtype=float32))\n",
      "0.9606303242027981\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.], dtype=float32), array([ 1.], dtype=float32)), mean=array([ 0.1215338], dtype=float32), variance=array([ 0.10678646], dtype=float32), skewness=array([ 2.31657362], dtype=float32), kurtosis=array([ 3.36651087], dtype=float32))\n",
      "0.509803921569\n",
      "0.123916811092\n",
      "TRAIN: [    0     1     2 ..., 23077 23078 23079] TEST: [    6     7    15 ..., 23070 23071 23072]\n",
      "Train on 18464 samples, validate on 4616 samples\n",
      "Epoch 1/10\n",
      "18464/18464 [==============================] - 5s - loss: 0.1748 - val_loss: 0.1443\n",
      "Epoch 2/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1434 - val_loss: 0.1445\n",
      "Epoch 3/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1409 - val_loss: 0.1439\n",
      "Epoch 4/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1386 - val_loss: 0.1446\n",
      "Epoch 5/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1353 - val_loss: 0.1457\n",
      "Epoch 6/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1294 - val_loss: 0.1493\n",
      "Epoch 7/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1238 - val_loss: 0.1535\n",
      "Epoch 8/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1160 - val_loss: 0.1592\n",
      "Epoch 9/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1056 - val_loss: 0.1635\n",
      "Epoch 10/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.0927 - val_loss: 0.1760\n",
      "DescribeResult(nobs=4616, minmax=(0, 1), mean=0.5, variance=0.25005417118093176, skewness=0.0, kurtosis=-2.0)\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.05343006], dtype=float32), array([ 1.62077606], dtype=float32)), mean=array([ 0.73779839], dtype=float32), variance=array([ 0.04022892], dtype=float32), skewness=array([-0.36885595], dtype=float32), kurtosis=array([ 0.60176921], dtype=float32))\n",
      "0.9606303242027981\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.], dtype=float32), array([ 1.], dtype=float32)), mean=array([ 0.10030329], dtype=float32), variance=array([ 0.09026209], dtype=float32), skewness=array([ 2.661062], dtype=float32), kurtosis=array([ 5.08124733], dtype=float32))\n",
      "0.518358531317\n",
      "0.103986135182\n",
      "TRAIN: [    0     1     2 ..., 23077 23078 23079] TEST: [    5     8    13 ..., 23065 23067 23076]\n",
      "Train on 18464 samples, validate on 4616 samples\n",
      "Epoch 1/10\n",
      "18464/18464 [==============================] - 5s - loss: 0.1894 - val_loss: 0.1438\n",
      "Epoch 2/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1422 - val_loss: 0.1434\n",
      "Epoch 3/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1413 - val_loss: 0.1435\n",
      "Epoch 4/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1402 - val_loss: 0.1446\n",
      "Epoch 5/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1376 - val_loss: 0.1450\n",
      "Epoch 6/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1340 - val_loss: 0.1476\n",
      "Epoch 7/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1305 - val_loss: 0.1509\n",
      "Epoch 8/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1253 - val_loss: 0.1532\n",
      "Epoch 9/10\n",
      "18464/18464 [==============================] - 5s - loss: 0.1195 - val_loss: 0.1584\n",
      "Epoch 10/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1125 - val_loss: 0.1629\n",
      "DescribeResult(nobs=4616, minmax=(0, 1), mean=0.5, variance=0.25005417118093176, skewness=0.0, kurtosis=-2.0)\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.1542698], dtype=float32), array([ 1.40754223], dtype=float32)), mean=array([ 0.67690438], dtype=float32), variance=array([ 0.02357015], dtype=float32), skewness=array([ 0.31153664], dtype=float32), kurtosis=array([ 1.05600834], dtype=float32))\n",
      "0.9606303242027981\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.], dtype=float32), array([ 1.], dtype=float32)), mean=array([ 0.0372617], dtype=float32), variance=array([ 0.03588104], dtype=float32), skewness=array([ 4.88629866], dtype=float32), kurtosis=array([ 21.87591362], dtype=float32))\n",
      "0.53488372093\n",
      "0.0398613518198\n",
      "TRAIN: [    0     2     3 ..., 23076 23078 23079] TEST: [    1     4    11 ..., 23045 23074 23077]\n",
      "Train on 18464 samples, validate on 4616 samples\n",
      "Epoch 1/10\n",
      "18464/18464 [==============================] - 5s - loss: 0.1715 - val_loss: 0.1461\n",
      "Epoch 2/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1429 - val_loss: 0.1442\n",
      "Epoch 3/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1421 - val_loss: 0.1445\n",
      "Epoch 4/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1392 - val_loss: 0.1478\n",
      "Epoch 5/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1368 - val_loss: 0.1473\n",
      "Epoch 6/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1317 - val_loss: 0.1504\n",
      "Epoch 7/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1280 - val_loss: 0.1523\n",
      "Epoch 8/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1219 - val_loss: 0.1602\n",
      "Epoch 9/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1136 - val_loss: 0.1615\n",
      "Epoch 10/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1052 - val_loss: 0.1735\n",
      "DescribeResult(nobs=4616, minmax=(0, 1), mean=0.5, variance=0.25005417118093176, skewness=0.0, kurtosis=-2.0)\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.08988979], dtype=float32), array([ 1.62009621], dtype=float32)), mean=array([ 0.60765505], dtype=float32), variance=array([ 0.02980101], dtype=float32), skewness=array([ 0.27927452], dtype=float32), kurtosis=array([ 1.0796299], dtype=float32))\n",
      "0.9606303242027981\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.], dtype=float32), array([ 1.], dtype=float32)), mean=array([ 0.02491334], dtype=float32), variance=array([ 0.02429793], dtype=float32), skewness=array([ 6.09628534], dtype=float32), kurtosis=array([ 35.16468048], dtype=float32))\n",
      "0.539130434783\n",
      "0.026863084922\n",
      "TRAIN: [    0     1     4 ..., 23077 23078 23079] TEST: [    2     3    12 ..., 23062 23068 23073]\n",
      "Train on 18464 samples, validate on 4616 samples\n",
      "Epoch 1/10\n",
      "18464/18464 [==============================] - 4s - loss: 0.1795 - val_loss: 0.1398\n",
      "Epoch 2/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1441 - val_loss: 0.1395\n",
      "Epoch 3/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1440 - val_loss: 0.1447\n",
      "Epoch 4/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1415 - val_loss: 0.1398\n",
      "Epoch 5/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1397 - val_loss: 0.1411\n",
      "Epoch 6/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1366 - val_loss: 0.1431\n",
      "Epoch 7/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1338 - val_loss: 0.1452\n",
      "Epoch 8/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1295 - val_loss: 0.1478\n",
      "Epoch 9/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1233 - val_loss: 0.1522\n",
      "Epoch 10/10\n",
      "18464/18464 [==============================] - 3s - loss: 0.1179 - val_loss: 0.1586\n",
      "DescribeResult(nobs=4616, minmax=(0, 1), mean=0.5, variance=0.25005417118093176, skewness=0.0, kurtosis=-2.0)\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.17409571], dtype=float32), array([ 1.41277134], dtype=float32)), mean=array([ 0.62176377], dtype=float32), variance=array([ 0.02048853], dtype=float32), skewness=array([ 0.26235551], dtype=float32), kurtosis=array([ 0.87866879], dtype=float32))\n",
      "0.9606303242027981\n",
      "DescribeResult(nobs=4616, minmax=(array([ 0.], dtype=float32), array([ 1.], dtype=float32)), mean=array([ 0.01646447], dtype=float32), variance=array([ 0.0161969], dtype=float32), skewness=array([ 7.59957695], dtype=float32), kurtosis=array([ 55.75357437], dtype=float32))\n",
      "0.605263157895\n",
      "0.0199306759099\n",
      "Precision: 0.54\n",
      "Recall: 0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N = 5\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=N, shuffle=True, random_state=seed)\n",
    "kfold.get_n_splits(x_full, y_full)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "for train_index, test_index in kfold.split(x_full, y2_full):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = x_full[train_index], x_full[test_index]\n",
    "    y_train, y_test = y_full[train_index], y_full[test_index]\n",
    "    y_cur_train, y_cur_test = y_cur[train_index], y_cur[test_index] \n",
    "    y2_full_train, y2_full_test = y2_full[train_index], y2_full[test_index] \n",
    "    model = create_baseline()\n",
    "    model.fit(x_train, y_cur_train, nb_epoch=10, batch_size=128, validation_data=(x_test, y_cur_test), callbacks=[])\n",
    "    p, r = validate(model, x_test, y2_full_test)\n",
    "    precision += p\n",
    "    recall += r\n",
    "    \n",
    "print(\"Precision: %.2f\" % (precision / N))\n",
    "print(\"Recall: %.2f\" % (recall / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
