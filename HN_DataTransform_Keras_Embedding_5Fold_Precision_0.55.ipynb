{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"/Users/Shared/data/HN_posts_year_to_Sep_26_2016.csv\"\n",
    "\n",
    "data = pd.read_csv(FILE)\n",
    "data = data[[\"id\", \"title\", \"num_points\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87282 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "title = data[\"title\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(title)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOOD_THRESHOLD = 5\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "train = data.sample(frac=0.8)\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(df):    \n",
    "    good = df[df[\"num_points\"] >= GOOD_THRESHOLD]\n",
    "    bad = df[df[\"num_points\"] < GOOD_THRESHOLD]\n",
    "    bad = bad.sample(n=good.shape[0])\n",
    "    data = good.append(bad)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    num_points = data[\"num_points\"].values\n",
    "\n",
    "    y_train = np.zeros((len(num_points), 2), dtype=int)\n",
    "    y_original = np.zeros((len(num_points)), dtype=int)\n",
    "    for i in range(0, len(num_points)):\n",
    "        y_train[i, 1] = int(num_points[i] >= GOOD_THRESHOLD)\n",
    "        y_train[i, 0] = int(num_points[i] < GOOD_THRESHOLD)\n",
    "        y_original[i] = int(num_points[i] >= GOOD_THRESHOLD)\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(data[\"title\"])\n",
    "    x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return x_train, y_train, y_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ..., 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "x_full, y_full, y2_full = prepareData(data)\n",
    "x_train, y_train, _ = prepareData(train)\n",
    "x_test, y_test, _ = prepareData(test)\n",
    "\n",
    "print(y2_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('/Users/Shared/data/glove.6B/', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Convolution1D, MaxPooling1D, Dense, Flatten, Dropout, Embedding\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, activity_l2\n",
    "\n",
    "def create_baseline():\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    #x = Convolution1D(16, 5, activation='relu')(embedded_sequences)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = MaxPooling1D()(x)\n",
    "    #x = Convolution1D(16, 5, activation='relu')(embedded_sequences)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = MaxPooling1D()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Flatten()(embedded_sequences)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, init='uniform', activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc', 'precision'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def validate(model, x_test, y_test):\n",
    "    test_truth = np.apply_along_axis(lambda x: np.argmax(x), 1, y_test)\n",
    "    test_pred = model.predict(x_test)\n",
    "    test_pred = np.apply_along_axis(lambda x: np.argmax(x), 1, test_pred)\n",
    "    precision = precision_score(test_truth, test_pred)\n",
    "    recall = recall_score(test_truth, test_pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping('val_precision', patience=1, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ..., 1 1 0]\n",
      "TRAIN: [     0      3      5 ..., 136977 136978 136980] TEST: [     1      2      4 ..., 136976 136979 136981]\n",
      "Train on 109584 samples, validate on 27398 samples\n",
      "Epoch 1/100\n",
      "109584/109584 [==============================] - 13s - loss: 0.6934 - acc: 0.5015 - precision: 0.5015 - val_loss: 0.6929 - val_acc: 0.5070 - val_precision: 0.5070\n",
      "Epoch 2/100\n",
      "109584/109584 [==============================] - 13s - loss: 0.6925 - acc: 0.5109 - precision: 0.5109 - val_loss: 0.6921 - val_acc: 0.5143 - val_precision: 0.5143\n",
      "Epoch 3/100\n",
      "109584/109584 [==============================] - 13s - loss: 0.6917 - acc: 0.5165 - precision: 0.5165 - val_loss: 0.6909 - val_acc: 0.5245 - val_precision: 0.5245\n",
      "Epoch 4/100\n",
      "109584/109584 [==============================] - 14s - loss: 0.6909 - acc: 0.5226 - precision: 0.5226 - val_loss: 0.6907 - val_acc: 0.5234 - val_precision: 0.5234\n",
      "Epoch 5/100\n",
      "109584/109584 [==============================] - 14s - loss: 0.6905 - acc: 0.5264 - precision: 0.5264 - val_loss: 0.6905 - val_acc: 0.5248 - val_precision: 0.5248\n",
      "Epoch 6/100\n",
      "109584/109584 [==============================] - 14s - loss: 0.6902 - acc: 0.5270 - precision: 0.5270 - val_loss: 0.6901 - val_acc: 0.5280 - val_precision: 0.5280\n",
      "Epoch 7/100\n",
      "109584/109584 [==============================] - 15s - loss: 0.6898 - acc: 0.5270 - precision: 0.5270 - val_loss: 0.6900 - val_acc: 0.5275 - val_precision: 0.5275\n",
      "Epoch 8/100\n",
      "109584/109584 [==============================] - 15s - loss: 0.6894 - acc: 0.5278 - precision: 0.5278 - val_loss: 0.6903 - val_acc: 0.5253 - val_precision: 0.5253\n",
      "0.61054313099\n",
      "0.139499233521\n",
      "TRAIN: [     0      1      2 ..., 136978 136979 136981] TEST: [     7     12     14 ..., 136970 136975 136980]\n",
      "Train on 109586 samples, validate on 27396 samples\n",
      "Epoch 1/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6933 - acc: 0.5048 - precision: 0.5048 - val_loss: 0.6928 - val_acc: 0.5145 - val_precision: 0.5145\n",
      "Epoch 2/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6926 - acc: 0.5116 - precision: 0.5116 - val_loss: 0.6920 - val_acc: 0.5173 - val_precision: 0.5173\n",
      "Epoch 3/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6915 - acc: 0.5202 - precision: 0.5202 - val_loss: 0.6909 - val_acc: 0.5195 - val_precision: 0.5195\n",
      "Epoch 4/100\n",
      "109586/109586 [==============================] - 18s - loss: 0.6905 - acc: 0.5237 - precision: 0.5237 - val_loss: 0.6905 - val_acc: 0.5280 - val_precision: 0.5280\n",
      "Epoch 5/100\n",
      "109586/109586 [==============================] - 20s - loss: 0.6901 - acc: 0.5268 - precision: 0.5268 - val_loss: 0.6905 - val_acc: 0.5291 - val_precision: 0.5291\n",
      "Epoch 6/100\n",
      "109586/109586 [==============================] - 16s - loss: 0.6894 - acc: 0.5274 - precision: 0.5274 - val_loss: 0.6901 - val_acc: 0.5299 - val_precision: 0.5299\n",
      "Epoch 7/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6890 - acc: 0.5305 - precision: 0.5305 - val_loss: 0.6903 - val_acc: 0.5246 - val_precision: 0.5246\n",
      "Epoch 8/100\n",
      "109586/109586 [==============================] - 16s - loss: 0.6884 - acc: 0.5302 - precision: 0.5302 - val_loss: 0.6899 - val_acc: 0.5300 - val_precision: 0.5300\n",
      "Epoch 9/100\n",
      "109586/109586 [==============================] - 16s - loss: 0.6880 - acc: 0.5323 - precision: 0.5323 - val_loss: 0.6898 - val_acc: 0.5287 - val_precision: 0.5287\n",
      "Epoch 10/100\n",
      "109586/109586 [==============================] - 16s - loss: 0.6878 - acc: 0.5335 - precision: 0.5335 - val_loss: 0.6903 - val_acc: 0.5262 - val_precision: 0.5262\n",
      "0.526939816899\n",
      "0.512629580961\n",
      "TRAIN: [     0      1      2 ..., 136979 136980 136981] TEST: [     5     18     28 ..., 136955 136961 136973]\n",
      "Train on 109586 samples, validate on 27396 samples\n",
      "Epoch 1/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6932 - acc: 0.5081 - precision: 0.5081 - val_loss: 0.6929 - val_acc: 0.5097 - val_precision: 0.5097\n",
      "Epoch 2/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6924 - acc: 0.5138 - precision: 0.5138 - val_loss: 0.6915 - val_acc: 0.5219 - val_precision: 0.5219\n",
      "Epoch 3/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6916 - acc: 0.5195 - precision: 0.5195 - val_loss: 0.6904 - val_acc: 0.5257 - val_precision: 0.5257\n",
      "Epoch 4/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6903 - acc: 0.5249 - precision: 0.5249 - val_loss: 0.6897 - val_acc: 0.5291 - val_precision: 0.5291\n",
      "Epoch 5/100\n",
      "109586/109586 [==============================] - 14s - loss: 0.6904 - acc: 0.5248 - precision: 0.5248 - val_loss: 0.6896 - val_acc: 0.5251 - val_precision: 0.5251\n",
      "Epoch 6/100\n",
      "109586/109586 [==============================] - 15s - loss: 0.6898 - acc: 0.5262 - precision: 0.5262 - val_loss: 0.6889 - val_acc: 0.5289 - val_precision: 0.5289\n",
      "0.558044210218\n",
      "0.278288801285\n",
      "TRAIN: [     0      1      2 ..., 136979 136980 136981] TEST: [     8     11     21 ..., 136966 136968 136972]\n",
      "Train on 109586 samples, validate on 27396 samples\n",
      "Epoch 1/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6933 - acc: 0.5049 - precision: 0.5049 - val_loss: 0.6928 - val_acc: 0.5126 - val_precision: 0.5126\n",
      "Epoch 2/100\n",
      "109586/109586 [==============================] - 11s - loss: 0.6926 - acc: 0.5111 - precision: 0.5111 - val_loss: 0.6921 - val_acc: 0.5072 - val_precision: 0.5072\n",
      "Epoch 3/100\n",
      "109586/109586 [==============================] - 11s - loss: 0.6916 - acc: 0.5180 - precision: 0.5180 - val_loss: 0.6909 - val_acc: 0.5223 - val_precision: 0.5223\n",
      "Epoch 4/100\n",
      "109586/109586 [==============================] - 11s - loss: 0.6907 - acc: 0.5223 - precision: 0.5223 - val_loss: 0.6916 - val_acc: 0.5174 - val_precision: 0.5174\n",
      "Epoch 5/100\n",
      "109586/109586 [==============================] - 11s - loss: 0.6903 - acc: 0.5247 - precision: 0.5247 - val_loss: 0.6910 - val_acc: 0.5106 - val_precision: 0.5106\n",
      "0.506111647835\n",
      "0.879617462403\n",
      "TRAIN: [     1      2      4 ..., 136979 136980 136981] TEST: [     0      3      6 ..., 136965 136977 136978]\n",
      "Train on 109586 samples, validate on 27396 samples\n",
      "Epoch 1/100\n",
      "109586/109586 [==============================] - 14s - loss: 0.6933 - acc: 0.5091 - precision: 0.5091 - val_loss: 0.6927 - val_acc: 0.5140 - val_precision: 0.5140\n",
      "Epoch 2/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6920 - acc: 0.5160 - precision: 0.5160 - val_loss: 0.6918 - val_acc: 0.5047 - val_precision: 0.5047\n",
      "Epoch 3/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6905 - acc: 0.5238 - precision: 0.5238 - val_loss: 0.6905 - val_acc: 0.5216 - val_precision: 0.5216\n",
      "Epoch 4/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6900 - acc: 0.5273 - precision: 0.5273 - val_loss: 0.6906 - val_acc: 0.5206 - val_precision: 0.5206\n",
      "Epoch 5/100\n",
      "109586/109586 [==============================] - 13s - loss: 0.6894 - acc: 0.5305 - precision: 0.5305 - val_loss: 0.6900 - val_acc: 0.5257 - val_precision: 0.5257\n",
      "Epoch 6/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6889 - acc: 0.5286 - precision: 0.5286 - val_loss: 0.6901 - val_acc: 0.5246 - val_precision: 0.5246\n",
      "Epoch 7/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6884 - acc: 0.5329 - precision: 0.5329 - val_loss: 0.6899 - val_acc: 0.5276 - val_precision: 0.5276\n",
      "Epoch 8/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6880 - acc: 0.5334 - precision: 0.5334 - val_loss: 0.6896 - val_acc: 0.5276 - val_precision: 0.5276\n",
      "Epoch 9/100\n",
      "109586/109586 [==============================] - 12s - loss: 0.6880 - acc: 0.5326 - precision: 0.5326 - val_loss: 0.6900 - val_acc: 0.5256 - val_precision: 0.5256\n",
      "0.527395115842\n",
      "0.491896627245\n",
      "Precision: 0.55\n",
      "Recall: 0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N = 5\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=N, shuffle=True, random_state=seed)\n",
    "kfold.get_n_splits(x_full, y_full)\n",
    "\n",
    "print(y2_full)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "for train_index, test_index in kfold.split(x_full, y2_full):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = x_full[train_index], x_full[test_index]\n",
    "    y_train, y_test = y_full[train_index], y_full[test_index]\n",
    "    model = create_baseline()\n",
    "    model.fit(x_train, y_train, nb_epoch=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[es])\n",
    "    p, r = validate(model, x_test, y_test)\n",
    "    precision += p\n",
    "    recall += r\n",
    "    \n",
    "print(\"Precision: %.2f\" % (precision / N))\n",
    "print(\"Recall: %.2f\" % (recall / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
