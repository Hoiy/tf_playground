{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f-zLlGUTnd5K",
        "colab_type": "code",
        "outputId": "bf6fc505-6c66-494f-b651-c8a759706cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pyarrow pandas sklearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarrow\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/da/79a31cf93dc4b06b51cd840e6b43233ba3a5ef2b9b5dd1d7976d6be89246/pyarrow-0.11.1-cp35-cp35m-manylinux1_x86_64.whl (11.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.6MB 120kB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/d4/6e9c56a561f1d27407bf29318ca43f36ccaa289271b805a30034eb3a8ec4/pandas-0.23.4-cp35-cp35m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.7MB 157kB/s \n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
            "Collecting six>=1.0.0 (from pyarrow)\n",
            "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting numpy>=1.14 (from pyarrow)\n",
            "  Downloading https://files.pythonhosted.org/packages/86/04/bd774106ae0ae1ada68c67efe89f1a16b2aa373cc2db15d974002a9f136d/numpy-1.15.4-cp35-cp35m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.8MB 100kB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.5.0 (from pandas)\n",
            "  Using cached https://files.pythonhosted.org/packages/74/68/d87d9b36af36f44254a8d512cbfc48369103a3b9e474be9bdfe536abfc45/python_dateutil-2.7.5-py2.py3-none-any.whl\n",
            "Collecting pytz>=2011k (from pandas)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/0e/2365ddc010afb3d79147f1dd544e5ee24bf4ece58ab99b16fbb465ce6dc0/pytz-2018.7-py2.py3-none-any.whl (506kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 2.8MB/s \n",
            "\u001b[?25hCollecting scikit-learn (from sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/18/d9/bea927c86bf78d583d517f24cbc87606cb333bfb3a5c99cb85b547305f0f/scikit_learn-0.20.2-cp35-cp35m-manylinux1_x86_64.whl (5.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.3MB 269kB/s \n",
            "\u001b[?25hCollecting scipy>=0.13.3 (from scikit-learn->sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/19/c0ad5b9183ef97030edd6297d1726525ff2c369a09fbb6ea52a1e616ffd6/scipy-1.2.0-cp35-cp35m-manylinux1_x86_64.whl (26.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 26.5MB 47kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
            "  Running setup.py bdist_wheel for sklearn ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /home/Hoiy/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
            "Successfully built sklearn\n",
            "Installing collected packages: six, numpy, pyarrow, python-dateutil, pytz, pandas, scipy, scikit-learn, sklearn\n",
            "Successfully installed numpy-1.15.4 pandas-0.23.4 pyarrow-0.11.1 python-dateutil-2.7.5 pytz-2018.7 scikit-learn-0.20.2 scipy-1.2.0 six-1.12.0 sklearn-0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YVVMjUu8dH04",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!sudo apt-get install unzip\n",
        "!wget http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip\n",
        "!unzip icwb2-data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_M-EQPcRnnV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbfc2ujRnxBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tokenization\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2DIHh1CnsXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BERT_PRETRAINED_DIR = 'gs://dev-test-bert-tpu/chinese_bert/'\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "BERT_MODEL = 'chinese_L-12_H-768_A-12'\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpGbR2Pzu3Gh",
        "colab_type": "code",
        "outputId": "9ea2e7e2-ffa0-425d-f734-485732dbb324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./icwb2-data/training/pku_training.utf8', header=None)\n",
        "display(data.head())\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "train.shape, test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                   0\n",
              "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...\n",
              "1                      中共中央  总书记  、  国家  主席  江  泽民  \n",
              "2                           （  一九九七年  十二月  三十一日  ）  \n",
              "3  １２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...\n",
              "4           同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>中共中央  总书记  、  国家  主席  江  泽民</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>（  一九九七年  十二月  三十一日  ）</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>１２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15243, 1), (3811, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yh6a_7JDz-Ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "025e60c7-4e10-4833-f8da-15deaf3d115a"
      },
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "      \n",
        "def get_train_examples():\n",
        "  examples = []\n",
        "  \n",
        "  def append_example(row):   \n",
        "    examples.append(InputExample(row.name, row[0]))\n",
        "\n",
        "  train.apply(append_example, axis=1)\n",
        "  return examples\n",
        "\n",
        "\n",
        "def get_test_examples():\n",
        "  examples = []\n",
        "  def append_example(row):\n",
        "    examples.append(InputExample(row.name, row[0]))\n",
        "  \n",
        "  test.apply(append_example, axis=1)\n",
        "  return examples\n",
        "\n",
        "\n",
        "train_examples = get_train_examples()\n",
        "test_examples = get_test_examples()\n",
        "train_examples[:3], len(train_examples)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<__main__.InputExample at 0x7fd84f44bc50>,\n",
              "  <__main__.InputExample at 0x7fd84f44bc88>,\n",
              "  <__main__.InputExample at 0x7fd84f44bcc0>],\n",
              " 15243)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "95m6kNRU10rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dc939c8e-05cf-4808-d007-9769e497ab4b"
      },
      "cell_type": "code",
      "source": [
        "l = list(filter(None, train_examples[5].text_a.split(' ')))\n",
        "print(''.join(l))\n",
        "t = ['１' if i < len(token) - 1 else '在' for token in l for i in range(len(token))]\n",
        "print(''.join(t))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在１９９８年来临之际，我十分高兴地通过中央人民广播电台、中国国际广播电台和中央电视台，向全国各族人民，向香港特别行政区同胞、澳门和台湾同胞、海外侨胞，向世界各国的朋友们，致以诚挚的问候和良好的祝愿！\n",
            "在１１１１在１在１在在在１在１在在１在１在１在１在１在在１在１在１在１在在１在１１在在在１在１在１在在在１在１在１１在１在在１在在１在１在在１在１在在在１在１在在１在在在１在１在在１在在１在在１在在\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k25CdQKJ0LbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # For char count frequency\n",
        "# keras_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, oov_token='<oov>', split='')\n",
        "# keras_tokenizer.fit_on_texts(train.main_content)\n",
        "# len(keras_tokenizer.index_word), keras_tokenizer.word_index['<oov>']\n",
        "\n",
        "# choice = []\n",
        "# p = []\n",
        "\n",
        "# for k, v in keras_tokenizer.word_counts.items():\n",
        "#   if k in tokenizer.vocab:\n",
        "#     choice.append(k)\n",
        "#     p.append(v)\n",
        "\n",
        "# p = p / np.sum(p)\n",
        "\n",
        "# choice[:10], p[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DvpQOd0NybbA",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "6e3dbaa4-18d2-4ad3-f98c-a2ff97d1465a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1315
        }
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               truths,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.is_real_example = is_real_example\n",
        "    self.truths = truths\n",
        "\n",
        "    \n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "def convert_single_example(ex_index, example, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "  \n",
        "  text = list(filter(None, example.text_a.split(' ')))\n",
        "  tokens_a_truth = [0. if i < len(token) - 1 else 1. for token in text for i in range(len(token))]\n",
        "  text_a = ''.join(text)\n",
        "    \n",
        "  assert len(text_a) == len(tokens_a_truth)\n",
        "  \n",
        "  tokens_a = tokenizer.tokenize(text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  truths = []\n",
        "  \n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(1.)\n",
        "  \n",
        "  for token, truth in zip(tokens_a, tokens_a_truth):\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(truth)\n",
        "    \n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(1.)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "    truths.append(1.)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(0.)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "  assert len(truths) == max_seq_length\n",
        "\n",
        "  if ex_index < 3:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"truths: %s\" % \" \".join([str(x) for x in truths]))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      truths=truths\n",
        "  )\n",
        "  return feature\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 1000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features  \n",
        "\n",
        "print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "\n",
        "MAX_SEQ_LENGTH = 128 #@param {'type': 'number'}\n",
        "\n",
        "train_features = convert_examples_to_features(train_examples, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = convert_examples_to_features(test_examples, MAX_SEQ_LENGTH, tokenizer)\n",
        "train_features[:10], len(train_features)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...\n",
            "INFO:tensorflow:Writing example 0 of 15243\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 0\n",
            "INFO:tensorflow:tokens: [CLS] 迈 向 充 满 希 望 的 新 世 纪 [UNK] [UNK] 一 九 九 八 年 新 年 讲 话 （ 附 图 片 １ 张 ） [SEP]\n",
            "INFO:tensorflow:input_ids: 101 6815 1403 1041 4007 2361 3307 4638 3173 686 5279 100 100 671 736 736 1061 2399 3173 2399 6382 6413 8020 7353 1745 4275 8029 2476 8021 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 1\n",
            "INFO:tensorflow:tokens: [CLS] 中 共 中 央 总 书 记 、 国 家 主 席 江 泽 民 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 704 1066 704 1925 2600 741 6381 510 1744 2157 712 2375 3736 3813 3696 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 2\n",
            "INFO:tensorflow:tokens: [CLS] （ 一 九 九 七 年 十 二 月 三 十 一 日 ） [SEP]\n",
            "INFO:tensorflow:input_ids: 101 8020 671 736 736 673 2399 1282 753 3299 676 1282 671 3189 8021 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:Writing example 1000 of 15243\n",
            "INFO:tensorflow:Writing example 2000 of 15243\n",
            "INFO:tensorflow:Writing example 3000 of 15243\n",
            "INFO:tensorflow:Writing example 4000 of 15243\n",
            "INFO:tensorflow:Writing example 5000 of 15243\n",
            "INFO:tensorflow:Writing example 6000 of 15243\n",
            "INFO:tensorflow:Writing example 7000 of 15243\n",
            "INFO:tensorflow:Writing example 8000 of 15243\n",
            "INFO:tensorflow:Writing example 9000 of 15243\n",
            "INFO:tensorflow:Writing example 10000 of 15243\n",
            "INFO:tensorflow:Writing example 11000 of 15243\n",
            "INFO:tensorflow:Writing example 12000 of 15243\n",
            "INFO:tensorflow:Writing example 13000 of 15243\n",
            "INFO:tensorflow:Writing example 14000 of 15243\n",
            "INFO:tensorflow:Writing example 15000 of 15243\n",
            "INFO:tensorflow:Writing example 0 of 3811\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 15243\n",
            "INFO:tensorflow:tokens: [CLS] 从 １ ##９ ##９ ##２ 年 人 民 出 版 社 成 立 策 划 室 并 由 方 鸣 任 主 任 开 始 ， 作 为 出 版 策 划 的 倡 言 者 ， 方 鸣 搞 了 一 系 列 的 出 版 策 划 行 动 ， 其 中 首 推 最 近 几 年 陆 续 出 版 的 令 出 版 界 和 读 书 界 为 之 振 奋 的 《 东 方 书 林 之 旅 》 。 这 套 由 ６ 个 书 系 、 ２４ 种 学 术 文 化 精 品 图 书 组 成 的 系 列 丛 书 ， 构 成 了 近 年 来 中 国 图 书 市 场 最 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 794 8029 9174 9174 8929 2399 782 3696 1139 4276 4852 2768 4989 5032 1153 2147 2400 4507 3175 7885 818 712 818 2458 1993 8024 868 711 1139 4276 5032 1153 4638 956 6241 5442 8024 3175 7885 3018 749 671 5143 1154 4638 1139 4276 5032 1153 6121 1220 8024 1071 704 7674 2972 3297 6818 1126 2399 7355 5330 1139 4276 4638 808 1139 4276 4518 1469 6438 741 4518 711 722 2920 1939 4638 517 691 3175 741 3360 722 3180 518 511 6821 1947 4507 8034 702 741 5143 510 12620 4905 2110 3318 3152 1265 5125 1501 1745 741 5299 2768 4638 5143 1154 690 741 8024 3354 2768 749 6818 2399 3341 704 1744 1745 741 2356 1767 3297 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 15244\n",
            "INFO:tensorflow:tokens: [CLS] 说 起 《 东 方 书 林 之 旅 》 ， 方 鸣 一 往 情 深 。 ６ 个 书 系 分 别 以 《 满 江 红 》 、 《 清 平 乐 》 、 《 西 江 月 》 、 《 采 桑 子 》 、 《 如 梦 令 》 、 《 一 剪 梅 》 等 ６ 个 词 牌 作 为 丛 书 书 名 ， 并 将 一 首 用 该 词 牌 填 写 的 词 印 在 封 面 上 。 方 鸣 说 ， 他 追 求 的 是 古 典 人 文 精 神 与 当 代 人 心 灵 间 的 感 应 。 以 新 一 代 青 年 学 者 为 创 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 6432 6629 517 691 3175 741 3360 722 3180 518 8024 3175 7885 671 2518 2658 3918 511 8034 702 741 5143 1146 1166 809 517 4007 3736 5273 518 510 517 3926 2398 727 518 510 517 6205 3736 3299 518 510 517 7023 3433 2094 518 510 517 1963 3457 808 518 510 517 671 1198 3449 518 5023 8034 702 6404 4277 868 711 690 741 741 1399 8024 2400 2199 671 7674 4500 6421 6404 4277 1856 1091 4638 6404 1313 1762 2196 7481 677 511 3175 7885 6432 8024 800 6841 3724 4638 3221 1367 1073 782 3152 5125 4868 680 2496 807 782 2552 4130 7313 4638 2697 2418 511 809 3173 671 807 7471 2399 2110 5442 711 1158 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 15245\n",
            "INFO:tensorflow:tokens: [CLS] 方 鸣 认 为 ， 策 划 是 对 图 书 进 行 整 体 包 装 ， 不 单 单 是 指 书 稿 内 容 和 装 帧 设 计 ， 还 包 括 市 场 等 。 他 认 为 ， 图 书 也 像 人 穿 衣 一 样 ， 不 仅 料 子 要 好 ， 还 要 做 工 精 致 。 只 有 追 求 精 致 、 精 道 、 精 当 、 精 美 ， 才 能 反 映 出 书 的 内 在 价 值 。 譬 如 《 东 方 书 林 之 旅 》 ， 首 次 采 用 黄 色 胶 板 纸 作 为 内 瓤 ， 柔 和 的 米 黄 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 3175 7885 6371 711 8024 5032 1153 3221 2190 1745 741 6822 6121 3146 860 1259 6163 8024 679 1296 1296 3221 2900 741 4943 1079 2159 1469 6163 2373 6392 6369 8024 6820 1259 2886 2356 1767 5023 511 800 6371 711 8024 1745 741 738 1008 782 4959 6132 671 3416 8024 679 788 3160 2094 6206 1962 8024 6820 6206 976 2339 5125 5636 511 1372 3300 6841 3724 5125 5636 510 5125 6887 510 5125 2496 510 5125 5401 8024 2798 5543 1353 3216 1139 741 4638 1079 1762 817 966 511 6357 1963 517 691 3175 741 3360 722 3180 518 8024 7674 3613 7023 4500 7942 5682 5540 3352 5291 868 711 1079 4481 8024 3382 1469 4638 5101 7942 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 1.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 0.0 1.0\n",
            "INFO:tensorflow:Writing example 1000 of 3811\n",
            "INFO:tensorflow:Writing example 2000 of 3811\n",
            "INFO:tensorflow:Writing example 3000 of 3811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<__main__.InputFeatures at 0x7fd84f181d68>,\n",
              "  <__main__.InputFeatures at 0x7fd84f181eb8>,\n",
              "  <__main__.InputFeatures at 0x7fd84f181ba8>,\n",
              "  <__main__.InputFeatures at 0x7fd84f181da0>,\n",
              "  <__main__.InputFeatures at 0x7fd84f181fd0>,\n",
              "  <__main__.InputFeatures at 0x7fd84f181f98>,\n",
              "  <__main__.InputFeatures at 0x7fd84f199160>,\n",
              "  <__main__.InputFeatures at 0x7fd84f199208>,\n",
              "  <__main__.InputFeatures at 0x7fd84f1991d0>,\n",
              "  <__main__.InputFeatures at 0x7fd84f199128>],\n",
              " 15243)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "oaLMQIY_1Ks2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p dataset\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('./dataset/train_features_%s.pkl'%MAX_SEQ_LENGTH, 'wb') as f:\n",
        "  pickle.dump(train_features, f)\n",
        "\n",
        "  \n",
        "with open('./dataset/test_features_%s.pkl'%MAX_SEQ_LENGTH, 'wb') as f:\n",
        "  pickle.dump(test_features, f)\n",
        "\n",
        "# !gsutil rsync ./dataset gs://dev-test-bert-tpu/dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3iHB8z8D034",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}