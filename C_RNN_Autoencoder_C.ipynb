{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sents = brown.sents()\n",
    "sents = [[token.lower() for token in sent] for sent in sents]\n",
    "words = brown.words()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1161192\n",
      "Number of sentences: 57340\n",
      "Longest sentences length: 180\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens: {}\".format(len(words)))\n",
    "print(\"Number of sentences: {}\".format(len(sents)))\n",
    "print(\"Longest sentences length: {}\".format(max([len(sent) for sent in sents])))\n",
    "MAX_SENTENCE_LENGTH = max([len(sent) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "five\n",
      "Number of unique tokens: 49815\n"
     ]
    }
   ],
   "source": [
    "words_dict, inv_words_dict = words2dict.convert(words)\n",
    "print(words_dict['five'])\n",
    "print(inv_words_dict[334])\n",
    "\n",
    "words_size = len(words_dict)\n",
    "print(\"Number of unique tokens: {}\".format(words_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batchPadding(batch, padding_symbol=words_dict['--']):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dataGenerator(sents, words_dict, window_size = 2, batch_size=32, train_length=2, epouch=-1):\n",
    "    train = []\n",
    "    label = []\n",
    "    length = []\n",
    "    while(epouch < 0 or epouch > 0):\n",
    "        left_window = [words_dict['--'] for i in range(window_size)]\n",
    "        target = [words_dict['--'] for i in range(train_length)]\n",
    "        right_window = [words_dict['--'] for i in range(window_size)]\n",
    "        for sent in sents:\n",
    "            for word in sent:\n",
    "                right_window.append(words_dict[word])\n",
    "                target.append(right_window.pop(0))\n",
    "                left_window.append(target.pop(0))\n",
    "                left_window.pop(0)\n",
    "                \n",
    "                for context in left_window + right_window:\n",
    "                    train.append(list(target))\n",
    "                    label.append(list([context]))\n",
    "                    length.append(len(target))\n",
    "                    if(len(train) == batch_size):\n",
    "                        yield train, label, length\n",
    "                        train = []\n",
    "                        label = []\n",
    "                        length = []\n",
    "        epouch -= 1\n",
    "        print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 1\n",
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generator = dataGenerator(sents, words_dict, window_size = WINDOW_SIZE, batch_size=BATCH_SIZE, train_length=TRAIN_LENGTH, epouch=1)\n",
    "#bigram_list = []\n",
    "#for batch_inputs, _, _ in generator:\n",
    "#    bigram_list += ['_'.join([inv_words_dict[idx] for idx in batch_input]) for batch_input in batch_inputs]\n",
    "        \n",
    "#bigrams_dict, inv_bigrams_dict = words2dict.convert(bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def visualizeData(generator):\n",
    "    train, label, length = next(generator)\n",
    "    for i in range(len(train)):\n",
    "        print([inv_words_dict[word] for word in train[i]], [inv_words_dict[word] for word in label[i]], length[i])\n",
    "\n",
    "generator = dataGenerator(sents[:1], words_dict, window_size = 1, batch_size=64, train_length=2)\n",
    "#print(sents[0])\n",
    "#visualizeData(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = dataGenerator(sents, words_dict, window_size = WINDOW_SIZE, batch_size=BATCH_SIZE, train_length=TRAIN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RNN_DIMENSION = [50]\n",
    "RNN_LAYERS = 1\n",
    "DIMENSION = 50\n",
    "VOCABULAY_SIZE = len(words_dict)\n",
    "NEGATIVE_SAMPLE = 128\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "        #IN\n",
    "        inputs = tf.placeholder(tf.int32, (None, None), name = \"Input_Sentence_Word_Index\")\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        #IN\n",
    "        input_lengths = tf.placeholder(tf.int32, (None), name = \"Input_Sentence_Length\")\n",
    "        #OUT: (batch) int32\n",
    "                \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        encoder_inputs = inputs\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        #decoder_inputs = tf.pad(tf.slice(encoder_inputs, [0,1], [batch_size, steps-1]), [[1, 0], [0, 0]])\n",
    "        decoder_inputs = tf.pad(tf.slice(encoder_inputs, [0,1], [batch_size, steps-1]), [[0, 0], [1, 0]])\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        decoder_input_lengths = [TRAIN_LENGTH for i in range(BATCH_SIZE)]\n",
    "        #OUT: (batch) int32\n",
    "\n",
    "        # assume same input length\n",
    "        decoder_masks = tf.ones((batch_size, steps), dtype=tf.float32, name=None)\n",
    "        #OUT: (batch, time)\n",
    "        \n",
    "        #labels = tf.placeholder(tf.int32, (None, 1), name = \"Context_Word_Index\")\n",
    "        #OUT: (batch, 1) int32\n",
    "        \n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([VOCABULAY_SIZE, DIMENSION], -1.0, 1.0), trainable=False, name=\"Word2Vec\")\n",
    "        \n",
    "        #IN: (batch, time) int32\n",
    "        inputs_embed = tf.nn.embedding_lookup(embeddings, inputs, max_norm=1)\n",
    "        #OUT: (batch, time, dim) float32\n",
    "        decoder_embed = tf.nn.embedding_lookup(embeddings, decoder_inputs, max_norm=1)\n",
    "\n",
    "        \"\"\"\n",
    "        #IN: (batch, time, dim) \n",
    "        #weights = [tf.Variable(tf.random_uniform([DIMENSION * 2, DIMENSION], minval=-1, maxval=1)) for i in range(RNN_LAYERS)]\n",
    "        #bias = [tf.Variable(tf.random_uniform([DIMENSION], minval=-1, maxval=1)) for i in range(RNN_LAYERS)]\n",
    "        \n",
    "        #nn_input = tf.reshape(inputs_embed, (batch_size, DIMENSION))\n",
    "        #nn_output = tf.ones((batch_size, DIMENSION))\n",
    "        #for i in range(RNN_LAYERS):\n",
    "        #    nn_output = tf.concat([nn_output, nn_input], 1)\n",
    "        #    nn_output = tf.tanh(tf.matmul(nn_output, weights[i]) + bias[i])\n",
    "        #out: (batch, DIMENSION)\n",
    "\n",
    "        #IN: (batch, time, dim) float32\n",
    "        #rnn_inputs = tf.transpose(inputs_embed, [1, 0, 2])\n",
    "        #rnn_inputs = inputs_embed\n",
    "        #OUT: (batch, time, dim) float32\n",
    "        \n",
    "        #cell = tf.contrib.rnn.LSTMCell(DIMENSION)\n",
    "        cell = tf.contrib.rnn.GRUCell(DIMENSION)\n",
    "        \n",
    "        outputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "            encoder_inputs = encoder_inputs,\n",
    "            decoder_inputs = decoder_inputs,\n",
    "            cell=cell,\n",
    "            num_encoder_symbols = VOCABULAY_SIZE,\n",
    "            num_decoder_symbols = VOCABULAY_SIZE,\n",
    "            embedding_size = DIMENSION,\n",
    "            output_projection=(\n",
    "                tf.Variable(tf.random_iniform([DIMENSION, VOCABULAY_SIZE], minval=-0.1 , maxval=0.1)),\n",
    "                tf.Variable(tf.random_iniform([VOCABULAY_SIZE], minval=-0.1 , maxval=0.1))\n",
    "            ),\n",
    "            feed_previous=False,\n",
    "        )\n",
    "        \n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            logits=outputs,\n",
    "            targets=labels,\n",
    "            weights,\n",
    "            average_across_timesteps=True,\n",
    "            average_across_batch=True,\n",
    "            softmax_loss_function=None,\n",
    "            name=None\n",
    "        )\n",
    "        \"\"\"\n",
    "        # OUT: [time [batch_size, DIMENSION]]\n",
    "        \n",
    "        \n",
    "        ###### IN: (batch, time, DIMENSION) float32 ######\n",
    "        \n",
    "        with tf.variable_scope(\"Encoder\") as encoder_scope:\n",
    "\n",
    "            cell = tf.contrib.rnn.LSTMCell(RNN_DIMENSION[-1])        \n",
    "            #stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(RNN_DIMENSION[i]) for i in range(RNN_LAYERS)])\n",
    "\n",
    "            #rnn_tuple_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(l[i][0], l[i][1]) for i in range(RNN_LAYERS)])\n",
    "\n",
    "            #cell = tf.contrib.rnn.LSTMCell(DIMENSION, state_is_tuple=True)        \n",
    "            #cell = tf.contrib.rnn.GRUCell(DIMENSION)\n",
    "            initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "            rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs_embed, initial_state=initial_state, sequence_length=input_lengths)\n",
    "            \n",
    "            #IN: (batch, time, RNN_DIMENSION[-1]) float32\n",
    "            index = tf.range(0, batch_size) * tf.shape(inputs)[1] + (input_lengths - 1)\n",
    "            rnn_final_state = tf.gather(tf.reshape(rnn_outputs, [-1, RNN_DIMENSION[-1]]), index)\n",
    "            #rnn_final_state = tf.clip_by_norm(rnn_final_state, 1, axes=[1])\n",
    "            ###rnn_final_state = tf.gather(tf.reshape(rnn_outputs, [-1, DIMENSION]), index)\n",
    "            #OUT: (batch, RNN_DIMENSION[-1])\n",
    "                    \n",
    "        ###### OUT: (batch, time, RNN_DIMENSION[-1]) float32 ######\n",
    "        \n",
    "        \n",
    "        ###### IN: (batch, time, RNN_DIMENSION[-1]) float32 ######\n",
    "        \n",
    "        with tf.variable_scope(\"Decoder\") as decoder_scope:\n",
    "            cell = tf.contrib.rnn.LSTMCell(RNN_DIMENSION[-1])   \n",
    "            \n",
    "            if MODE == \"train\":\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs=decoder_embed,\n",
    "                    sequence_length=decoder_input_lengths)\n",
    "                \n",
    "            elif MODE == \"infer\":\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=embedding,\n",
    "                    start_tokens=tf.tile([GO_SYMBOL], [batch_size]),\n",
    "                    end_token=END_SYMBOL)\n",
    "\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=cell,\n",
    "                helper=helper,\n",
    "                initial_state=rnn_states,\n",
    "                output_layer=layers_core.Dense(VOCABULAY_SIZE, use_bias=True, activation=tf.nn.softmax))\n",
    "\n",
    "            decoder_outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=decoder_outputs.rnn_output,\n",
    "                targets=inputs,\n",
    "                weights=decoder_masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True,\n",
    "                softmax_loss_function=None,\n",
    "                name=None\n",
    "            ))\n",
    "\n",
    "            \"\"\"\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs, decoder_input_lengths)\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, rnn_final_state)\n",
    "        \n",
    "            decoder_outputs, decoder_states = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n",
    "            print(decoder_outputs)\n",
    "            #rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs_embed, initial_state=rnn_final_state, sequence_length=input_lengths)\n",
    "            \n",
    "            #weights: A 2D Tensor of shape [batch_size x sequence_length] and dtype float. Weights constitutes the weighting of each prediction in the sequence. When using weights as masking set all valid timesteps to 1 and all padded timesteps to 0.\n",
    "            loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=decoder_outputs,\n",
    "                targets=labels,\n",
    "                weights=decoder_masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True,\n",
    "                softmax_loss_function=None,\n",
    "                name=None\n",
    "            ))\n",
    "            \"\"\"\n",
    "        \"\"\"\n",
    "        #IN: (batch, DIMENSION)\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([VOCABULAY_SIZE, DIMENSION],\n",
    "                                stddev=1.0 / math.sqrt(DIMENSION)), name=\"NCE_Weights\", trainable=True)\n",
    "\n",
    "        nce_biases = tf.Variable(tf.zeros([VOCABULAY_SIZE]), name=\"NCE_Biases\", trainable=True)\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=labels,\n",
    "                     inputs=rnn_final_state,\n",
    "                     num_sampled=NEGATIVE_SAMPLE,\n",
    "                     num_classes=VOCABULAY_SIZE))\n",
    "        \n",
    "        \"\"\"\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        optimizer = tf.train.MomentumOptimizer(1.0, 0.5).minimize(loss)\n",
    "        #optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        embeddings_saver = tf.train.Saver({'Words2Vec': embeddings})\n",
    "        #context = tf.nn.softmax(tf.matmul(rnn_final_state, tf.transpose(nce_weights)) + nce_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show_graph(graph.as_graph_def())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "MODEL = './model/seq2seq-autoencoder.ckpt'\n",
    "WORDS2VEC_MODEL = './model/brown-Words2Vec-{}.ckpt'.format(DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cloestWord(vec, words_vec, count=10, method='cos'):\n",
    "    if method == 'cos':\n",
    "        dist = np.array([ sum(vec * words_vec[key]) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[::-1][:10]\n",
    "    else:\n",
    "        dist = np.array([ sum(np.square(np.array(vec) - np.array(words_vec[key]))) for key in words_vec.keys()])\n",
    "        top_ten = dist.argsort()[:10]\n",
    "    return [list(words_vec.keys())[i] for i in top_ten]\n",
    "\n",
    "def cloestWord2(word, emb, count=10, method='cos'):\n",
    "    return cloestWord3(emb[words_dict[word]], emb, count, method)\n",
    "\n",
    "def cloestWord3(vec, emb, count=10, method='cos'):\n",
    "    if method == 'cos':\n",
    "        dist = np.array([ sum(vec * emb[i]) for i in range(emb.shape[0])])\n",
    "        # dist: word index -> dist\n",
    "        \n",
    "        top = dist.argsort()[::-1][:count]\n",
    "        # top: ranking -> word index\n",
    "        \n",
    "    return [(inv_words_dict[i], \"%.2f\" % dist[i])  for i in top]\n",
    "\n",
    "def to_word_indices(words):\n",
    "    return [words_dict[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/brown-Words2Vec-50.ckpt\n",
      "Model saved in file: ./model/seq2seq-autoencoder.ckpt\n",
      "Average loss at step  200 :  10.8701518679\n",
      "word2vec:  [('two', '1.00'), ('three', '0.90'), ('four', '0.87'), ('differences', '0.86'), ('five', '0.86'), ('eight', '0.85'), ('several', '0.85'), ('six', '0.85'), ('companies', '0.83'), ('helping', '0.83')]\n",
      "rnn:  [('two', '1.00'), ('several', '0.92'), ('helping', '0.91'), ('three', '0.90'), ('eight', '0.90'), ('four', '0.90'), ('five', '0.89'), ('six', '0.89'), ('nine', '0.89'), ('cuts', '0.89')]\n",
      "rnn:  [('friday', '0.94'), ('stage', '0.94'), ('budget', '0.94'), ('danger', '0.93'), ('proposal', '0.93'), ('happy', '0.93'), ('evening', '0.93'), ('news', '0.93'), ('choice', '0.92'), ('concert', '0.92')]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Input_Sentence_Word_Index' with dtype int32\n\t [[Node: Input_Sentence_Word_Index = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Input_Sentence_Word_Index', defined at:\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-a3c27e0081dc>\", line 6, in <module>\n    inputs = tf.placeholder(tf.int32, (None, None), name = \"Input_Sentence_Word_Index\")\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Input_Sentence_Word_Index' with dtype int32\n\t [[Node: Input_Sentence_Word_Index = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Input_Sentence_Word_Index' with dtype int32\n\t [[Node: Input_Sentence_Word_Index = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a2a346dafd71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnn: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloestWord3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3739\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3740\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3741\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Input_Sentence_Word_Index' with dtype int32\n\t [[Node: Input_Sentence_Word_Index = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Input_Sentence_Word_Index', defined at:\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-a3c27e0081dc>\", line 6, in <module>\n    inputs = tf.placeholder(tf.int32, (None, None), name = \"Input_Sentence_Word_Index\")\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Input_Sentence_Word_Index' with dtype int32\n\t [[Node: Input_Sentence_Word_Index = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "DEBUG_SIZE = 200\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    #saver.restore(session, MODEL)\n",
    "      \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels, batch_input_lengths = next(generator)\n",
    "        feed_dict = {inputs: batch_inputs, input_lengths: batch_input_lengths}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "                emb = embeddings.eval()\n",
    "                normalize(emb, norm='l2', axis=1, copy=False)\n",
    "                print('word2vec: ', cloestWord2('two', emb))\n",
    "                \n",
    "                dict_list = [[i] for i in range(len(words_dict))]\n",
    "                dict_list_lengths = [1 for i in range(len(words_dict))]\n",
    "                emb = rnn_final_state.eval(feed_dict={inputs: dict_list, input_lengths: dict_list_lengths})\n",
    "                normalize(emb, norm='l2', axis=1, copy=False)\n",
    "                print('rnn: ', cloestWord2('two', emb))\n",
    "                \n",
    "                vec = rnn_final_state.eval(feed_dict={inputs: [to_word_indices(['good', 'morning'])], input_lengths: [2]})\n",
    "                normalize(vec, norm='l2', axis=1, copy=False)\n",
    "                print('rnn: ', cloestWord3(vec[0], emb))\n",
    "                \n",
    "                print(decoder_masks.eval())\n",
    "\n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    \n",
    "    feed_dict={inputs: [[words_dict['two']], [words_dict['new']], [words_dict['funny']]], input_lengths: [1, 1, 1]}\n",
    "    \n",
    "    index_eval = index.eval(feed_dict=feed_dict)\n",
    "    rnn_outputs_eval = rnn_outputs.eval(feed_dict=feed_dict)\n",
    "    rnn_final_state_eval = rnn_final_state.eval(feed_dict=feed_dict)\n",
    "    print(\"index:\\n\", index_eval)\n",
    "    print(\"rnn_output:\\n\", rnn_outputs_eval)\n",
    "    print(\"rnn_final:\\n\", rnn_final_state_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    #saver.restore(session, MODEL)\n",
    "    embeddings_saver.restore(session, WORDS2VEC_MODEL)\n",
    "    final_embeddings = embeddings.eval()\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize(final_embeddings, norm='l2', axis=1, copy=False)\n",
    "\n",
    "words_vec = {}\n",
    "for i in range(final_embeddings.shape[0]):\n",
    "    words_vec[inv_words_dict[i]] = final_embeddings[i]\n",
    "  \n",
    "words_vec2 = {}\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    \n",
    "    for key in words_vec.keys():\n",
    "        feed_dict = {inputs: [[words_dict[key]]], input_lengths: [1]}\n",
    "        words_vec2[key] = normalize(rnn_final_state.eval(feed_dict), norm='l2', copy=True)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sent2Context(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        context_pred = context.eval(feed_dict)\n",
    "        print(rnn_final_state.eval(feed_dict))\n",
    "        return [inv_words_dict[i] for i in context_pred.argsort()[0][::-1][:10]]\n",
    "    \n",
    "def twoWords2Vec(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        return normalize(rnn_final_state.eval(feed_dict), norm='l2', copy=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(twoWords2Vec(['run', 'faster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(cloestWord(twoWords2Vec(['run', 'faster'])))\n",
    "#print(cloestWord(twoWords2Vec(['two', 'idiot'])))\n",
    "#print(cloestWord(words_vec['two']))\n",
    "#print(cloestWord(words_vec['but']))\n",
    "#print(cloestWord(words_vec['man']))\n",
    "\n",
    "\n",
    "#print(cloestWord(words_vec2['two'], words_vec2))\n",
    "#print(cloestWord(words_vec2['but'], words_vec2))\n",
    "#print(cloestWord(words_vec2['man'], words_vec2))\n",
    "\n",
    "print(cloestWord(words_vec2['two'], words_vec2))\n",
    "print(cloestWord(words_vec2['however'], words_vec2))\n",
    "print(cloestWord(words_vec2['man'], words_vec2))\n",
    "\n",
    "#print(cloestWord(twoWords2Vec(['but']), words_vec2))\n",
    "#print(cloestWord(twoWords2Vec(['man']), words_vec2))\n",
    "#print(cloestWord(twoWords2Vec(['two'])))\n",
    "\"\"\"\n",
    "def rnn_out(sent):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        saver.restore(session, MODEL)\n",
    "        feed_dict = {inputs: [[words_dict[word] for word in sent]], input_lengths: ([len(sent)])}\n",
    "        print('rnn_inputs: ', rnn_inputs.eval(feed_dict))\n",
    "        print('rnn_outputs: ', rnn_outputs.eval(feed_dict))\n",
    "        print('rnn_final_state:', rnn_final_state.eval(feed_dict))\n",
    "\n",
    "rnn_out(['two'])\n",
    "rnn_out(['three'])\n",
    "words_vec['two']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WORDS2VEC_MODEL = './model/brown-Words2Vec.ckpt'\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "\n",
    "with graph2.as_default():\n",
    "    embeddings2 = tf.Variable(\n",
    "            tf.random_uniform([VOCABULAY_SIZE, DIMENSION], -1.0, 1.0), name='Words2Vec')\n",
    "    embeddings_saver2 = tf.train.Saver({'Words2Vec': embeddings2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph2) as session2:\n",
    "    embeddings_saver2.restore(session2, WORDS2VEC_MODEL)\n",
    "    final_embeddings2 = embeddings2.eval()\n",
    "    \n",
    "final_embeddings2 = normalize(final_embeddings2, norm='l2', axis=1, copy=True)\n",
    "\n",
    "words_vec3 = {}\n",
    "for i in range(final_embeddings2.shape[0]):\n",
    "    words_vec3[inv_words_dict[i]] = final_embeddings2[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(cloestWord(words_vec3['two'], words_vec3))\n",
    "print(cloestWord(words_vec3['however'], words_vec3))\n",
    "print(cloestWord(words_vec3['man'], words_vec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(words_dict['five'])\n",
    "print(inv_words_dict[334])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
