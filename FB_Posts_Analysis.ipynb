{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000000.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000000.gzip\n",
      "Downloading posts-000000000001.gzip to /Users/Hoiy/project/tf_playground/posts-000000000001.gzip ...\n",
      "Downloaded posts-000000000001.gzip to /Users/Hoiy/project/tf_playground/posts-000000000001.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000001.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000001.gzip\n",
      "Downloading posts-000000000002.gzip to /Users/Hoiy/project/tf_playground/posts-000000000002.gzip ...\n",
      "Downloaded posts-000000000002.gzip to /Users/Hoiy/project/tf_playground/posts-000000000002.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000002.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000002.gzip\n",
      "Downloading posts-000000000003.gzip to /Users/Hoiy/project/tf_playground/posts-000000000003.gzip ...\n",
      "Downloaded posts-000000000003.gzip to /Users/Hoiy/project/tf_playground/posts-000000000003.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000003.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000003.gzip\n",
      "Downloading posts-000000000004.gzip to /Users/Hoiy/project/tf_playground/posts-000000000004.gzip ...\n",
      "Downloaded posts-000000000004.gzip to /Users/Hoiy/project/tf_playground/posts-000000000004.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000004.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000004.gzip\n",
      "Downloading posts-000000000005.gzip to /Users/Hoiy/project/tf_playground/posts-000000000005.gzip ...\n",
      "Downloaded posts-000000000005.gzip to /Users/Hoiy/project/tf_playground/posts-000000000005.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000005.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000005.gzip\n",
      "Downloading posts-000000000006.gzip to /Users/Hoiy/project/tf_playground/posts-000000000006.gzip ...\n",
      "Downloaded posts-000000000006.gzip to /Users/Hoiy/project/tf_playground/posts-000000000006.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000006.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000006.gzip\n",
      "Downloading posts-000000000007.gzip to /Users/Hoiy/project/tf_playground/posts-000000000007.gzip ...\n",
      "Downloaded posts-000000000007.gzip to /Users/Hoiy/project/tf_playground/posts-000000000007.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000007.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000007.gzip\n",
      "Downloading posts-000000000008.gzip to /Users/Hoiy/project/tf_playground/posts-000000000008.gzip ...\n",
      "Downloaded posts-000000000008.gzip to /Users/Hoiy/project/tf_playground/posts-000000000008.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000008.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000008.gzip\n",
      "Downloading posts-000000000009.gzip to /Users/Hoiy/project/tf_playground/posts-000000000009.gzip ...\n",
      "Downloaded posts-000000000009.gzip to /Users/Hoiy/project/tf_playground/posts-000000000009.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000009.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000009.gzip\n",
      "Downloading posts-000000000010.gzip to /Users/Hoiy/project/tf_playground/posts-000000000010.gzip ...\n",
      "Downloaded posts-000000000010.gzip to /Users/Hoiy/project/tf_playground/posts-000000000010.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000010.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000010.gzip\n",
      "Downloading posts-000000000011.gzip to /Users/Hoiy/project/tf_playground/posts-000000000011.gzip ...\n",
      "Downloaded posts-000000000011.gzip to /Users/Hoiy/project/tf_playground/posts-000000000011.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000011.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000011.gzip\n",
      "Downloading posts-000000000012.gzip to /Users/Hoiy/project/tf_playground/posts-000000000012.gzip ...\n",
      "Downloaded posts-000000000012.gzip to /Users/Hoiy/project/tf_playground/posts-000000000012.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000012.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000012.gzip\n",
      "Downloading posts-000000000013.gzip to /Users/Hoiy/project/tf_playground/posts-000000000013.gzip ...\n",
      "Downloaded posts-000000000013.gzip to /Users/Hoiy/project/tf_playground/posts-000000000013.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000013.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000013.gzip\n",
      "Downloading posts-000000000014.gzip to /Users/Hoiy/project/tf_playground/posts-000000000014.gzip ...\n",
      "Downloaded posts-000000000014.gzip to /Users/Hoiy/project/tf_playground/posts-000000000014.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000014.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000014.gzip\n",
      "Downloading posts-000000000015.gzip to /Users/Hoiy/project/tf_playground/posts-000000000015.gzip ...\n",
      "Downloaded posts-000000000015.gzip to /Users/Hoiy/project/tf_playground/posts-000000000015.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000015.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000015.gzip\n",
      "Downloading posts-000000000016.gzip to /Users/Hoiy/project/tf_playground/posts-000000000016.gzip ...\n",
      "Downloaded posts-000000000016.gzip to /Users/Hoiy/project/tf_playground/posts-000000000016.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000016.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000016.gzip\n",
      "Downloading posts-000000000017.gzip to /Users/Hoiy/project/tf_playground/posts-000000000017.gzip ...\n",
      "Downloaded posts-000000000017.gzip to /Users/Hoiy/project/tf_playground/posts-000000000017.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000017.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000017.gzip\n",
      "Downloading posts-000000000018.gzip to /Users/Hoiy/project/tf_playground/posts-000000000018.gzip ...\n",
      "Downloaded posts-000000000018.gzip to /Users/Hoiy/project/tf_playground/posts-000000000018.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000018.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000018.gzip\n",
      "Downloading posts-000000000019.gzip to /Users/Hoiy/project/tf_playground/posts-000000000019.gzip ...\n",
      "Downloaded posts-000000000019.gzip to /Users/Hoiy/project/tf_playground/posts-000000000019.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000019.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000019.gzip\n",
      "Downloading posts-000000000020.gzip to /Users/Hoiy/project/tf_playground/posts-000000000020.gzip ...\n",
      "Downloaded posts-000000000020.gzip to /Users/Hoiy/project/tf_playground/posts-000000000020.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000020.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000020.gzip\n",
      "Downloading posts-000000000021.gzip to /Users/Hoiy/project/tf_playground/posts-000000000021.gzip ...\n",
      "Downloaded posts-000000000021.gzip to /Users/Hoiy/project/tf_playground/posts-000000000021.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000021.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000021.gzip\n",
      "Downloading posts-000000000022.gzip to /Users/Hoiy/project/tf_playground/posts-000000000022.gzip ...\n",
      "Downloaded posts-000000000022.gzip to /Users/Hoiy/project/tf_playground/posts-000000000022.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000022.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000022.gzip\n",
      "Downloading posts-000000000023.gzip to /Users/Hoiy/project/tf_playground/posts-000000000023.gzip ...\n",
      "Downloaded posts-000000000023.gzip to /Users/Hoiy/project/tf_playground/posts-000000000023.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000023.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000023.gzip\n",
      "Downloading posts-000000000024.gzip to /Users/Hoiy/project/tf_playground/posts-000000000024.gzip ...\n",
      "Downloaded posts-000000000024.gzip to /Users/Hoiy/project/tf_playground/posts-000000000024.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000024.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000024.gzip\n",
      "Downloading posts-000000000025.gzip to /Users/Hoiy/project/tf_playground/posts-000000000025.gzip ...\n",
      "Downloaded posts-000000000025.gzip to /Users/Hoiy/project/tf_playground/posts-000000000025.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000025.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000025.gzip\n",
      "Downloading posts-000000000026.gzip to /Users/Hoiy/project/tf_playground/posts-000000000026.gzip ...\n",
      "Downloaded posts-000000000026.gzip to /Users/Hoiy/project/tf_playground/posts-000000000026.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000026.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000026.gzip\n",
      "Downloading posts-000000000027.gzip to /Users/Hoiy/project/tf_playground/posts-000000000027.gzip ...\n",
      "Downloaded posts-000000000027.gzip to /Users/Hoiy/project/tf_playground/posts-000000000027.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000027.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000027.gzip\n",
      "Downloading posts-000000000028.gzip to /Users/Hoiy/project/tf_playground/posts-000000000028.gzip ...\n",
      "Downloaded posts-000000000028.gzip to /Users/Hoiy/project/tf_playground/posts-000000000028.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000028.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000028.gzip\n",
      "Downloading posts-000000000029.gzip to /Users/Hoiy/project/tf_playground/posts-000000000029.gzip ...\n",
      "Downloaded posts-000000000029.gzip to /Users/Hoiy/project/tf_playground/posts-000000000029.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000029.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000029.gzip\n",
      "Downloading posts-000000000030.gzip to /Users/Hoiy/project/tf_playground/posts-000000000030.gzip ...\n",
      "Downloaded posts-000000000030.gzip to /Users/Hoiy/project/tf_playground/posts-000000000030.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000030.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000030.gzip\n",
      "Downloading posts-000000000031.gzip to /Users/Hoiy/project/tf_playground/posts-000000000031.gzip ...\n",
      "Downloaded posts-000000000031.gzip to /Users/Hoiy/project/tf_playground/posts-000000000031.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000031.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000031.gzip\n",
      "Downloading posts-000000000032.gzip to /Users/Hoiy/project/tf_playground/posts-000000000032.gzip ...\n",
      "Downloaded posts-000000000032.gzip to /Users/Hoiy/project/tf_playground/posts-000000000032.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000032.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000032.gzip\n",
      "Downloading posts-000000000033.gzip to /Users/Hoiy/project/tf_playground/posts-000000000033.gzip ...\n",
      "Downloaded posts-000000000033.gzip to /Users/Hoiy/project/tf_playground/posts-000000000033.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000033.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000033.gzip\n",
      "Downloading posts-000000000034.gzip to /Users/Hoiy/project/tf_playground/posts-000000000034.gzip ...\n",
      "Downloaded posts-000000000034.gzip to /Users/Hoiy/project/tf_playground/posts-000000000034.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000034.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000034.gzip\n",
      "Downloading posts-000000000035.gzip to /Users/Hoiy/project/tf_playground/posts-000000000035.gzip ...\n",
      "Downloaded posts-000000000035.gzip to /Users/Hoiy/project/tf_playground/posts-000000000035.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000035.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000035.gzip\n",
      "Downloading posts-000000000036.gzip to /Users/Hoiy/project/tf_playground/posts-000000000036.gzip ...\n",
      "Downloaded posts-000000000036.gzip to /Users/Hoiy/project/tf_playground/posts-000000000036.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000036.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000036.gzip\n",
      "Downloading posts-000000000037.gzip to /Users/Hoiy/project/tf_playground/posts-000000000037.gzip ...\n",
      "Downloaded posts-000000000037.gzip to /Users/Hoiy/project/tf_playground/posts-000000000037.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000037.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000037.gzip\n",
      "Downloading posts-000000000038.gzip to /Users/Hoiy/project/tf_playground/posts-000000000038.gzip ...\n",
      "Downloaded posts-000000000038.gzip to /Users/Hoiy/project/tf_playground/posts-000000000038.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000038.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000038.gzip\n",
      "Downloading posts-000000000039.gzip to /Users/Hoiy/project/tf_playground/posts-000000000039.gzip ...\n",
      "Downloaded posts-000000000039.gzip to /Users/Hoiy/project/tf_playground/posts-000000000039.gzip!\n",
      "Extracting /Users/Hoiy/project/tf_playground/posts-000000000039.gzip\n",
      "Extracted /Users/Hoiy/project/tf_playground/posts-000000000039.gzip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os.path\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('facebook-posts')\n",
    "\n",
    "for blob in bucket.list_blobs():\n",
    "    if not blob.name.lower().endswith('.gzip'):\n",
    "        continue\n",
    "    path = os.path.join(os.getcwd(), blob.name)\n",
    "    if not os.path.exists(path):\n",
    "        print('Downloading {0} to {1} ...'.format(blob.name, path))\n",
    "        with open(path, 'wb') as file_obj:\n",
    "            blob.download_to_file(file_obj)\n",
    "        print('Downloaded {0} to {1}!'.format(blob.name, path))\n",
    "    \n",
    "    print('Extracting {0}'.format(path))\n",
    "    with gzip.open(path, 'rb') as f_in, open(path + \".csv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print('Extracted {0}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./posts-000000000000.gzip.csv\n",
      "(258109, 23)\n",
      "./posts-000000000001.gzip.csv\n",
      "(512727, 23)\n",
      "./posts-000000000002.gzip.csv\n",
      "(751558, 23)\n",
      "./posts-000000000003.gzip.csv\n",
      "(990036, 23)\n",
      "./posts-000000000004.gzip.csv\n",
      "(1285162, 23)\n",
      "./posts-000000000005.gzip.csv\n",
      "(1522273, 23)\n",
      "./posts-000000000006.gzip.csv\n",
      "(1786717, 23)\n",
      "./posts-000000000007.gzip.csv\n",
      "(2036815, 23)\n",
      "./posts-000000000008.gzip.csv\n",
      "(2286757, 23)\n",
      "./posts-000000000009.gzip.csv\n"
     ]
    }
   ],
   "source": [
    "posts = pd.DataFrame()\n",
    "for i in range(40):\n",
    "    print('./posts-0000000000{:0>2d}.gzip.csv'.format(i))\n",
    "    posts = posts.append(pd.read_csv('./posts-{:0>12d}.gzip.csv'.format(i)), ignore_index=True)\n",
    "    print(posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 23)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = posts.head(10000)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245        330778070297798_924677370907862\n",
       "285              184695207750_438694347750\n",
       "1108      145300805513355_1038373536206073\n",
       "1764        344460080577_10150309825765578\n",
       "2075        136727474679_10150513586469680\n",
       "2396         49249798825_10150428245218826\n",
       "2496        110865425251_10150699141335252\n",
       "2744       109062392492028_200658033332463\n",
       "3306       146085438735912_996287687049012\n",
       "3420         28198464871_10151051464034872\n",
       "3776        193084362082_10150171780782083\n",
       "4861       127432237332984_233378150071725\n",
       "5407         83711079303_10153164981609304\n",
       "5417          107322412447_101119429946997\n",
       "5943        178550329805_10150394530129806\n",
       "6374       117604218271416_180359785329192\n",
       "6767         43394094549_10150289751959550\n",
       "6856             139195819664_479614124664\n",
       "7063       196264433742623_416691551699909\n",
       "7620             171623547203_471102722203\n",
       "7846          368278541524_119682861439780\n",
       "8595       117604218271416_180646288633875\n",
       "8924      146085438735912_1020302047980909\n",
       "8933       289177264494290_867625893316088\n",
       "9105        136727474679_10150633380494680\n",
       "9132             218607073865_417789468865\n",
       "9260             184695207750_436842067750\n",
       "9622             368278541524_468314611524\n",
       "9646       144251955614598_252212134818579\n",
       "9831         43394094549_10150693566919550\n",
       "                        ...               \n",
       "243970       12922850458_10157354554980459\n",
       "244269     134848776528294_155238021156036\n",
       "244544     226754307338391_448824568464696\n",
       "245314       49249798825_10150414256393826\n",
       "245566     144251955614598_251432468229879\n",
       "245592      178550329805_10150380493874806\n",
       "245776       43394094549_10150698585354550\n",
       "246543     117604218271416_180922428606261\n",
       "247036      178550329805_10150377864849806\n",
       "247078           171623547203_465872842203\n",
       "247755      193084362082_10150177850792083\n",
       "248471       39910168890_10150248895503891\n",
       "248821       10747695798_10150167386045799\n",
       "248910     198535763497184_212115458805881\n",
       "249442     127034223999785_138297289540145\n",
       "249493     149963331689048_209175942434453\n",
       "249509     136250749779174_154047104666205\n",
       "249787    146085438735912_1006625092681938\n",
       "249963     149963331689048_245070328845014\n",
       "251392     330778070297798_932024393506493\n",
       "251914     144251955614598_252857708087355\n",
       "252225     255027481177550_340883492591948\n",
       "252661     201925033186612_335170849862029\n",
       "252821           154526009569_397478689569\n",
       "254415        8291513953_10154462244433954\n",
       "254986     107998989273290_120594284680427\n",
       "256598      147563482044_10150964981527045\n",
       "256944     198535763497184_212573402093420\n",
       "257441      147563482044_10151041812557045\n",
       "257491     108087509216859_260629117296030\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = posts[posts['status_type'] == 'published_story']\n",
    "test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import TextPreprocessing.Language.detector as ld\n",
    "pages['lang_name'] = pages['name'].apply(ld.detect)\n",
    "pages['lang_description'] = pages['description'].astype(str).apply(ld.detect)\n",
    "pages['lang_about'] = pages['about'].astype(str).apply(ld.detect)\n",
    "\n",
    "#%timeit pages['name'].apply(lambda x: ld.detect(x, 'langdetect'))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x, 'langdetect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47230, 12)\n",
      "(22790, 12)\n"
     ]
    }
   ],
   "source": [
    "eng_pages = pages[pages['lang_about'] == 'en']\n",
    "print(eng_pages.shape)\n",
    "eng_pages = eng_pages[eng_pages['lang_description'] == 'en']\n",
    "print(eng_pages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "x = np.linspace(0.0000001, 0.0002, 1000)\n",
    "plt.hist(posts['share_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.0000001, 0.002, 1000)\n",
    "plt.hist(posts['like_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.0000001, 0.0002, 1000)\n",
    "plt.hist(posts['comment_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_posts = posts[posts['type'] == 'link']\n",
    "link_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def isEng(x):\n",
    "    try:\n",
    "        return langid.classify(x['message'])[0] == 'en'\n",
    "    except:\n",
    "        try:\n",
    "            return detect(x['description'])[0] == 'en'\n",
    "        except:\n",
    "            try:\n",
    "                return detect(x['name'])[0] == 'en'\n",
    "            except:\n",
    "                return False\n",
    "    \n",
    "\n",
    "eng_link_posts = link_posts[link_posts.apply(isEng, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_link_posts.shape\n",
    "backup = eng_link_posts\n",
    "data = eng_link_posts\n",
    "eng_link_posts.iloc[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def isEng(x):\n",
    "    try:\n",
    "        return detect(x['message']) == 'en'\n",
    "    except:\n",
    "        try:\n",
    "            return detect(x['description']) == 'en'\n",
    "        except:\n",
    "            try:\n",
    "                return detect(x['name']) == 'en'\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "eng_link_posts = link_posts[link_posts.apply(isEng, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "eng_link_posts.to_csv('/Users/Shared/fb/eng_link_posts.csv', doublequote=False, quotechar='\"', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/Shared/fb/eng_link_posts.csv', nrows=10000, error_bad_lines=False, doublequote=False, quotechar='\"', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_link_posts[['name']].to_csv('/Users/Shared/fb/eng_link_posts_name.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_link_posts['data'] = eng_link_posts['name'].astype(str)\n",
    "eng_link_posts['share_ratio'].describe()\n",
    "eng_link_posts['good'] = eng_link_posts['share_ratio'].apply(lambda x: 1 if x >= 0.0001 else 0)\n",
    "eng_link_posts['good'].describe()\n",
    "data = eng_link_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from codeDump.testHeadline import tokenize\n",
    "tokenize(data['data'])\n",
    "#test_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word vector by fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = eng_link_posts[['like', 'facebook_id', 'name', 'like_ratio']]\n",
    "data['like_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['good'] = data['like_ratio'].apply(lambda x: 1 if x > 0.0005 else 0)\n",
    "data['good'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"name\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data[\"name\"] = data[\"name\"].astype('str')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"name\"])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('/Users/Shared/fb/eng_link_posts_name_model.vec'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(df):    \n",
    "    good = df[df[\"good\"] == 1]\n",
    "    bad = df[df[\"good\"] == 0]\n",
    "    bad = bad.sample(n=good.shape[0])\n",
    "    data = good.append(bad)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    y_train = np.zeros((data.shape[0], 2), dtype=int)\n",
    "    for i in range(0, data.shape[0]):\n",
    "        y_train[i, 1] = 1 if data.iloc[i]['good'] == 1 else 0\n",
    "        y_train[i, 0] = 1 if data.iloc[i]['good'] == 0 else 0\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(data[\"name\"])\n",
    "    x_train = pad_sequences(sequences, maxlen=25)\n",
    "\n",
    "    return x_train, y_train, data['good'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_full, y_full, y2_full = prepareData(eng_link_posts)\n",
    "x_train, y_train, y2_train = prepareData(train)\n",
    "x_test, y_test, y2_test = prepareData(test)\n",
    "\n",
    "print(y2_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def validate(model, x_test, y_test):\n",
    "    test_truth = np.apply_along_axis(lambda x: np.argmax(x), 1, y_test)\n",
    "    test_pred = model.predict(x_test)\n",
    "    test_pred = np.apply_along_axis(lambda x: np.argmax(x), 1, test_pred)\n",
    "    precision = precision_score(test_truth, test_pred)\n",
    "    recall = recall_score(test_truth, test_pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall\n",
    "\n",
    "def validate_2(truth, pred):\n",
    "    truth = np.apply_along_axis(lambda x: np.argmax(x), 1, truth)\n",
    "    pred = np.apply_along_axis(lambda x: np.argmax(x), 1, pred)\n",
    "    precision = precision_score(truth, pred)\n",
    "    recall = recall_score(truth, pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, criterion=\"entropy\", random_state=1)\n",
    "rf.fit(x_train, y2_train)\n",
    "\n",
    "res = rf.predict(x_test)\n",
    "\n",
    "#validate_2(y_test, res)\n",
    "precision = precision_score(y2_test, res)\n",
    "recall = recall_score(y2_test, res)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin = pd.read_csv('/Users/Shared/data/HN_posts_year_to_Sep_26_2016.csv')\n",
    "origin_1 = origin[['id', 'num_points']]\n",
    "data = pd.merge(title_vec, origin_1, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sentence_length = pd.read_csv('sentence_length.csv', header=None, )\n",
    "sentence_length.rename(columns={0: 'id', 1:'length'}, inplace=True)\n",
    "data = pd.merge(data, sentence_length, how='inner', on=['id'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOOD_THRESHOLD = 10\n",
    "\n",
    "data['good'] = data['num_points'].apply(lambda x: 1 if x >= GOOD_THRESHOLD else 0)\n",
    "data['good'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'num_points'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.8)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "abnormal = data.loc[data['good'] == 1]\n",
    "abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = train[list(range(0, 1200))].as_matrix()\n",
    "u, s, v = np.linalg.svd(m, full_matrices=False)\n",
    "plt.plot(s[0:400])\n",
    "plt.show()\n",
    "\n",
    "print(u.shape, s.shape, v.shape)\n",
    "print(m[0].shape, v.transpose().shape)\n",
    "\n",
    "transform = np.delete(v, range(2, v.shape[0]), 0)\n",
    "print(transform.shape)\n",
    "\n",
    "def reduce(vec):\n",
    "    return np.dot(vec, transform.transpose())\n",
    "\n",
    "def to_coor(matrix):\n",
    "    coor = np.apply_along_axis(reduce, 1, matrix).transpose()\n",
    "    print(matrix.shape, coor.shape)\n",
    "    return (coor[0], coor[1])\n",
    "    \n",
    "normal_x, normal_y = to_coor(m)\n",
    "\n",
    "m2 = abnormal[list(range(0,1200))].as_matrix()\n",
    "\n",
    "ab_x, ab_y = to_coor(m2)\n",
    "\n",
    "print(ab_x.shape)\n",
    "print(ab_y.shape)\n",
    "\n",
    "plt.scatter(normal_x, normal_y)\n",
    "plt.scatter(ab_x, ab_y, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m2 = abnormal.as_matrix()\n",
    "u, s, v = np.linalg.svd(m2)\n",
    "plt.plot(s[0:400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "origin['num_points']\n",
    "\n",
    "plot_data = origin[origin['num_points'] >= 0 ]['num_points'].values\n",
    "\n",
    "x = np.array(list(range(100, 1001)))\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "xdata = x_bin\n",
    "ydata = y\n",
    "\n",
    "print(xdata.shape)\n",
    "print(ydata.shape)\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata, p0=[0.0007, 0.0004])\n",
    "\n",
    "print(popt)\n",
    "print(pcov)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=10)\n",
    "plt.plot(x_bin, y, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import expon\n",
    "loc, lamb = expon.fit(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "plt.hist(plot_data, bins=x_bin)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a[(a>1) & (a<5)]=0\n",
    "a\n",
    "\n",
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Study double / triple exponential behaviour of the distribution\n",
    "\n",
    "x = np.linspace(0.0000001, 1, 100000)\n",
    "\n",
    "def log_tower(plot_data, k):\n",
    "    for i in range(0, k):\n",
    "        plot_data[plot_data < 1] = 1.\n",
    "        plot_data = np.log(plot_data)\n",
    "    #plot_data[plot_data < 1] = 0.\n",
    "    return plot_data\n",
    "\n",
    "ln_plot_data = log_tower(np.array(plot_data), 3)\n",
    "\n",
    "print(ln_plot_data.shape)\n",
    "\n",
    "y, x = np.histogram(ln_plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the elbow of exponential curve\n",
    "\n",
    "x = np.linspace(0, 100, 10000)\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. Compute the derivative\n",
    "\n",
    "def derivative(np_array):\n",
    "    a = np.delete(np_array, len(np_array)-1)\n",
    "    b = np.delete(np_array, 0)\n",
    "    return a-b\n",
    "\n",
    "d_y = derivative(derivative(y))\n",
    "d_x_bin = np.linspace(0, len(d_y), len(d_y))\n",
    "\n",
    "print(x_bin.shape)\n",
    "print(d_y.shape)\n",
    "print(d_x_bin.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(d_x_bin, d_y)\n",
    "plt.show()\n",
    "\n",
    "print(np.argmax(d_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find elbow from curve estimate\n",
    "\n",
    "origin['num_points']\n",
    "\n",
    "plot_data = origin[origin['num_points'] >= 0 ]['num_points'].values\n",
    "\n",
    "x = np.linspace(0, 20, 20)\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "xdata = x_bin\n",
    "ydata = y\n",
    "\n",
    "print(xdata.shape)\n",
    "print(ydata.shape)\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata, p0=[0.0007, 0.0004])\n",
    "\n",
    "print(popt)\n",
    "print(pcov)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=10)\n",
    "plt.plot(x_bin, y, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = popt\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# second derivative of a * e^(-bx) is ab^2e^(-bx)\n",
    "\n",
    "def firstD(a,b,x):\n",
    "    return - a * b * np.exp(-b * x)\n",
    "\n",
    "\n",
    "def secondD(a,b,x):\n",
    "    return a * np.square(b) * np.exp(-b * x)\n",
    "\n",
    "plt.plot(x_bin, secondD(a,b,x_bin))\n",
    "plt.plot(x_bin, firstD(a,b,x_bin))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "input_dim = 50\n",
    "timesteps = 24\n",
    "latent_dim = 200\n",
    "\n",
    "predictor = Sequential()\n",
    "predictor.add(LSTM(100, input_shape=(timesteps, input_dim)))\n",
    "predictor.add(Dense(100, init='normal', activation='tanh'))\n",
    "predictor.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "X = train.drop(['good'], axis=1)\n",
    "Y = test.drop(['good'], axis=1)\n",
    "\n",
    "X = X.as_matrix().reshape(train.shape[0], 24, 50)\n",
    "Y = Y.as_matrix().reshape(test.shape[0], 24, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', 'precision', 'recall', 'fmeasure'])\n",
    "predictor.fit(X, train['good'], validation_data=(Y, test['good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sequence_autoencoder.evaluate(Y,test['good'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
