{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os.path\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('facebook-posts')\n",
    "\n",
    "for blob in bucket.list_blobs():\n",
    "    if not blob.name.lower().endswith('.gzip'):\n",
    "        continue\n",
    "    path = os.path.join(os.getcwd(), blob.name)\n",
    "    if not os.path.exists(path):\n",
    "        print('Downloading {0} to {1} ...'.format(blob.name, path))\n",
    "        with open(path, 'wb') as file_obj:\n",
    "            blob.download_to_file(file_obj)\n",
    "        print('Downloaded {0} to {1}!'.format(blob.name, path))\n",
    "    \n",
    "    print('Extracting {0}'.format(path))\n",
    "    with gzip.open(path, 'rb') as f_in, open(path + \".csv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print('Extracted {0}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts = pd.DataFrame()\n",
    "for i in range(40):\n",
    "    print('./posts-0000000000{:0>2d}.gzip.csv'.format(i))\n",
    "    p = pd.read_csv('./posts-{:0>12d}.gzip.csv'.format(i))\n",
    "    p = p[p['type'] == 'link']\n",
    "    posts = posts.append(p, ignore_index=True)\n",
    "    print(posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts.to_hdf('link_posts.h5', 'posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f1a76caad24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./link_posts.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'posts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                                      'contains multiple datasets.')\n\u001b[1;32m    359\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_only_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v_pathname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# if there is an error, close the store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m                            chunksize=chunksize, auto_close=auto_close)\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     def select_as_coordinates(\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0;31m# directly return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(_start, _stop, _where)\u001b[0m\n\u001b[1;32m    715\u001b[0m             return s.read(start=_start, stop=_stop,\n\u001b[1;32m    716\u001b[0m                           \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                           columns=columns, **kwargs)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# create the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, **kwargs)\u001b[0m\n\u001b[1;32m   2849\u001b[0m             \u001b[0mblk_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'block%d_items'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m             values = self.read_array('block%d_values' % i,\n\u001b[0;32m-> 2851\u001b[0;31m                                      start=_start, stop=_stop)\n\u001b[0m\u001b[1;32m   2852\u001b[0m             blk = make_block(values,\n\u001b[1;32m   2853\u001b[0m                              placement=items.get_indexer(blk_items))\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, key, start, stop)\u001b[0m\n\u001b[1;32m   2398\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tables/vlarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    673\u001b[0m             start, stop, step = self._process_range(\n\u001b[1;32m    674\u001b[0m                 key.start, key.stop, key.step)\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;31m# Try with a boolean or point selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/tables/vlarray.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0matom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.VLArray._read_array (tables/hdf5extension.c:24255)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = pd.read_hdf('./link_posts.h5', 'posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import TextPreprocessing.Language.detector as ld\n",
    "posts['lang_name'] = posts['name'].astype(str).apply(ld.detect)\n",
    "#posts['lang_description'] = posts['description'].astype(str).apply(ld.detect)\n",
    "#posts['lang_about'] = posts['about'].astype(str).apply(ld.detect)\n",
    "\n",
    "#%timeit pages['name'].apply(lambda x: ld.detect(x, 'langdetect'))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x, 'langdetect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_link_posts = posts[posts['lang_name'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_link_posts.to_hdf('link_posts.h5', 'en_link_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991673, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "en_link_posts = pd.read_hdf('link_posts.h5', 'en_link_posts')\n",
    "en_link_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = en_link_posts[['name', 'page_id', 'share']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "pages = pd.read_csv('./pages.csv', nrows=NROWS)\n",
    "pages = pages[['id', 'fan_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.merge(posts, pages, left_on='page_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result['share_ratio'] = result['share'] / result['fan_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result['normalized_share_ratio'] = result['share_ratio'] / result['share_ratio'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    990127.000000\n",
       "mean          0.000442\n",
       "std           0.002739\n",
       "min           0.000000\n",
       "25%           0.000005\n",
       "50%           0.000037\n",
       "75%           0.000190\n",
       "max           1.000000\n",
       "Name: normalized_share_ratio, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['normalized_share_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = result[['name', 'normalized_share_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_share_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Local Wanderer: Eclectic live music in Havana,...</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weekend Wanderer: Seattle to Whitefish</td>\n",
       "      <td>0.001722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heroic Flight Attendant Rescues Teenage Human ...</td>\n",
       "      <td>0.010129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekend Wanderer: San Diego to New York City</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Best and Worst Airlines in the United States</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Travel Alert: Alaska Airlines braces for morni...</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In gesture of love, Alaska pilot to donate kid...</td>\n",
       "      <td>0.001841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Travel Advisory: Winter weather impacts Pacifi...</td>\n",
       "      <td>0.004972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What‚Äôs happening behind the scenes before your...</td>\n",
       "      <td>0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Travel like a pro: Fly like a flash through th...</td>\n",
       "      <td>0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Welcome to Mileage Plan: Let‚Äôs take a tour</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Virgin America / Alaska Airlines: Twogether</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Love at 35,000 feet: A connection formed on an...</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Travel Advisories | Alaska Airlines</td>\n",
       "      <td>0.001090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Weekend Wanderer: Vancouver, B.C. to Charlesto...</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How to ski for free with your Alaska Airlines ...</td>\n",
       "      <td>0.004874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oldest-Known Pearl Harbor Veteran, 104, Bulks ...</td>\n",
       "      <td>0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cyber Week flight sale</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016 year in review: Mileage Plan by the numbers</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Give me a break: After long holiday weekend, C...</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alaska‚Äôs newest Charity Miles partner brings d...</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alaska employee teamwork at 35K ft. helps dad ...</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Virgin America / Alaska Airlines: Twogether Sw...</td>\n",
       "      <td>0.003251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What you need to know about Alaska Airlines Mi...</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Oklahoma Court Restricts Life Without Parole f...</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>EJI's History of Racial Injustice Highlight: S...</td>\n",
       "      <td>0.004453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Police in Austin, Texas, Violently Arrest Blac...</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Death Penalty Support Reaches 45-Year Low</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EJI Releases New Report on Lynching: Targeting...</td>\n",
       "      <td>0.030174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990097</th>\n",
       "      <td>WeChoice Awards 2016</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990098</th>\n",
       "      <td>8 robes de mari√©e originales qui rivalisent av...</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990099</th>\n",
       "      <td>@pharwacy ‚Ä¢ Instagram photos and videos</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990100</th>\n",
       "      <td>The Three Most Valuable Lessons I‚Äôve Learned -...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990101</th>\n",
       "      <td>Indiegogo: The Largest Global Crowdfunding &amp; F...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990102</th>\n",
       "      <td>How Do Israel‚Äôs Tech Firms Do Business in Saud...</td>\n",
       "      <td>0.002603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990103</th>\n",
       "      <td>More Health Spa</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990104</th>\n",
       "      <td>Imgur: The most awesome images on the Internet</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990105</th>\n",
       "      <td>James Rodriguez Love</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990106</th>\n",
       "      <td>'Shraddha Das' Diwali Special Wishes for Fans</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990107</th>\n",
       "      <td>'Shraddha Das' Diwali Special Wishes for Fans</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990108</th>\n",
       "      <td>www.jokesoftheday.net</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990109</th>\n",
       "      <td>ResearchGate - Share and discover research</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990110</th>\n",
       "      <td>An Nhi√™n ‚ù§Ô∏èüíõüíöüíôüíúüíö üòä 11/O8/2O16 (@annhien.quotes...</td>\n",
       "      <td>0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990111</th>\n",
       "      <td>TIPS MENDISIPLINKAN ANAK TANPA KEKERASAN ~ Our...</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990112</th>\n",
       "      <td>Beautiful wedding hair (y)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990113</th>\n",
       "      <td>Jones Alsayed - Art Work</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990114</th>\n",
       "      <td>Check it out! A closer look at the Lewis Chessmen</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990115</th>\n",
       "      <td>dress.gifsgamers.com</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990116</th>\n",
       "      <td>Mahmoud Gennesh</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990117</th>\n",
       "      <td>Keren! Bus-Bus Wonderful Indonesia Blusukan di...</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990118</th>\n",
       "      <td>Shopping On Line / chief Khaldoon</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990119</th>\n",
       "      <td>Ojooo.com - Watching Ad - Sign up!</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990120</th>\n",
       "      <td>thumbs.gfycat.com</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990121</th>\n",
       "      <td>thumbs.gfycat.com</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990122</th>\n",
       "      <td>11 actrices que aunque lo intentan, simplement...</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990123</th>\n",
       "      <td>Mahmoud lotfy photographer</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990124</th>\n",
       "      <td>A Glimpse Inside a Father‚Äôs Thoughts and Dream...</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990125</th>\n",
       "      <td>uproxx.files.wordpress.com</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990126</th>\n",
       "      <td>1.bp.blogspot.com</td>\n",
       "      <td>0.016490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990127 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0       Local Wanderer: Eclectic live music in Havana,...   \n",
       "1                  Weekend Wanderer: Seattle to Whitefish   \n",
       "2       Heroic Flight Attendant Rescues Teenage Human ...   \n",
       "3            Weekend Wanderer: San Diego to New York City   \n",
       "4        The Best and Worst Airlines in the United States   \n",
       "5       Travel Alert: Alaska Airlines braces for morni...   \n",
       "6       In gesture of love, Alaska pilot to donate kid...   \n",
       "7       Travel Advisory: Winter weather impacts Pacifi...   \n",
       "8       What‚Äôs happening behind the scenes before your...   \n",
       "9       Travel like a pro: Fly like a flash through th...   \n",
       "10             Welcome to Mileage Plan: Let‚Äôs take a tour   \n",
       "11            Virgin America / Alaska Airlines: Twogether   \n",
       "12      Love at 35,000 feet: A connection formed on an...   \n",
       "13                    Travel Advisories | Alaska Airlines   \n",
       "14      Weekend Wanderer: Vancouver, B.C. to Charlesto...   \n",
       "15      How to ski for free with your Alaska Airlines ...   \n",
       "16      Oldest-Known Pearl Harbor Veteran, 104, Bulks ...   \n",
       "17                                 Cyber Week flight sale   \n",
       "18       2016 year in review: Mileage Plan by the numbers   \n",
       "19      Give me a break: After long holiday weekend, C...   \n",
       "20      Alaska‚Äôs newest Charity Miles partner brings d...   \n",
       "21      Alaska employee teamwork at 35K ft. helps dad ...   \n",
       "22      Virgin America / Alaska Airlines: Twogether Sw...   \n",
       "23      What you need to know about Alaska Airlines Mi...   \n",
       "24      Oklahoma Court Restricts Life Without Parole f...   \n",
       "25      EJI's History of Racial Injustice Highlight: S...   \n",
       "26      Police in Austin, Texas, Violently Arrest Blac...   \n",
       "27                                     The New York Times   \n",
       "28              Death Penalty Support Reaches 45-Year Low   \n",
       "29      EJI Releases New Report on Lynching: Targeting...   \n",
       "...                                                   ...   \n",
       "990097                               WeChoice Awards 2016   \n",
       "990098  8 robes de mari√©e originales qui rivalisent av...   \n",
       "990099            @pharwacy ‚Ä¢ Instagram photos and videos   \n",
       "990100  The Three Most Valuable Lessons I‚Äôve Learned -...   \n",
       "990101  Indiegogo: The Largest Global Crowdfunding & F...   \n",
       "990102  How Do Israel‚Äôs Tech Firms Do Business in Saud...   \n",
       "990103                                    More Health Spa   \n",
       "990104     Imgur: The most awesome images on the Internet   \n",
       "990105                               James Rodriguez Love   \n",
       "990106      'Shraddha Das' Diwali Special Wishes for Fans   \n",
       "990107      'Shraddha Das' Diwali Special Wishes for Fans   \n",
       "990108                              www.jokesoftheday.net   \n",
       "990109         ResearchGate - Share and discover research   \n",
       "990110  An Nhi√™n ‚ù§Ô∏èüíõüíöüíôüíúüíö üòä 11/O8/2O16 (@annhien.quotes...   \n",
       "990111  TIPS MENDISIPLINKAN ANAK TANPA KEKERASAN ~ Our...   \n",
       "990112                         Beautiful wedding hair (y)   \n",
       "990113                           Jones Alsayed - Art Work   \n",
       "990114  Check it out! A closer look at the Lewis Chessmen   \n",
       "990115                               dress.gifsgamers.com   \n",
       "990116                                    Mahmoud Gennesh   \n",
       "990117  Keren! Bus-Bus Wonderful Indonesia Blusukan di...   \n",
       "990118                  Shopping On Line / chief Khaldoon   \n",
       "990119                 Ojooo.com - Watching Ad - Sign up!   \n",
       "990120                                  thumbs.gfycat.com   \n",
       "990121                                  thumbs.gfycat.com   \n",
       "990122  11 actrices que aunque lo intentan, simplement...   \n",
       "990123                         Mahmoud lotfy photographer   \n",
       "990124  A Glimpse Inside a Father‚Äôs Thoughts and Dream...   \n",
       "990125                         uproxx.files.wordpress.com   \n",
       "990126                                  1.bp.blogspot.com   \n",
       "\n",
       "        normalized_share_ratio  \n",
       "0                     0.000378  \n",
       "1                     0.001722  \n",
       "2                     0.010129  \n",
       "3                     0.000065  \n",
       "4                     0.000952  \n",
       "5                     0.001028  \n",
       "6                     0.001841  \n",
       "7                     0.004972  \n",
       "8                     0.000381  \n",
       "9                     0.000607  \n",
       "10                    0.000225  \n",
       "11                    0.000527  \n",
       "12                    0.000044  \n",
       "13                    0.001090  \n",
       "14                    0.000167  \n",
       "15                    0.004874  \n",
       "16                    0.002749  \n",
       "17                    0.000621  \n",
       "18                    0.000131  \n",
       "19                    0.000291  \n",
       "20                    0.000033  \n",
       "21                    0.000563  \n",
       "22                    0.003251  \n",
       "23                    0.000458  \n",
       "24                    0.001949  \n",
       "25                    0.004453  \n",
       "26                    0.007830  \n",
       "27                    0.000000  \n",
       "28                    0.000465  \n",
       "29                    0.030174  \n",
       "...                        ...  \n",
       "990097                0.000015  \n",
       "990098                0.000019  \n",
       "990099                0.000000  \n",
       "990100                0.000000  \n",
       "990101                0.000000  \n",
       "990102                0.002603  \n",
       "990103                0.000000  \n",
       "990104                0.000282  \n",
       "990105                0.000000  \n",
       "990106                0.000017  \n",
       "990107                0.000007  \n",
       "990108                0.000546  \n",
       "990109                0.000276  \n",
       "990110                0.001272  \n",
       "990111                0.000115  \n",
       "990112                0.000000  \n",
       "990113                0.000000  \n",
       "990114                0.000033  \n",
       "990115                0.000012  \n",
       "990116                0.000000  \n",
       "990117                0.000003  \n",
       "990118                0.000000  \n",
       "990119                0.000004  \n",
       "990120                0.002435  \n",
       "990121                0.000000  \n",
       "990122                0.000007  \n",
       "990123                0.000000  \n",
       "990124                0.000002  \n",
       "990125                0.000011  \n",
       "990126                0.016490  \n",
       "\n",
       "[990127 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def string_to_words( string ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(string).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_to_words(data['name'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_name = data[\"name\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_name = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range( 0, num_name ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_name ))\n",
    "    clean_train_name.append( string_to_words( data[\"name\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean_list_post_name.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_train_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean_list_post_name.pkl', 'rb') as f:\n",
    "    clean_train_name = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_name)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean_list_post_name_features.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-15243a34533d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# This may take a few minutes to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_data_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"normalized_share_ratio\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hoiy/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m             'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, data[\"normalized_share_ratio\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
