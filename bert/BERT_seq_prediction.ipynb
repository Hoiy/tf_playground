{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT seq prediction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rkTLZ3I4_7c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# BERT finetuning tasks in 5 minutes with Cloud TPU\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "metadata": {
        "id": "1wtjs1QDb3DX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BERT**, or **B**idirectional **E**mbedding **R**epresentations from **T**ransformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. The academic paper can be found here: https://arxiv.org/abs/1810.04805.\n",
        "\n",
        "This Colab demonstates using a free Colab Cloud TPU to fine-tune sentence and sentence-pair classification tasks built on top of pretrained BERT models.\n",
        "\n",
        "**Note:**  You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud \n",
        "Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) for how to create GCP account and GCS bucket. You have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs.\n",
        "\n",
        "Once you finish the setup, let's start!"
      ]
    },
    {
      "metadata": {
        "id": "ycHMh-bhC-vX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Firstly**, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage."
      ]
    },
    {
      "metadata": {
        "id": "QZrG5D5TTdAc",
        "colab_type": "code",
        "outputId": "a8fa1340-1fb7-42f8-dc61-dc0c8968a1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "191zq3ZErihP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qgmgBOFcLJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0a717692-9441-4f50-846b-6a78880efe6e"
      },
      "cell_type": "code",
      "source": [
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.63.176.194:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 1369215045962293475),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9426068062629538741),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 15516382406546958278),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 18344184531694828712),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13827879249916831370),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13105778417158121679),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11332683515315229136),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14279864364375530158),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6310200481431953177),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3403636596179803568),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 242468359387548999),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 4080866702285330144)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HUBP35oCDmbF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Secondly**, prepare and import BERT modules."
      ]
    },
    {
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRu1aKO1D7-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Thirdly**, prepare for training:\n",
        "\n",
        "*  Specify task and download training data.\n",
        "*  Specify BERT pretrained model\n",
        "*  Specify GS bucket, create output directory for model checkpoints and eval results.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab_type": "code",
        "outputId": "8c031c49-32c0-41c9-b88f-71290f29b018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# TASK = 'CoLA' #@param {type:\"string\"}\n",
        "# assert TASK in ('MRPC', 'CoLA'), 'Only (MRPC, CoLA) are demonstrated here.'\n",
        "# Download glue data.\n",
        "# ! test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo\n",
        "# !python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=$TASK\n",
        "TASK = 'seq_prediction'\n",
        "!mkdir -p ./dataset\n",
        "!gsutil rsync gs://kaggle-195702-dataset/hk01 ./dataset\n",
        "\n",
        "# !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
        "# !unzip -o chinese_L-12_H-768_A-12.zip\n",
        "  \n",
        "TASK_DATA_DIR = 'dataset'\n",
        "print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
        "!ls $TASK_DATA_DIR\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "\n",
        "BERT_MODEL = 'chinese_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "# BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "# BERT_PRETRAINED_DIR = '/content/' + BERT_MODEL\n",
        "BERT_PRETRAINED_DIR = 'gs://dev-test-bert-tpu/chinese_bert/'\n",
        "\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "BUCKET = 'dev-test-bert-tpu' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert/models/{}'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "***** Task data directory: dataset *****\n",
            "article_contents.snappy.parquet\n",
            "***** BERT pretrained directory: gs://dev-test-bert-tpu/chinese_bert/ *****\n",
            "gs://dev-test-bert-tpu/chinese_bert/\n",
            "gs://dev-test-bert-tpu/chinese_bert/bert_config.json\n",
            "gs://dev-test-bert-tpu/chinese_bert/bert_model.ckpt.data-00000-of-00001\n",
            "gs://dev-test-bert-tpu/chinese_bert/bert_model.ckpt.index\n",
            "gs://dev-test-bert-tpu/chinese_bert/bert_model.ckpt.meta\n",
            "gs://dev-test-bert-tpu/chinese_bert/vocab.txt\n",
            "***** Model output directory: gs://dev-test-bert-tpu/bert/models/seq_prediction *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRuLgBmFHvHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  final_hidden = model.get_sequence_output()\n",
        "\n",
        "  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n",
        "  batch_size = final_hidden_shape[0]\n",
        "  seq_length = final_hidden_shape[1]\n",
        "  hidden_size = final_hidden_shape[2]\n",
        "  \n",
        "  print(batch_size)\n",
        "  print(seq_length)\n",
        "  print(hidden_size)\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"cls/squad/output_weights\", [1, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"cls/squad/output_bias\", [1], initializer=tf.zeros_initializer())\n",
        "\n",
        "  final_hidden_matrix = tf.reshape(final_hidden,\n",
        "                                   [batch_size * seq_length, hidden_size])\n",
        "  \n",
        "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
        "  logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "  logits = tf.reshape(logits, [batch_size, seq_length, 1])\n",
        "  logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "  unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "  (pred_logits) = (unstacked_logits[0])\n",
        "  \n",
        "  print(pred_logits)\n",
        "\n",
        "  return (pred_logits)\n",
        "    \n",
        "    \n",
        "\n",
        "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]    \n",
        "    truths = features[\"truths\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    pred = create_model(\n",
        "        bert_config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "  \n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
        "      total_loss = tf.reduce_mean(\n",
        "          tf.reduce_mean(\n",
        "              tf.keras.backend.binary_crossentropy(truths, pred), axis=-1))\n",
        "      \n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "      \n",
        "      print(truths)\n",
        "      print(pred)\n",
        "      print(total_loss)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    \n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      predictions = {\n",
        "          \"truths\": pred\n",
        "      }\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "  def get_train_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @classmethod\n",
        "  def _read_tsv(cls, input_file, quotechar=None):\n",
        "    \"\"\"Reads a tab separated value file.\"\"\"\n",
        "    with tf.gfile.Open(input_file, \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "      lines = []\n",
        "      for line in reader:\n",
        "        lines.append(line)\n",
        "      return lines\n",
        "\n",
        "\n",
        "class DataSetProcessor(DataProcessor):\n",
        "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "  def get_train_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_examples(\n",
        "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_examples(\n",
        "        self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
        "        \"dev_matched\")\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_examples(\n",
        "        self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "  def _create_examples(self, lines, set_type):\n",
        "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "    examples = []\n",
        "    for (i, line) in enumerate(lines):\n",
        "      if i == 0:\n",
        "        continue\n",
        "      guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[0]))\n",
        "      text_a = tokenization.convert_to_unicode(line[8])\n",
        "      text_b = tokenization.convert_to_unicode(line[9])\n",
        "      if set_type == \"test\":\n",
        "        label = \"contradiction\"\n",
        "      else:\n",
        "        label = tokenization.convert_to_unicode(line[-1])\n",
        "      examples.append(\n",
        "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "    return examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TcizdHx9KGvc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup task specific model and TPU running config.\n",
        "\n",
        "import modeling\n",
        "import optimization\n",
        "# import run_classifier\n",
        "import tokenization\n",
        "\n",
        "\n",
        "# Model Hyper Parameters\n",
        "TRAIN_BATCH_SIZE = 32 \n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 100.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "\n",
        "# processors = {\n",
        "#   \"cola\": run_classifier.ColaProcessor,\n",
        "#   \"mnli\": run_classifier.MnliProcessor,\n",
        "#   \"mrpc\": run_classifier.MrpcProcessor,\n",
        "# }\n",
        "# processor = processors[TASK.lower()]()\n",
        "# label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DV-8ggqON4bD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf60c78e-b465-48a6-ec06-050d32671cad"
      },
      "cell_type": "code",
      "source": [
        "! ls $BERT_PRETRAINED_DIR"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'gs://dev-test-bert-tpu/chinese_bert/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZOsIe62nKPEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z09LlbgrLwJ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OSyGuTVtLyqY",
        "outputId": "7d38bc73-853d-41ba-85f1-dc5a9495e085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_parquet('%s/article_contents.snappy.parquet'%TASK_DATA_DIR)[['article_id', 'main_content']].set_index('article_id')\n",
        "data = data[~data.main_content.isnull()]\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "TRAIN_SIZE = -1 #@param {type:\"number\"}\n",
        "    \n",
        "def get_train_examples():\n",
        "  examples = []\n",
        "  def append_example(row):\n",
        "    examples.append(InputExample(row.name, row.main_content))\n",
        "  \n",
        "  train[:TRAIN_SIZE].apply(append_example, axis=1)\n",
        "  return examples\n",
        "\n",
        "\n",
        "train_examples = get_train_examples()\n",
        "train_examples[:10], len(train_examples)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<__main__.InputExample at 0x7f09a711f908>,\n",
              "  <__main__.InputExample at 0x7f09a517bac8>,\n",
              "  <__main__.InputExample at 0x7f09a517bfd0>,\n",
              "  <__main__.InputExample at 0x7f09e5bca4e0>,\n",
              "  <__main__.InputExample at 0x7f09a1aa7630>,\n",
              "  <__main__.InputExample at 0x7f09a1aa7898>,\n",
              "  <__main__.InputExample at 0x7f09a1aa7860>,\n",
              "  <__main__.InputExample at 0x7f09a1aa7828>,\n",
              "  <__main__.InputExample at 0x7f09a1aa7940>,\n",
              "  <__main__.InputExample at 0x7f09a1aa78d0>],\n",
              " 174663)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "aXhSf4rxbQ1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09592931-84af-421b-80f0-581b99f4fd29"
      },
      "cell_type": "code",
      "source": [
        "# For char count frequency\n",
        "keras_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, oov_token='<oov>', split='')\n",
        "keras_tokenizer.fit_on_texts(train[:TRAIN_SIZE].main_content)\n",
        "len(keras_tokenizer.index_word), keras_tokenizer.word_index['<oov>']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12496, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "d2JA2KUucCnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4f8f5e41-25cf-4c64-fbb1-c4cbf7b0c5fa"
      },
      "cell_type": "code",
      "source": [
        "choice = []\n",
        "p = []\n",
        "\n",
        "for k, v in keras_tokenizer.word_counts.items():\n",
        "  if k in tokenizer.vocab:\n",
        "    choice.append(k)\n",
        "    p.append(v)\n",
        "\n",
        "p = p / np.sum(p)\n",
        "\n",
        "choice[:10], p[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['英', '超', '曼', '聯', '今', '晨', '主', '場', '出', '擊'],\n",
              " array([0.00076436, 0.00048691, 0.00012413, 0.00073225, 0.00169406,\n",
              "        0.00012515, 0.00210092, 0.00184992, 0.00373267, 0.00049875]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "lX3TblIXLAnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bdc8f7e5-4a6e-452d-a2fb-fbdc744f176e"
      },
      "cell_type": "code",
      "source": [
        "num_train_steps = int(\n",
        "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "print(num_train_steps)\n",
        "print(num_warmup_steps)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "545821\n",
            "54582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uu2dQ_TId-uH",
        "colab_type": "code",
        "outputId": "44cc7318-3a49-43ab-cc46-db30f6347eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f09a8076b70>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://dev-test-bert-tpu/bert/models/seq_prediction', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.63.176.194:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a5109128>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.63.176.194:8470', '_evaluation_master': b'grpc://10.63.176.194:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f09a7481e80>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FJfLMf7LaThW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokens_a = tokenizer.tokenize(train.iloc[0].main_content)  \n",
        "\n",
        "# MASK_PROB = 0.15 #@param {type:\"number\"}\n",
        "# assert MASK_PROB >= 0. and MASK_PROB <= 1\n",
        "# MASK_ERROR_PROB = 0.8 #@param {type:\"number\"}\n",
        "\n",
        "# random_tokens = np.random.choice(choice, len(tokens_a), p=p)\n",
        "# random_mask = np.random.choice([0, 1], len(tokens_a), p=[1-MASK_PROB, MASK_PROB])\n",
        "\n",
        "# aug_tokens_a = np.ma.array(tokens_a, mask=random_mask).filled(random_tokens)\n",
        "\n",
        "\n",
        "# tokens_a[:10], random_tokens[:10], random_mask[:10], aug_tokens_a[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z8V_eYSKMVao",
        "colab_type": "code",
        "outputId": "c2c152f6-d48f-4e79-cd84-c58d91cadcb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               truths,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.is_real_example = is_real_example\n",
        "    self.truths = truths\n",
        "\n",
        "    \n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "def convert_single_example(ex_index, example, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  \n",
        "  MASK_PROB = 0.15 #@param {type:\"number\"}\n",
        "  assert MASK_PROB >= 0. and MASK_PROB <= 1\n",
        "  MASK_ERROR_PROB = 0.8 #@param {type:\"number\"}\n",
        "\n",
        "  random_tokens = np.random.choice(choice, len(tokens_a), p=p)\n",
        "  random_mask = np.random.choice([0., 1.], len(tokens_a), p=[1-MASK_PROB, MASK_PROB])\n",
        "\n",
        "  aug_tokens_a = np.ma.array(tokens_a, mask=random_mask).filled(random_tokens)\n",
        "  \n",
        "  tokens_a = aug_tokens_a\n",
        "  tokens_a_truth = random_mask\n",
        "  \n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  truths = []\n",
        "  \n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(0.)\n",
        "  \n",
        "  for token, truth in zip(tokens_a, tokens_a_truth):\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(truth)\n",
        "    \n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(0.)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(0.)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "  assert len(truths) == max_seq_length\n",
        "\n",
        "  if ex_index < 3:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"truths: %s\" % \" \".join([str(x) for x in truths]))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      truths=truths\n",
        "  )\n",
        "  return feature\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 1000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features  \n",
        "\n",
        "print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "\n",
        "train_features = convert_examples_to_features(train_examples, MAX_SEQ_LENGTH, tokenizer)\n",
        "train_features[:10], len(train_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...\n",
            "INFO:tensorflow:Writing example 0 of 174663\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 3\n",
            "INFO:tensorflow:tokens: [CLS] 英 超 曼 聯 今 晨 主 場 昏 擊 ， 只 能 與 剛 剛 上 任 為 年 路 士 的 均 用 軒 迪 克 ， 和 0 : 0 ， 染 8 場 現 嘗 勝 績 ， 備 。 除 申 察 紅 魔 領 隊 雲 高 爾 賽 才 可 表 表 絕 不 辭 職 。 今 仗 曼 聯 ， 越 f 表 韋 恩 史 迪 加 停 賽 復 出 ， 新 車 路 士 亦 有 中 實 夏 薩 特 出 任 正 選 ， 主 隊 早 段 便 獲 得 黃 金 機 會 ， 但 中 場 桑 馬 達 及 前 鋒 安 症 尼 課 迪 爾 先 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 5739 6631 3294 5474 791 3247 712 1842 3210 3080 8024 1372 5543 5645 1190 1190 677 818 4158 2399 6662 1894 4638 1772 4500 6726 6832 1046 8024 1469 121 131 121 8024 3381 129 1842 4412 1655 1245 5245 8024 991 511 7370 4509 2175 5148 7795 7526 7386 7437 7770 4273 6555 2798 1377 6134 6134 5179 679 6798 5480 511 791 801 3294 5474 8024 6632 148 6134 7500 2617 1380 6832 1217 977 6555 2541 1139 8024 3173 6722 6662 1894 771 3300 704 2179 1909 5958 4294 1139 818 3633 6908 8024 712 7386 3193 3667 912 4363 2533 7941 7032 3582 3298 8024 852 704 1842 3433 7679 6888 1350 1184 7081 2128 4568 2225 6307 6832 4273 1044 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 6\n",
            "INFO:tensorflow:tokens: [CLS] 港 超 勁 時 自 華 宣 布 羅 致 前 厄 瓜 次 月 國 明 菲 力 斯 「 耶 （ [UNK] [UNK] ） ， 去 約 一 年 半 ； 足 主 京 央 勇 期 待 該 32 歲 「 鋒 可 提 利 球 隊 的 進 攻 能 力 ， 增 加 球 隊 的 爭 標 機 會 。 35 出 老 將 伊 達 北 上 菲 ， 斯 是 現 南 華 門 將 摩 拉 的 前 國 家 隊 隊 r ， 兩 人 曾 一 同 征 戰 2006 年 外 國 世 界 一 ， 他 該 屆 只 在 兒 幕 戰 對 德 國 時 上 陣 。 菲 力 斯 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 3949 6631 1233 3229 5632 5836 2146 2357 5397 5636 1184 1323 4478 3613 3299 1751 3209 5838 1213 3172 519 5456 8020 100 100 8021 8024 1343 5147 671 2399 1288 8039 6639 712 776 1925 1235 3309 2521 6283 8211 3641 519 7081 1377 2990 1164 4413 7386 4638 6868 3122 5543 1213 8024 1872 1217 4413 7386 4638 4261 3560 3582 3298 511 8198 1139 5439 2200 823 6888 1266 677 5838 8024 3172 3221 4412 1298 5836 7271 2200 3040 2861 4638 1184 1751 2157 7386 7386 160 8024 1060 782 3295 671 1398 2519 2782 8213 2399 1912 1751 686 4518 671 8024 800 6283 2234 1372 1762 1051 2391 2782 2205 2548 1751 3229 677 7369 511 5838 1213 3172 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 7\n",
            "INFO:tensorflow:tokens: [CLS] 阿 刻 奴 走 出 日 前 慘 吞 修 咸 頓 4 球 的 陰 影 ， 今 晨 主 場 憑 奧 斯 爾 一 傳 一 射 ， 以 鐵 ： 0 氣 走 般 尼 茅 夫 ， 以 19 的 39 分 升 上 l 首 ， 較 少 踢 一 場 的 李 斯 特 城 多 1 分 身 主 帥 雲 格 大 讚 奧 斯 爾 戲 賽 兵 工 廠 」 名 宿 家 金 的 影 子 。 主 場 出 擊 的 兵 工 廠 以 哥 林 張 伯 斯 、 基 比 爾 包 列 主 達 a 阿 歷 褥 張 伯 倫 及 基 蘭 基 比 斯 擔 正 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 7350 1174 1958 6624 1139 3189 1184 2711 1411 934 1496 7524 125 4413 4638 7374 2512 8024 791 3247 712 1842 2731 1953 3172 4273 671 1001 671 2198 8024 809 7136 8038 121 3706 6624 5663 2225 5747 1923 8024 809 8131 4638 8240 1146 1285 677 154 7674 8024 6733 2208 6677 671 1842 4638 3330 3172 4294 1814 1914 122 1146 6716 712 2371 7437 3419 1920 6367 1953 3172 4273 2783 6555 1070 2339 2449 520 1399 2162 2157 7032 4638 2512 2094 511 712 1842 1139 3080 4638 1070 2339 2449 809 1520 3360 2484 843 3172 510 1825 3683 4273 1259 1154 712 6888 143 7350 3644 6191 2484 843 961 1350 1825 5984 1825 3683 3172 3085 3633 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:Writing example 1000 of 174663\n",
            "INFO:tensorflow:Writing example 2000 of 174663\n",
            "INFO:tensorflow:Writing example 3000 of 174663\n",
            "INFO:tensorflow:Writing example 4000 of 174663\n",
            "INFO:tensorflow:Writing example 5000 of 174663\n",
            "INFO:tensorflow:Writing example 6000 of 174663\n",
            "INFO:tensorflow:Writing example 7000 of 174663\n",
            "INFO:tensorflow:Writing example 8000 of 174663\n",
            "INFO:tensorflow:Writing example 9000 of 174663\n",
            "INFO:tensorflow:Writing example 10000 of 174663\n",
            "INFO:tensorflow:Writing example 11000 of 174663\n",
            "INFO:tensorflow:Writing example 12000 of 174663\n",
            "INFO:tensorflow:Writing example 13000 of 174663\n",
            "INFO:tensorflow:Writing example 14000 of 174663\n",
            "INFO:tensorflow:Writing example 15000 of 174663\n",
            "INFO:tensorflow:Writing example 16000 of 174663\n",
            "INFO:tensorflow:Writing example 17000 of 174663\n",
            "INFO:tensorflow:Writing example 18000 of 174663\n",
            "INFO:tensorflow:Writing example 19000 of 174663\n",
            "INFO:tensorflow:Writing example 20000 of 174663\n",
            "INFO:tensorflow:Writing example 21000 of 174663\n",
            "INFO:tensorflow:Writing example 22000 of 174663\n",
            "INFO:tensorflow:Writing example 23000 of 174663\n",
            "INFO:tensorflow:Writing example 24000 of 174663\n",
            "INFO:tensorflow:Writing example 25000 of 174663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5JtebpyKjzaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_truths = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_truths.append(feature.truths)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    print(num_examples)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, \n",
        "                shape=[num_examples, seq_length],\n",
        "                name='input_ids_tf_constant',\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                name='input_mask_tf_constant',\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                name='segment_ids_tf_constant',\n",
        "                dtype=tf.int32),\n",
        "        \"truths\":\n",
        "            tf.constant(\n",
        "                all_truths, \n",
        "                shape=[num_examples, seq_length], \n",
        "                name='truths_tf_constant',\n",
        "                dtype=tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5U_c8s2AvhgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eoXRtSPZvdiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Eval the model.\n",
        "eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
        "eval_features = run_classifier.convert_examples_to_features(\n",
        "    eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(eval_examples)))\n",
        "print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "# Eval will be slightly WRONG on the TPU because it will truncate\n",
        "# the last batch.\n",
        "eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "eval_input_fn = run_classifier.input_fn_builder(\n",
        "    features=eval_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=True)\n",
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "  print(\"***** Eval results *****\")\n",
        "  for key in sorted(result.keys()):\n",
        "    print('  {} = {}'.format(key, str(result[key])))\n",
        "    writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dla1K-9_oQmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}