{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_dataset.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "f-zLlGUTnd5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pyarrow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_M-EQPcRnnV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbfc2ujRnxBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tokenization\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2DIHh1CnsXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BERT_PRETRAINED_DIR = 'gs://dev-test-bert-tpu/chinese_bert/'\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "BERT_MODEL = 'chinese_L-12_H-768_A-12'\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JpGbR2Pzu3Gh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5c6f9e70-ea10-4968-b749-52369688daf4"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ./dataset && gsutil rsync gs://dev-test-bert-tpu/dataset ./dataset\n",
        "  \n",
        "article_contents = pd.read_csv('./dataset/article_contents.csv').set_index('article_id')\n",
        "article_contents = article_contents[~article_contents.main_content.isnull()]\n",
        "article_contents.head()\n",
        "\n",
        "train, test = train_test_split(article_contents, test_size=0.2, shuffle=False)\n",
        "\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "TRAIN_SIZE = -1 #@param {type:\"number\"}\n",
        "    \n",
        "def get_train_examples():\n",
        "  examples = []\n",
        "  def append_example(row):\n",
        "    examples.append(InputExample(row.name, row.main_content))\n",
        "  \n",
        "  train[:TRAIN_SIZE].apply(append_example, axis=1)\n",
        "  return examples\n",
        "\n",
        "\n",
        "train_examples = get_train_examples()\n",
        "train_examples[:10], len(train_examples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<__main__.InputExample at 0x7fa8780aa9e8>,\n",
              "  <__main__.InputExample at 0x7fa8780aa978>,\n",
              "  <__main__.InputExample at 0x7fa8780aa908>,\n",
              "  <__main__.InputExample at 0x7fa8780aac18>,\n",
              "  <__main__.InputExample at 0x7fa8780aac50>,\n",
              "  <__main__.InputExample at 0x7fa8780aaba8>,\n",
              "  <__main__.InputExample at 0x7fa8780aac88>,\n",
              "  <__main__.InputExample at 0x7fa8780aab70>,\n",
              "  <__main__.InputExample at 0x7fa8780aacc0>,\n",
              "  <__main__.InputExample at 0x7fa8780aacf8>],\n",
              " 174663)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "k25CdQKJ0LbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ee9aed16-3935-408e-fdec-88e4040bf128"
      },
      "cell_type": "code",
      "source": [
        "# For char count frequency\n",
        "keras_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, oov_token='<oov>', split='')\n",
        "keras_tokenizer.fit_on_texts(train[:TRAIN_SIZE].main_content)\n",
        "len(keras_tokenizer.index_word), keras_tokenizer.word_index['<oov>']\n",
        "\n",
        "choice = []\n",
        "p = []\n",
        "\n",
        "for k, v in keras_tokenizer.word_counts.items():\n",
        "  if k in tokenizer.vocab:\n",
        "    choice.append(k)\n",
        "    p.append(v)\n",
        "\n",
        "p = p / np.sum(p)\n",
        "\n",
        "choice[:10], p[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['英', '超', '曼', '聯', '今', '晨', '主', '場', '出', '擊'],\n",
              " array([0.00076436, 0.00048691, 0.00012413, 0.00073225, 0.00169406,\n",
              "        0.00012515, 0.00210092, 0.00184992, 0.00373267, 0.00049875]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "DvpQOd0NybbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3573
        },
        "outputId": "69bfb92d-a9e1-4a64-b66f-b5495659b57f"
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               truths,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.is_real_example = is_real_example\n",
        "    self.truths = truths\n",
        "\n",
        "    \n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "def convert_single_example(ex_index, example, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  \n",
        "  MASK_PROB = 0.15 #@param {type:\"number\"}\n",
        "  assert MASK_PROB >= 0. and MASK_PROB <= 1\n",
        "  MASK_ERROR_PROB = 0.8 #@param {type:\"number\"}\n",
        "\n",
        "  random_tokens = np.random.choice(choice, len(tokens_a), p=p)\n",
        "  random_mask = np.random.choice([0., 1.], len(tokens_a), p=[1-MASK_PROB, MASK_PROB])\n",
        "\n",
        "  aug_tokens_a = np.ma.array(tokens_a, mask=random_mask).filled(random_tokens)\n",
        "  \n",
        "  tokens_a = aug_tokens_a\n",
        "  tokens_a_truth = random_mask\n",
        "  \n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  truths = []\n",
        "  \n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(0.)\n",
        "  \n",
        "  for token, truth in zip(tokens_a, tokens_a_truth):\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(truth)\n",
        "    \n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "  truths.append(0.)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "    truths.append(0.)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "  assert len(truths) == max_seq_length\n",
        "\n",
        "  if ex_index < 3:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"truths: %s\" % \" \".join([str(x) for x in truths]))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      truths=truths\n",
        "  )\n",
        "  return feature\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 1000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features  \n",
        "\n",
        "print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "\n",
        "MAX_SEQ_LENGTH = 128 #@param {'type': 'number'}\n",
        "\n",
        "train_features = convert_examples_to_features(train_examples, MAX_SEQ_LENGTH, tokenizer)\n",
        "train_features[:10], len(train_features)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...\n",
            "INFO:tensorflow:Writing example 0 of 174663\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 3\n",
            "INFO:tensorflow:tokens: [CLS] 英 超 完 聯 今 晨 主 場 出 擊 ， 只 能 大 剛 明 上 在 ， 車 路 士 的 體 隊 軒 迪 克 打 和 0 : 0 示 近 8 場 干 嘗 勝 績 ， 備 受 批 評 的 紅 魔 領 隊 雲 高 爾 賽 後 上 表 示 絕 不 辭 職 。 今 運 曼 聯 有 中 場 舒 韋 恩 有 迪 加 停 賽 c 出 ， 而 車 路 士 亦 有 中 場 用 炎 特 出 可 正 選 ， 主 隊 早 u 便 獲 省 黃 金 機 會 ， 但 中 場 桑 馬 士 及 前 鋒 安 東 尼 馬 迪 爾 先 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 5739 6631 2130 5474 791 3247 712 1842 1139 3080 8024 1372 5543 1920 1190 3209 677 1762 8024 6722 6662 1894 4638 7768 7386 6726 6832 1046 2802 1469 121 131 121 4850 6818 129 1842 2397 1655 1245 5245 8024 991 1358 2821 6268 4638 5148 7795 7526 7386 7437 7770 4273 6555 2527 677 6134 4850 5179 679 6798 5480 511 791 6880 3294 5474 3300 704 1842 5653 7500 2617 3300 6832 1217 977 6555 145 1139 8024 5445 6722 6662 1894 771 3300 704 1842 4500 4142 4294 1139 1377 3633 6908 8024 712 7386 3193 163 912 4363 4689 7941 7032 3582 3298 8024 852 704 1842 3433 7679 1894 1350 1184 7081 2128 3346 2225 7679 6832 4273 1044 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 6\n",
            "INFO:tensorflow:tokens: [CLS] 港 超 勁 旅 南 華 宣 文 羅 致 前 厄 瓜 多 e 之 腳 趕 o 斯 保 耶 （ [UNK] [UNK] ） 客 簽 約 一 年 半 ； 足 主 張 廣 勇 期 待 該 32 歲 前 鋒 可 提 說 球 隊 的 進 攻 能 力 ， 》 加 球 隊 的 爭 標 機 會 。 35 訴 v 將 伊 達 級 上 菲 力 斯 是 並 南 華 門 將 摩 拉 ， 球 國 家 隊 隊 友 ， 兩 人 曾 一 同 征 戰 當 年 德 國 世 界 盃 ， 他 該 六 只 在 揭 幕 ， 對 呼 國 時 上 陣 然 菲 ， 斯 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 3949 6631 1233 3180 1298 5836 2146 3152 5397 5636 1184 1323 4478 1914 147 722 5589 6634 157 3172 924 5456 8020 100 100 8021 2145 5087 5147 671 2399 1288 8039 6639 712 2484 2451 1235 3309 2521 6283 8211 3641 1184 7081 1377 2990 6303 4413 7386 4638 6868 3122 5543 1213 8024 518 1217 4413 7386 4638 4261 3560 3582 3298 511 8198 6260 164 2200 823 6888 5159 677 5838 1213 3172 3221 699 1298 5836 7271 2200 3040 2861 8024 4413 1751 2157 7386 7386 1351 8024 1060 782 3295 671 1398 2519 2782 4534 2399 2548 1751 686 4518 4656 8024 800 6283 1063 1372 1762 2999 2391 8024 2205 1461 1751 3229 677 7369 4197 5838 8024 3172 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: 7\n",
            "INFO:tensorflow:tokens: [CLS] 阿 仙 奴 走 出 日 威 慘 必 修 咸 頓 經 （ 的 陰 影 態 士 晨 主 場 憑 奧 0 爾 一 傳 一 射 ， 以 2 ： 0 氣 走 般 尼 茅 夫 ， 以 19 戰 39 分 升 上 榜 首 要 較 少 踢 一 場 的 、 斯 b 城 多 障 分 ， 主 帥 雲 格 大 l 奧 再 爾 有 「 在 工 廠 」 名 宿 柏 金 的 影 子 。 主 元 出 擊 的 兵 工 廠 以 哥 林 誤 伯 斯 、 基 比 爾 包 列 斯 顯 、 阿 歷 士 張 伯 倫 及 基 蘭 基 比 斯 擔 a [SEP]\n",
            "INFO:tensorflow:input_ids: 101 7350 803 1958 6624 1139 3189 2014 2711 2553 934 1496 7524 5195 8020 4638 7374 2512 2706 1894 3247 712 1842 2731 1953 121 4273 671 1001 671 2198 8024 809 123 8038 121 3706 6624 5663 2225 5747 1923 8024 809 8131 2782 8240 1146 1285 677 3528 7674 6206 6733 2208 6677 671 1842 4638 510 3172 144 1814 1914 7397 1146 8024 712 2371 7437 3419 1920 154 1953 1086 4273 3300 519 1762 2339 2449 520 1399 2162 3377 7032 4638 2512 2094 511 712 1039 1139 3080 4638 1070 2339 2449 809 1520 3360 6299 843 3172 510 1825 3683 4273 1259 1154 3172 7549 510 7350 3644 1894 2484 843 961 1350 1825 5984 1825 3683 3172 3085 143 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:truths: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
            "INFO:tensorflow:Writing example 1000 of 174663\n",
            "INFO:tensorflow:Writing example 2000 of 174663\n",
            "INFO:tensorflow:Writing example 3000 of 174663\n",
            "INFO:tensorflow:Writing example 4000 of 174663\n",
            "INFO:tensorflow:Writing example 5000 of 174663\n",
            "INFO:tensorflow:Writing example 6000 of 174663\n",
            "INFO:tensorflow:Writing example 7000 of 174663\n",
            "INFO:tensorflow:Writing example 8000 of 174663\n",
            "INFO:tensorflow:Writing example 9000 of 174663\n",
            "INFO:tensorflow:Writing example 10000 of 174663\n",
            "INFO:tensorflow:Writing example 11000 of 174663\n",
            "INFO:tensorflow:Writing example 12000 of 174663\n",
            "INFO:tensorflow:Writing example 13000 of 174663\n",
            "INFO:tensorflow:Writing example 14000 of 174663\n",
            "INFO:tensorflow:Writing example 15000 of 174663\n",
            "INFO:tensorflow:Writing example 16000 of 174663\n",
            "INFO:tensorflow:Writing example 17000 of 174663\n",
            "INFO:tensorflow:Writing example 18000 of 174663\n",
            "INFO:tensorflow:Writing example 19000 of 174663\n",
            "INFO:tensorflow:Writing example 20000 of 174663\n",
            "INFO:tensorflow:Writing example 21000 of 174663\n",
            "INFO:tensorflow:Writing example 22000 of 174663\n",
            "INFO:tensorflow:Writing example 23000 of 174663\n",
            "INFO:tensorflow:Writing example 24000 of 174663\n",
            "INFO:tensorflow:Writing example 25000 of 174663\n",
            "INFO:tensorflow:Writing example 26000 of 174663\n",
            "INFO:tensorflow:Writing example 27000 of 174663\n",
            "INFO:tensorflow:Writing example 28000 of 174663\n",
            "INFO:tensorflow:Writing example 29000 of 174663\n",
            "INFO:tensorflow:Writing example 30000 of 174663\n",
            "INFO:tensorflow:Writing example 31000 of 174663\n",
            "INFO:tensorflow:Writing example 32000 of 174663\n",
            "INFO:tensorflow:Writing example 33000 of 174663\n",
            "INFO:tensorflow:Writing example 34000 of 174663\n",
            "INFO:tensorflow:Writing example 35000 of 174663\n",
            "INFO:tensorflow:Writing example 36000 of 174663\n",
            "INFO:tensorflow:Writing example 37000 of 174663\n",
            "INFO:tensorflow:Writing example 38000 of 174663\n",
            "INFO:tensorflow:Writing example 39000 of 174663\n",
            "INFO:tensorflow:Writing example 40000 of 174663\n",
            "INFO:tensorflow:Writing example 41000 of 174663\n",
            "INFO:tensorflow:Writing example 42000 of 174663\n",
            "INFO:tensorflow:Writing example 43000 of 174663\n",
            "INFO:tensorflow:Writing example 44000 of 174663\n",
            "INFO:tensorflow:Writing example 45000 of 174663\n",
            "INFO:tensorflow:Writing example 46000 of 174663\n",
            "INFO:tensorflow:Writing example 47000 of 174663\n",
            "INFO:tensorflow:Writing example 48000 of 174663\n",
            "INFO:tensorflow:Writing example 49000 of 174663\n",
            "INFO:tensorflow:Writing example 50000 of 174663\n",
            "INFO:tensorflow:Writing example 51000 of 174663\n",
            "INFO:tensorflow:Writing example 52000 of 174663\n",
            "INFO:tensorflow:Writing example 53000 of 174663\n",
            "INFO:tensorflow:Writing example 54000 of 174663\n",
            "INFO:tensorflow:Writing example 55000 of 174663\n",
            "INFO:tensorflow:Writing example 56000 of 174663\n",
            "INFO:tensorflow:Writing example 57000 of 174663\n",
            "INFO:tensorflow:Writing example 58000 of 174663\n",
            "INFO:tensorflow:Writing example 59000 of 174663\n",
            "INFO:tensorflow:Writing example 60000 of 174663\n",
            "INFO:tensorflow:Writing example 61000 of 174663\n",
            "INFO:tensorflow:Writing example 62000 of 174663\n",
            "INFO:tensorflow:Writing example 63000 of 174663\n",
            "INFO:tensorflow:Writing example 64000 of 174663\n",
            "INFO:tensorflow:Writing example 65000 of 174663\n",
            "INFO:tensorflow:Writing example 66000 of 174663\n",
            "INFO:tensorflow:Writing example 67000 of 174663\n",
            "INFO:tensorflow:Writing example 68000 of 174663\n",
            "INFO:tensorflow:Writing example 69000 of 174663\n",
            "INFO:tensorflow:Writing example 70000 of 174663\n",
            "INFO:tensorflow:Writing example 71000 of 174663\n",
            "INFO:tensorflow:Writing example 72000 of 174663\n",
            "INFO:tensorflow:Writing example 73000 of 174663\n",
            "INFO:tensorflow:Writing example 74000 of 174663\n",
            "INFO:tensorflow:Writing example 75000 of 174663\n",
            "INFO:tensorflow:Writing example 76000 of 174663\n",
            "INFO:tensorflow:Writing example 77000 of 174663\n",
            "INFO:tensorflow:Writing example 78000 of 174663\n",
            "INFO:tensorflow:Writing example 79000 of 174663\n",
            "INFO:tensorflow:Writing example 80000 of 174663\n",
            "INFO:tensorflow:Writing example 81000 of 174663\n",
            "INFO:tensorflow:Writing example 82000 of 174663\n",
            "INFO:tensorflow:Writing example 83000 of 174663\n",
            "INFO:tensorflow:Writing example 84000 of 174663\n",
            "INFO:tensorflow:Writing example 85000 of 174663\n",
            "INFO:tensorflow:Writing example 86000 of 174663\n",
            "INFO:tensorflow:Writing example 87000 of 174663\n",
            "INFO:tensorflow:Writing example 88000 of 174663\n",
            "INFO:tensorflow:Writing example 89000 of 174663\n",
            "INFO:tensorflow:Writing example 90000 of 174663\n",
            "INFO:tensorflow:Writing example 91000 of 174663\n",
            "INFO:tensorflow:Writing example 92000 of 174663\n",
            "INFO:tensorflow:Writing example 93000 of 174663\n",
            "INFO:tensorflow:Writing example 94000 of 174663\n",
            "INFO:tensorflow:Writing example 95000 of 174663\n",
            "INFO:tensorflow:Writing example 96000 of 174663\n",
            "INFO:tensorflow:Writing example 97000 of 174663\n",
            "INFO:tensorflow:Writing example 98000 of 174663\n",
            "INFO:tensorflow:Writing example 99000 of 174663\n",
            "INFO:tensorflow:Writing example 100000 of 174663\n",
            "INFO:tensorflow:Writing example 101000 of 174663\n",
            "INFO:tensorflow:Writing example 102000 of 174663\n",
            "INFO:tensorflow:Writing example 103000 of 174663\n",
            "INFO:tensorflow:Writing example 104000 of 174663\n",
            "INFO:tensorflow:Writing example 105000 of 174663\n",
            "INFO:tensorflow:Writing example 106000 of 174663\n",
            "INFO:tensorflow:Writing example 107000 of 174663\n",
            "INFO:tensorflow:Writing example 108000 of 174663\n",
            "INFO:tensorflow:Writing example 109000 of 174663\n",
            "INFO:tensorflow:Writing example 110000 of 174663\n",
            "INFO:tensorflow:Writing example 111000 of 174663\n",
            "INFO:tensorflow:Writing example 112000 of 174663\n",
            "INFO:tensorflow:Writing example 113000 of 174663\n",
            "INFO:tensorflow:Writing example 114000 of 174663\n",
            "INFO:tensorflow:Writing example 115000 of 174663\n",
            "INFO:tensorflow:Writing example 116000 of 174663\n",
            "INFO:tensorflow:Writing example 117000 of 174663\n",
            "INFO:tensorflow:Writing example 118000 of 174663\n",
            "INFO:tensorflow:Writing example 119000 of 174663\n",
            "INFO:tensorflow:Writing example 120000 of 174663\n",
            "INFO:tensorflow:Writing example 121000 of 174663\n",
            "INFO:tensorflow:Writing example 122000 of 174663\n",
            "INFO:tensorflow:Writing example 123000 of 174663\n",
            "INFO:tensorflow:Writing example 124000 of 174663\n",
            "INFO:tensorflow:Writing example 125000 of 174663\n",
            "INFO:tensorflow:Writing example 126000 of 174663\n",
            "INFO:tensorflow:Writing example 127000 of 174663\n",
            "INFO:tensorflow:Writing example 128000 of 174663\n",
            "INFO:tensorflow:Writing example 129000 of 174663\n",
            "INFO:tensorflow:Writing example 130000 of 174663\n",
            "INFO:tensorflow:Writing example 131000 of 174663\n",
            "INFO:tensorflow:Writing example 132000 of 174663\n",
            "INFO:tensorflow:Writing example 133000 of 174663\n",
            "INFO:tensorflow:Writing example 134000 of 174663\n",
            "INFO:tensorflow:Writing example 135000 of 174663\n",
            "INFO:tensorflow:Writing example 136000 of 174663\n",
            "INFO:tensorflow:Writing example 137000 of 174663\n",
            "INFO:tensorflow:Writing example 138000 of 174663\n",
            "INFO:tensorflow:Writing example 139000 of 174663\n",
            "INFO:tensorflow:Writing example 140000 of 174663\n",
            "INFO:tensorflow:Writing example 141000 of 174663\n",
            "INFO:tensorflow:Writing example 142000 of 174663\n",
            "INFO:tensorflow:Writing example 143000 of 174663\n",
            "INFO:tensorflow:Writing example 144000 of 174663\n",
            "INFO:tensorflow:Writing example 145000 of 174663\n",
            "INFO:tensorflow:Writing example 146000 of 174663\n",
            "INFO:tensorflow:Writing example 147000 of 174663\n",
            "INFO:tensorflow:Writing example 148000 of 174663\n",
            "INFO:tensorflow:Writing example 149000 of 174663\n",
            "INFO:tensorflow:Writing example 150000 of 174663\n",
            "INFO:tensorflow:Writing example 151000 of 174663\n",
            "INFO:tensorflow:Writing example 152000 of 174663\n",
            "INFO:tensorflow:Writing example 153000 of 174663\n",
            "INFO:tensorflow:Writing example 154000 of 174663\n",
            "INFO:tensorflow:Writing example 155000 of 174663\n",
            "INFO:tensorflow:Writing example 156000 of 174663\n",
            "INFO:tensorflow:Writing example 157000 of 174663\n",
            "INFO:tensorflow:Writing example 158000 of 174663\n",
            "INFO:tensorflow:Writing example 159000 of 174663\n",
            "INFO:tensorflow:Writing example 160000 of 174663\n",
            "INFO:tensorflow:Writing example 161000 of 174663\n",
            "INFO:tensorflow:Writing example 162000 of 174663\n",
            "INFO:tensorflow:Writing example 163000 of 174663\n",
            "INFO:tensorflow:Writing example 164000 of 174663\n",
            "INFO:tensorflow:Writing example 165000 of 174663\n",
            "INFO:tensorflow:Writing example 166000 of 174663\n",
            "INFO:tensorflow:Writing example 167000 of 174663\n",
            "INFO:tensorflow:Writing example 168000 of 174663\n",
            "INFO:tensorflow:Writing example 169000 of 174663\n",
            "INFO:tensorflow:Writing example 170000 of 174663\n",
            "INFO:tensorflow:Writing example 171000 of 174663\n",
            "INFO:tensorflow:Writing example 172000 of 174663\n",
            "INFO:tensorflow:Writing example 173000 of 174663\n",
            "INFO:tensorflow:Writing example 174000 of 174663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<__main__.InputFeatures at 0x7fa8780b90f0>,\n",
              "  <__main__.InputFeatures at 0x7fa874faaf98>,\n",
              "  <__main__.InputFeatures at 0x7fa874faafd0>,\n",
              "  <__main__.InputFeatures at 0x7fa8783adeb8>,\n",
              "  <__main__.InputFeatures at 0x7fa8780b93c8>,\n",
              "  <__main__.InputFeatures at 0x7fa8780b9320>,\n",
              "  <__main__.InputFeatures at 0x7fa8780b9400>,\n",
              "  <__main__.InputFeatures at 0x7fa874fb20f0>,\n",
              "  <__main__.InputFeatures at 0x7fa874fb2470>,\n",
              "  <__main__.InputFeatures at 0x7fa874fb24a8>],\n",
              " 174663)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "oaLMQIY_1Ks2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "01496c93-ce9a-4f93-df88-af9c0832264f"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('./dataset/train_features.pkl', 'wb') as f:\n",
        "  pickle.dump(train_features, f)\n",
        "  \n",
        "!gsutil rsync ./dataset gs://dev-test-bert-tpu/dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying file://./dataset/train_features.pkl [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\\n",
            "Operation completed over 1 objects/807.2 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YuOKP7kd2Em_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTMxs6GlwFLG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def generate_dataset(corpus, error_prob):\n",
        "#   for article in corpus:\n",
        "#     seqs = tokenizer.tokenize(article)\n",
        "    \n",
        "#   padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=SEQ_LENGTH, padding='post', truncating='post', value=0.0)\n",
        "  \n",
        "#   index_word_count = [(tokenizer.word_index[k], v) for k, v in tokenizer.word_counts.items()]\n",
        "#   index_word_count.append((0, sum([x[1] for x in index_word_count]) / (error_prob) * (1 - error_prob)))\n",
        "  \n",
        "#   choice = np.array([x[0] for x in index_word_count])\n",
        "\n",
        "#   p = np.array([x[1] for x in index_word_count])\n",
        "#   p = p / np.sum(p)\n",
        "  \n",
        "#   # for each sequence, randomly replace the index\n",
        "#   random_error = np.random.choice(choice, padded_seqs.shape, p=p)\n",
        "#   random_error_mask = (random_error > 0).astype(int)\n",
        "  \n",
        "#   inp = padded_seqs * (1-random_error_mask) + random_error\n",
        "#   out = random_error_mask\n",
        "  \n",
        "#   return inp, out\n",
        "\n",
        "# ERROR_PROB = 0.15 #@param {'type': 'number'}\n",
        "# generate_dataset(article_contents.main_content, ERROR_PROB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDqNZ8aRnvip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1123
        },
        "outputId": "b9f9fd79-7af8-4016-c6f5-57c78b6a4b3f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# dataset = tf.data.TextLineDataset(['gs://dev-test-bert-tpu/dataset/article_contents.txt'])\n",
        "# # dataset = dataset.map(tf.map_fn(tokenize, ))\n",
        "\n",
        "# def tokenize(line):\n",
        "#   return tokenizer.tokenize(line)\n",
        "\n",
        "# dataset = dataset.map(lambda article: tf.py_func(tokenize, [article], [tf.string]))\n",
        "# # dataset.output_classes == (tf.Tensor,)\n",
        "# # dataset.output_types == (tf.string,)\n",
        "# # dataset.output_shapes == ([128],)\n",
        "\n",
        "# # dataset = dataset.map(lambda article: tf.py_func(tokenize, [article], [list]))\n",
        "# # dataset.output_classes == (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor)\n",
        "# # dataset.output_types == (tf.string, tf.float32, tf.string, tf.float64)\n",
        "# # dataset.output_shapes == ([], [], [3], [2])\n",
        "\n",
        "# # dataset.map(lambda x: tokenizer.tokenize(x))\n",
        "\n",
        "# iterator = dataset.make_one_shot_iterator()\n",
        "# next_element = iterator.get_next()\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   for i in range(10):\n",
        "#     value = sess.run(next_element)\n",
        "#     print(value.decode('utf-8'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: pyfunc_3 returns 205 values, but expects to see 1 values.\n\t [[{{node PyFunc}} = PyFunc[Tin=[DT_STRING], Tout=[DT_STRING], token=\"pyfunc_3\"](arg0)]]\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bb1b144a7c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: pyfunc_3 returns 205 values, but expects to see 1 values.\n\t [[{{node PyFunc}} = PyFunc[Tin=[DT_STRING], Tout=[DT_STRING], token=\"pyfunc_3\"](arg0)]]\n\t [[node IteratorGetNext (defined at <ipython-input-11-bb1b144a7c66>:20)  = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9Vnhm4vkwD3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fY-T4SeFol93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}