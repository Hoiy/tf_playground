{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "title_vec = pd.read_csv('hn_title_norm_vec.csv', nrows=NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin = pd.read_csv('/Users/Shared/data/HN_posts_year_to_Sep_26_2016.csv')\n",
    "origin = origin[['id', 'num_points']]\n",
    "data = pd.merge(title_vec, origin, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsentence_length = pd.read_csv('sentence_length.csv', header=None, )\\nsentence_length.rename(columns={0: 'id', 1:'length'}, inplace=True)\\ndata = pd.merge(data, sentence_length, how='inner', on=['id'])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sentence_length = pd.read_csv('sentence_length.csv', header=None, )\n",
    "sentence_length.rename(columns={0: 'id', 1:'length'}, inplace=True)\n",
    "data = pd.merge(data, sentence_length, how='inner', on=['id'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    293119.000000\n",
      "mean          0.039370\n",
      "std           0.194473\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: good, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "GOOD_THRESHOLD = 100\n",
    "\n",
    "data['good'] = data['num_points'].apply(lambda x: 1 if x >= GOOD_THRESHOLD else 0)\n",
    "print(data['good'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'num_points'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.8)\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "good = train[train['good'] == 1]\n",
    "bad = train[train['good'] == 0].head(good.shape[0])\n",
    "trainer = good.append(bad)\n",
    "train = trainer.reindex(np.random.permutation(trainer.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndecoded = RepeatVector(timesteps)(encoded)\\n#decoded = LSTM(input_dim, return_sequences=True)(encoded)\\ndecoded = LSTM(input_dim, return_sequences=True)(decoded)\\n\\nsequence_autoencoder = Model(inputs, decoded)\\nencoder = Model(inputs, encoded)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector, Dense, Activation, Bidirectional, Dropout, Convolution1D, GlobalMaxPooling1D, Flatten, MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "input_dim = 50\n",
    "timesteps = 24\n",
    "latent_dim = 200\n",
    "\n",
    "predictor = Sequential()\n",
    "#predictor.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(timesteps, input_dim), merge_mode='ave'))\n",
    "#predictor.add(LSTM(100, input_shape=(timesteps, input_dim)))\n",
    "predictor.add(Convolution1D(250, 3, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "predictor.add(GlobalMaxPooling1D())\n",
    "#predictor.add(Flatten())\n",
    "predictor.add(Dense(250))\n",
    "predictor.add(Activation('relu'))\n",
    "\n",
    "predictor.add(Dense(2))\n",
    "predictor.add(Activation('softmax'))\n",
    "\n",
    "'''\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label = ['good']\n",
    "\n",
    "X = train.drop(label, axis=1)\n",
    "Y = test.drop(label, axis=1)\n",
    "\n",
    "X_label = to_categorical(train[label].as_matrix(), 2)\n",
    "Y_label = to_categorical(test[label].as_matrix(), 2)\n",
    "\n",
    "X = X.as_matrix().reshape(train.shape[0], 24, 50)\n",
    "Y = Y.as_matrix().reshape(test.shape[0], 24, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18552 samples, validate on 58624 samples\n",
      "Epoch 1/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.6825 - acc: 0.5581 - precision: 0.5581 - recall: 0.5581 - fmeasure: 0.5581 - val_loss: 0.6376 - val_acc: 0.6793 - val_precision: 0.6793 - val_recall: 0.6793 - val_fmeasure: 0.6793\n",
      "Epoch 2/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.6602 - acc: 0.5991 - precision: 0.5991 - recall: 0.5991 - fmeasure: 0.5991 - val_loss: 0.7368 - val_acc: 0.4717 - val_precision: 0.4717 - val_recall: 0.4717 - val_fmeasure: 0.4717\n",
      "Epoch 3/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.6303 - acc: 0.6420 - precision: 0.6420 - recall: 0.6420 - fmeasure: 0.6420 - val_loss: 0.6873 - val_acc: 0.5751 - val_precision: 0.5751 - val_recall: 0.5751 - val_fmeasure: 0.5751\n",
      "Epoch 4/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.5917 - acc: 0.6792 - precision: 0.6792 - recall: 0.6792 - fmeasure: 0.6792 - val_loss: 0.6019 - val_acc: 0.6734 - val_precision: 0.6734 - val_recall: 0.6734 - val_fmeasure: 0.6734\n",
      "Epoch 5/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.5329 - acc: 0.7315 - precision: 0.7315 - recall: 0.7315 - fmeasure: 0.7315 - val_loss: 0.8139 - val_acc: 0.5167 - val_precision: 0.5167 - val_recall: 0.5167 - val_fmeasure: 0.5167\n",
      "Epoch 6/10\n",
      "18552/18552 [==============================] - 13s - loss: 0.4719 - acc: 0.7761 - precision: 0.7761 - recall: 0.7761 - fmeasure: 0.7761 - val_loss: 1.1356 - val_acc: 0.3921 - val_precision: 0.3921 - val_recall: 0.3921 - val_fmeasure: 0.3921\n",
      "Epoch 7/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.4148 - acc: 0.8063 - precision: 0.8063 - recall: 0.8063 - fmeasure: 0.8063 - val_loss: 0.7600 - val_acc: 0.6227 - val_precision: 0.6227 - val_recall: 0.6227 - val_fmeasure: 0.6227\n",
      "Epoch 8/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.3411 - acc: 0.8526 - precision: 0.8526 - recall: 0.8526 - fmeasure: 0.8526 - val_loss: 1.3361 - val_acc: 0.4389 - val_precision: 0.4389 - val_recall: 0.4389 - val_fmeasure: 0.4389\n",
      "Epoch 9/10\n",
      "18552/18552 [==============================] - 14s - loss: 0.2999 - acc: 0.8731 - precision: 0.8731 - recall: 0.8731 - fmeasure: 0.8731 - val_loss: 0.7959 - val_acc: 0.6714 - val_precision: 0.6714 - val_recall: 0.6714 - val_fmeasure: 0.6714\n",
      "Epoch 10/10\n",
      "18552/18552 [==============================] - 15s - loss: 0.2497 - acc: 0.8988 - precision: 0.8988 - recall: 0.8988 - fmeasure: 0.8988 - val_loss: 1.6075 - val_acc: 0.4561 - val_precision: 0.4561 - val_recall: 0.4561 - val_fmeasure: 0.4561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10e12ce10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'precision', 'recall', 'fmeasure'])\n",
    "predictor.fit(X, X_label, validation_data=(Y, Y_label), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
