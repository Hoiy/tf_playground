{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101120, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "pages = pd.read_csv('./pages.csv', nrows=NROWS)\n",
    "pages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>about</th>\n",
       "      <th>fan_count</th>\n",
       "      <th>category</th>\n",
       "      <th>website</th>\n",
       "      <th>talking_about_count</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36203</th>\n",
       "      <td>Lifehack.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Make your life better in every aspect</td>\n",
       "      <td>1211390</td>\n",
       "      <td>Media/News Company</td>\n",
       "      <td>http://lifehack.org</td>\n",
       "      <td>37292</td>\n",
       "      <td>109565802405671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name description                                  about  \\\n",
       "36203  Lifehack.org         NaN  Make your life better in every aspect   \n",
       "\n",
       "       fan_count            category               website  \\\n",
       "36203    1211390  Media/News Company  http://lifehack.org    \n",
       "\n",
       "       talking_about_count               id  \n",
       "36203                37292  109565802405671  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[pages['name'] == 'Lifehack.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import TextPreprocessing.Language.detector as ld\n",
    "pages['lang_name'] = pages['name'].apply(ld.detect)\n",
    "pages['lang_description'] = pages['description'].astype(str).apply(ld.detect)\n",
    "pages['lang_about'] = pages['about'].astype(str).apply(ld.detect)\n",
    "\n",
    "#%timeit pages['name'].apply(lambda x: ld.detect(x, 'langdetect'))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x))\n",
    "#%timeit pages['about'].astype(str).apply(lambda x: ld.detect(x, 'langdetect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47230, 12)\n",
      "(22790, 12)\n"
     ]
    }
   ],
   "source": [
    "eng_pages = pages[pages['lang_about'] == 'en']\n",
    "print(eng_pages.shape)\n",
    "eng_pages = eng_pages[eng_pages['lang_description'] == 'en']\n",
    "print(eng_pages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "x = np.linspace(0.0000001, 0.0002, 1000)\n",
    "plt.hist(posts['share_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.0000001, 0.002, 1000)\n",
    "plt.hist(posts['like_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.0000001, 0.0002, 1000)\n",
    "plt.hist(posts['comment_ratio'], bins=x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_posts = posts[posts['type'] == 'link']\n",
    "link_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def isEng(x):\n",
    "    try:\n",
    "        return langid.classify(x['message'])[0] == 'en'\n",
    "    except:\n",
    "        try:\n",
    "            return detect(x['description'])[0] == 'en'\n",
    "        except:\n",
    "            try:\n",
    "                return detect(x['name'])[0] == 'en'\n",
    "            except:\n",
    "                return False\n",
    "    \n",
    "\n",
    "eng_link_posts = link_posts[link_posts.apply(isEng, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_link_posts.shape\n",
    "backup = eng_link_posts\n",
    "data = eng_link_posts\n",
    "eng_link_posts.iloc[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def isEng(x):\n",
    "    try:\n",
    "        return detect(x['message']) == 'en'\n",
    "    except:\n",
    "        try:\n",
    "            return detect(x['description']) == 'en'\n",
    "        except:\n",
    "            try:\n",
    "                return detect(x['name']) == 'en'\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "eng_link_posts = link_posts[link_posts.apply(isEng, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "eng_link_posts.to_csv('/Users/Shared/fb/eng_link_posts.csv', doublequote=False, quotechar='\"', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/Shared/fb/eng_link_posts.csv', nrows=10000, error_bad_lines=False, doublequote=False, quotechar='\"', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_link_posts[['name']].to_csv('/Users/Shared/fb/eng_link_posts_name.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_link_posts['data'] = eng_link_posts['name'].astype(str)\n",
    "eng_link_posts['share_ratio'].describe()\n",
    "eng_link_posts['good'] = eng_link_posts['share_ratio'].apply(lambda x: 1 if x >= 0.0001 else 0)\n",
    "eng_link_posts['good'].describe()\n",
    "data = eng_link_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from codeDump.testHeadline import tokenize\n",
    "tokenize(data['data'])\n",
    "#test_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word vector by fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = eng_link_posts[['like', 'facebook_id', 'name', 'like_ratio']]\n",
    "data['like_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['good'] = data['like_ratio'].apply(lambda x: 1 if x > 0.0005 else 0)\n",
    "data['good'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"name\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data[\"name\"] = data[\"name\"].astype('str')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"name\"])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('/Users/Shared/fb/eng_link_posts_name_model.vec'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(df):    \n",
    "    good = df[df[\"good\"] == 1]\n",
    "    bad = df[df[\"good\"] == 0]\n",
    "    bad = bad.sample(n=good.shape[0])\n",
    "    data = good.append(bad)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    y_train = np.zeros((data.shape[0], 2), dtype=int)\n",
    "    for i in range(0, data.shape[0]):\n",
    "        y_train[i, 1] = 1 if data.iloc[i]['good'] == 1 else 0\n",
    "        y_train[i, 0] = 1 if data.iloc[i]['good'] == 0 else 0\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(data[\"name\"])\n",
    "    x_train = pad_sequences(sequences, maxlen=25)\n",
    "\n",
    "    return x_train, y_train, data['good'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_full, y_full, y2_full = prepareData(eng_link_posts)\n",
    "x_train, y_train, y2_train = prepareData(train)\n",
    "x_test, y_test, y2_test = prepareData(test)\n",
    "\n",
    "print(y2_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def validate(model, x_test, y_test):\n",
    "    test_truth = np.apply_along_axis(lambda x: np.argmax(x), 1, y_test)\n",
    "    test_pred = model.predict(x_test)\n",
    "    test_pred = np.apply_along_axis(lambda x: np.argmax(x), 1, test_pred)\n",
    "    precision = precision_score(test_truth, test_pred)\n",
    "    recall = recall_score(test_truth, test_pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall\n",
    "\n",
    "def validate_2(truth, pred):\n",
    "    truth = np.apply_along_axis(lambda x: np.argmax(x), 1, truth)\n",
    "    pred = np.apply_along_axis(lambda x: np.argmax(x), 1, pred)\n",
    "    precision = precision_score(truth, pred)\n",
    "    recall = recall_score(truth, pred)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, criterion=\"entropy\", random_state=1)\n",
    "rf.fit(x_train, y2_train)\n",
    "\n",
    "res = rf.predict(x_test)\n",
    "\n",
    "#validate_2(y_test, res)\n",
    "precision = precision_score(y2_test, res)\n",
    "recall = recall_score(y2_test, res)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin = pd.read_csv('/Users/Shared/data/HN_posts_year_to_Sep_26_2016.csv')\n",
    "origin_1 = origin[['id', 'num_points']]\n",
    "data = pd.merge(title_vec, origin_1, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sentence_length = pd.read_csv('sentence_length.csv', header=None, )\n",
    "sentence_length.rename(columns={0: 'id', 1:'length'}, inplace=True)\n",
    "data = pd.merge(data, sentence_length, how='inner', on=['id'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOOD_THRESHOLD = 10\n",
    "\n",
    "data['good'] = data['num_points'].apply(lambda x: 1 if x >= GOOD_THRESHOLD else 0)\n",
    "data['good'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'num_points'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.8)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "abnormal = data.loc[data['good'] == 1]\n",
    "abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = train[list(range(0, 1200))].as_matrix()\n",
    "u, s, v = np.linalg.svd(m, full_matrices=False)\n",
    "plt.plot(s[0:400])\n",
    "plt.show()\n",
    "\n",
    "print(u.shape, s.shape, v.shape)\n",
    "print(m[0].shape, v.transpose().shape)\n",
    "\n",
    "transform = np.delete(v, range(2, v.shape[0]), 0)\n",
    "print(transform.shape)\n",
    "\n",
    "def reduce(vec):\n",
    "    return np.dot(vec, transform.transpose())\n",
    "\n",
    "def to_coor(matrix):\n",
    "    coor = np.apply_along_axis(reduce, 1, matrix).transpose()\n",
    "    print(matrix.shape, coor.shape)\n",
    "    return (coor[0], coor[1])\n",
    "    \n",
    "normal_x, normal_y = to_coor(m)\n",
    "\n",
    "m2 = abnormal[list(range(0,1200))].as_matrix()\n",
    "\n",
    "ab_x, ab_y = to_coor(m2)\n",
    "\n",
    "print(ab_x.shape)\n",
    "print(ab_y.shape)\n",
    "\n",
    "plt.scatter(normal_x, normal_y)\n",
    "plt.scatter(ab_x, ab_y, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m2 = abnormal.as_matrix()\n",
    "u, s, v = np.linalg.svd(m2)\n",
    "plt.plot(s[0:400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "origin['num_points']\n",
    "\n",
    "plot_data = origin[origin['num_points'] >= 0 ]['num_points'].values\n",
    "\n",
    "x = np.array(list(range(100, 1001)))\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "xdata = x_bin\n",
    "ydata = y\n",
    "\n",
    "print(xdata.shape)\n",
    "print(ydata.shape)\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata, p0=[0.0007, 0.0004])\n",
    "\n",
    "print(popt)\n",
    "print(pcov)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=10)\n",
    "plt.plot(x_bin, y, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import expon\n",
    "loc, lamb = expon.fit(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "plt.hist(plot_data, bins=x_bin)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a[(a>1) & (a<5)]=0\n",
    "a\n",
    "\n",
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Study double / triple exponential behaviour of the distribution\n",
    "\n",
    "x = np.linspace(0.0000001, 1, 100000)\n",
    "\n",
    "def log_tower(plot_data, k):\n",
    "    for i in range(0, k):\n",
    "        plot_data[plot_data < 1] = 1.\n",
    "        plot_data = np.log(plot_data)\n",
    "    #plot_data[plot_data < 1] = 0.\n",
    "    return plot_data\n",
    "\n",
    "ln_plot_data = log_tower(np.array(plot_data), 3)\n",
    "\n",
    "print(ln_plot_data.shape)\n",
    "\n",
    "y, x = np.histogram(ln_plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the elbow of exponential curve\n",
    "\n",
    "x = np.linspace(0, 100, 10000)\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. Compute the derivative\n",
    "\n",
    "def derivative(np_array):\n",
    "    a = np.delete(np_array, len(np_array)-1)\n",
    "    b = np.delete(np_array, 0)\n",
    "    return a-b\n",
    "\n",
    "d_y = derivative(derivative(y))\n",
    "d_x_bin = np.linspace(0, len(d_y), len(d_y))\n",
    "\n",
    "print(x_bin.shape)\n",
    "print(d_y.shape)\n",
    "print(d_x_bin.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(d_x_bin, d_y)\n",
    "plt.show()\n",
    "\n",
    "print(np.argmax(d_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find elbow from curve estimate\n",
    "\n",
    "origin['num_points']\n",
    "\n",
    "plot_data = origin[origin['num_points'] >= 0 ]['num_points'].values\n",
    "\n",
    "x = np.linspace(0, 20, 20)\n",
    "y, x = np.histogram(plot_data, bins=x)\n",
    "x_bin = np.delete(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_bin, y)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "xdata = x_bin\n",
    "ydata = y\n",
    "\n",
    "print(xdata.shape)\n",
    "print(ydata.shape)\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata, p0=[0.0007, 0.0004])\n",
    "\n",
    "print(popt)\n",
    "print(pcov)\n",
    "plt.plot(x_bin, func(x_bin,*popt), color='g', linewidth=10)\n",
    "plt.plot(x_bin, y, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = popt\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# second derivative of a * e^(-bx) is ab^2e^(-bx)\n",
    "\n",
    "def firstD(a,b,x):\n",
    "    return - a * b * np.exp(-b * x)\n",
    "\n",
    "\n",
    "def secondD(a,b,x):\n",
    "    return a * np.square(b) * np.exp(-b * x)\n",
    "\n",
    "plt.plot(x_bin, secondD(a,b,x_bin))\n",
    "plt.plot(x_bin, firstD(a,b,x_bin))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "input_dim = 50\n",
    "timesteps = 24\n",
    "latent_dim = 200\n",
    "\n",
    "predictor = Sequential()\n",
    "predictor.add(LSTM(100, input_shape=(timesteps, input_dim)))\n",
    "predictor.add(Dense(100, init='normal', activation='tanh'))\n",
    "predictor.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "X = train.drop(['good'], axis=1)\n",
    "Y = test.drop(['good'], axis=1)\n",
    "\n",
    "X = X.as_matrix().reshape(train.shape[0], 24, 50)\n",
    "Y = Y.as_matrix().reshape(test.shape[0], 24, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', 'precision', 'recall', 'fmeasure'])\n",
    "predictor.fit(X, train['good'], validation_data=(Y, test['good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sequence_autoencoder.evaluate(Y,test['good'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
