{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline\n",
    "import time\n",
    "from DataLoader import GloVe\n",
    "from TextPreprocess.sequences import Sequences\n",
    "from TextPreprocess.Tokenizer.RegExp import tokenize\n",
    "import Utils.pandas_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "WORD_DIM = 100\n",
    "WORD_COUNT = 400000+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Loading Glove Model\n",
      "End: Loaded 400000 rows.\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe.load2('./data/GloVe/glove.6B.{}d.txt'.format(WORD_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: 100\n",
      "Embedding Symbols: 400003\n",
      "Index to symbol: [(0, 'camcorders'), (1, '50-38'), (2, 'someway'), (3, 'many-body'), (4, 'osh'), (5, 'striper'), (6, 'mixedly'), (7, 'haylee'), (8, 'flier'), (9, '48.59')]\n"
     ]
    }
   ],
   "source": [
    "# emb: Symbol to float32 of fixed DIMENSION\n",
    "# Create an index mapping, index to symbol, symbol to index\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, emb, verbose = False):\n",
    "        # assert emb is dictionary and each entry has same dimension\n",
    "        self.emb = emb\n",
    "        self.dim = len(self.emb[list(self.emb.keys())[0]])\n",
    "        self.emb['<UNK>'] = [0. for i in range(self.dim)]\n",
    "        self.emb['<PAD>'] = [1. for i in range(self.dim)]\n",
    "        self.emb['<GO>'] = [-1. for i in range(self.dim)]\n",
    "        \n",
    "        self.build_dicts()\n",
    "        \n",
    "        if verbose:\n",
    "            self.describe()\n",
    "        \n",
    "    def describe(self):\n",
    "        print('Embedding Dimension: {}'.format(self.dim))\n",
    "        print('Embedding Symbols: {}'.format(len(self.emb)))\n",
    "        print('Index to symbol: {}'.format([(i, self.idx2Sym[i]) for i in range(10)]))\n",
    "        \n",
    "    def getIndex(self, symbol):\n",
    "        if symbol in self.sym2Idx:\n",
    "            return self.sym2Idx[symbol]\n",
    "        else:\n",
    "            return self.sym2Idx['<UNK>']\n",
    "\n",
    "    def getEmb(self, symbol):\n",
    "        return self.emb[self.idx2Sym[self.getIndex(symbol)]]\n",
    "    \n",
    "    def getSymbols(self, indices):\n",
    "        return [self.idx2Sym[idx] for idx in indices]\n",
    "\n",
    "    def getNumpyArray(self):\n",
    "        return np.array([self.emb[self.idx2Sym[idx]] for idx in range(len(self.emb))])\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        self.sym2Idx = {}\n",
    "        index = 0\n",
    "        for key in self.emb.keys():\n",
    "            self.sym2Idx[key] = index\n",
    "            index += 1\n",
    "            \n",
    "        self.idx2Sym = { v:k for k, v in self.sym2Idx.items()}\n",
    "\n",
    "glove_emb = Embedding(glove, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = file.read('data/Quora/train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.question1 = df.question1.astype(str)\n",
    "df.question2 = df.question2.astype(str)\n",
    "df.is_duplicate = df.is_duplicate.astype(float)\n",
    "\n",
    "df = df.as_matrix(['question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "data = {}\n",
    "data['train'], data['test'] = train_test_split(df, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessQuestion(string):\n",
    "    try:\n",
    "        return [glove_emb.getIndex(token.lower()) for token in tokenize(string)]\n",
    "    except:\n",
    "        print(string)\n",
    "\n",
    "\n",
    "def preprocessData(data):\n",
    "    return [[preprocessQuestion(rec[0]), preprocessQuestion(rec[1]), float(rec[2])] for rec in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ['train', 'test']:\n",
    "    data[i] = preprocessData(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turns iteratable of symbols into padded batch\n",
    "from functools import lru_cache\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.verbose = verbose\n",
    "        self.size = len(self.seqs)\n",
    "        self.seq_lens = [len(seq) for seq in self.seqs]\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.describe()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def max_length(self):\n",
    "        return max(self.seq_lens)\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def longgest_sequence(self):\n",
    "        for seq in self.seqs:\n",
    "            if len(seq) == self.max_length():\n",
    "                return seq\n",
    "    \n",
    "    def describe(self):\n",
    "        print('Size: {}'.format(self.size))\n",
    "        print(\"Longest sequence length: {}\".format(self.max_length()))\n",
    "        bin_width = max(1, self.max_length() // 30)\n",
    "        plt.hist(self.seq_lens, range(0, self.max_length() + bin_width, bin_width))\n",
    "        plt.title('Sequence length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def batchPadding(self, batch, padding_symbol):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), padding_symbol)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = batch[i]\n",
    "        return result\n",
    "\n",
    "    def batchMask(self, batch):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), 0.0)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = 1.0\n",
    "        return result\n",
    "        \n",
    "    # Same length within the batch, stuffed with padding symbol\n",
    "    def generator(self, padding_symbol, batch_size=None, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        length = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for seq in self.seqs:\n",
    "                train.append([sym for sym in seq])\n",
    "                length.append(len(seq))\n",
    "                if(len(train) == batch_size):\n",
    "                    yield self.batchPadding(train, padding_symbol), length, self.batchMask(train)\n",
    "                    train = []\n",
    "                    length = []\n",
    "            epouch -= 1\n",
    "            if self.verbose:\n",
    "                print('epouch done...')\n",
    "                \n",
    "class Batcher2:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.size = len(self.seqs)\n",
    "\n",
    "    def generator(self, batch_size=32, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for sym in self.seqs:\n",
    "                train.append([sym])\n",
    "                if(len(train) == batch_size):\n",
    "                    yield train\n",
    "                    train = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')\n",
    "            \n",
    "            \n",
    "# Turn data into batch, where data is iterable over records, record is iterable over fields\n",
    "class Batcher3:\n",
    "    def __init__(self, data):\n",
    "        #assert it is doubly iterable\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        \n",
    "    def generator(self, batch_size = 32, epouch = -1):\n",
    "        batch = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for record in self.data:\n",
    "                batch.append(record)\n",
    "                if(len(batch) == batch_size):\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batcher = {}\n",
    "for i in ['train', 'test']:\n",
    "    batcher[i] = Batcher3(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LV1_DIM = 10\n",
    "LV2_STEP = 1\n",
    "LV2_DIM = 150\n",
    "\n",
    "DROPOUT_RNN = 0.1\n",
    "DROP_DENSE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embeddings_initializer(shape):\n",
    "    with tf.variable_scope(\"Embeddings_Initializer\"):\n",
    "        in_emb = tf.placeholder(\n",
    "            dtype = tf.float32, \n",
    "            shape = shape, \n",
    "            name = \"Placeholder\"\n",
    "        )\n",
    "        \n",
    "        emb = tf.Variable(\n",
    "            tf.constant(0.0, shape = shape), \n",
    "            trainable=False, \n",
    "            name = 'Embeddings', \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        init_emb = emb.assign(in_emb)\n",
    "    return in_emb, init_emb, emb\n",
    "\n",
    "def cells_initializer(num_units, reuse):\n",
    "    with tf.variable_scope(\"Cells_Initializer\"):\n",
    "        cells = tf.contrib.rnn.GRUCell(\n",
    "            num_units = num_units,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "    return cells\n",
    "\n",
    "\n",
    "#IN (batch, time, 1)\n",
    "def simple_embedding(inputs, embeddings):\n",
    "    with tf.variable_scope(\"Simple_Embedding\"):\n",
    "        lookup = tf.nn.embedding_lookup(\n",
    "            params = embeddings,\n",
    "            ids = inputs,\n",
    "            partition_strategy='mod',\n",
    "            name='Embedding_Lookup',\n",
    "            validate_indices=True,\n",
    "            max_norm=None\n",
    "        )\n",
    "\n",
    "    return lookup\n",
    "\n",
    "#OUT: (batch, time, dim) float32\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_dynamic_rnn(cell, inputs, lengths):\n",
    "    with tf.variable_scope(\"Simple_Dynamic_RNN\"):        \n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        step_size = tf.shape(inputs)[1]\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell, \n",
    "            inputs, \n",
    "            dtype = tf.float32, \n",
    "            sequence_length = lengths,\n",
    "            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        indices = tf.range(0, batch_size) * step_size + (lengths - 1)\n",
    "        gather = tf.reshape(\n",
    "            tf.gather(\n",
    "                tf.reshape(outputs, [-1, cell.output_size]), indices\n",
    "            ), \n",
    "            [-1, cell.output_size]\n",
    "         )\n",
    "        \n",
    "    return gather\n",
    "#OUT (batch, dim)\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_encoder(inputs, input_lengths, embeddings, dropout = 0.0, reuse = None):\n",
    "    with tf.variable_scope('Simple_Encoder'):\n",
    "        \n",
    "        emb = simple_embedding(inputs, embeddings)\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell,\n",
    "            input_keep_prob = 1. - dropout,\n",
    "            output_keep_prob = 1. - dropout,\n",
    "            state_keep_prob = 1. - dropout,\n",
    "            variational_recurrent=False,\n",
    "            input_size=None,\n",
    "            dtype=None,\n",
    "            seed=None\n",
    "        )\n",
    "\n",
    "        rnn = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = emb,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn, emb\n",
    "            \n",
    "        #\n",
    "        # Conv layer does not support dynamic length ;/\n",
    "        #\n",
    "    \"\"\"\n",
    "        filter_2 = tf.Variable(\n",
    "            tf.random_uniform([2, WORD_DIM, LV1_DIM], -1, 1), \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        #IN (batch, time, dim)\n",
    "        conv_2 = tf.nn.conv1d(\n",
    "            value = inputs,\n",
    "            filters = filter_2,\n",
    "            stride = 1,\n",
    "            padding = 'VALID',\n",
    "            use_cudnn_on_gpu=True,\n",
    "            data_format=None,\n",
    "            name='Conv_Witdh_2'\n",
    "        )\n",
    "        #OUT (batch, time-1, dim)\n",
    "\n",
    "    with tf.variable_scope('Level_2_RNN'):\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size=None,\n",
    "            activation=tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        rnn_output_2 = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = inputs,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn_output_2\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#OUT (batch, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Concatenate, Reshape\n",
    "from keras.layers import Conv1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "num_dense = 130\n",
    "\n",
    "conv_dim = 10\n",
    "\n",
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(WORD_COUNT,\n",
    "        WORD_DIM,\n",
    "        weights=[glove_emb.getNumpyArray()],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(200, dropout=0.3, recurrent_dropout=0.3)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "\n",
    "rnn_step = 2\n",
    "\n",
    "# create same conv for both\n",
    "conv = [[Conv1D(conv_dim, j+2, padding='same') for j in range(2)] for i in range(rnn_step)]\n",
    "\n",
    "# create conv1d for each step and each length\n",
    "feat_1 = [[conv[i][j](embedded_sequences_1) for j in range(2)] for i in range(rnn_step)]\n",
    "feat_1 = [Reshape((1, conv_dim * MAX_SEQUENCE_LENGTH * 2))(Concatenate()(feat_1[i])) for i in range(rnn_step)]\n",
    "feat_1 = Concatenate(1)(feat_1)\n",
    "\n",
    "x1 = lstm_layer(feat_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "\n",
    "# create conv1d for each step and each length\n",
    "feat_2 = [[conv[i][j](embedded_sequences_2) for j in range(2)] for i in range(rnn_step)]\n",
    "feat_2 = [Reshape((1, MAX_SEQUENCE_LENGTH * conv_dim * 2))(Concatenate()(feat_2[i])) for i in range(rnn_step)]\n",
    "feat_2 = Concatenate(1)(feat_2)\n",
    "\n",
    "y1 = lstm_layer(feat_2)\n",
    "\n",
    "merged = Concatenate()([x1, y1])\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[sequence_1_input, sequence_2_input],\n",
    "    outputs=preds\n",
    ")\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1 = {}\n",
    "q2 = {}\n",
    "label = {}\n",
    "\n",
    "for i in ['train', 'test']:\n",
    "    q1[i] = [rec[0] for rec in data[i]]\n",
    "    q1[i] += [rec[1] for rec in data[i]]\n",
    "    \n",
    "    q1[i] = pad_sequences(q1[i], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    q2[i] = [rec[1] for rec in data[i]]\n",
    "    q2[i] += [rec[0] for rec in data[i]]\n",
    "    \n",
    "    q2[i] = pad_sequences(q2[i], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    label[i] = [rec[2] for rec in data[i]]\n",
    "    label[i] += [rec[2] for rec in data[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727722\n",
      "727722\n",
      "727722\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[ 1.          1.          1.          1.30902834  1.          1.30902834\n",
      "  1.30902834  1.30902834  1.30902834  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(len(q1['train']))\n",
    "print(len(q2['train']))\n",
    "print(len(label['train']))\n",
    "\n",
    "weight_val = np.ones(len(label['test']))\n",
    "#weight_val *= 0.472001959\n",
    "for i in range(len(label['test'])):\n",
    "    if label['test'][i] == 0:\n",
    "        weight_val[i] = 1.309028344\n",
    "\n",
    "class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "\n",
    "print(label['test'][:10])\n",
    "print(weight_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='./Tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "hist = model.fit(\n",
    "    [q1['train'], q2['train']], \n",
    "    label['train'],\n",
    "    validation_data=([q1['test'], q2['test']], label['test']),\n",
    "    epochs=200, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    "    #callbacks=[tbCallBack]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.variable_scope(\"Inputs_Layer\"):\n",
    "    \n",
    "        #IN\n",
    "        in_word_indices = [tf.placeholder(tf.int32, (None, None), name = \"Q{}_Word_Indices\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch, time) int32\n",
    "        \n",
    "        #batch_size = [tf.shape(inputs[i], name= \"Q{}_Batch_Size\".format(i+1))[0] for i in range(2)]\n",
    "        #steps = [tf.shape(inputs[i], name= \"Q{}_Steps\".format(i+1))[1] for i in range(2)]\n",
    "        \n",
    "        #IN\n",
    "        in_lengths = [tf.placeholder(tf.int32, (None), name = \"Q{}_Lengths\".format(i+1)) for i in range(2)]\n",
    "        #OUT: (batch) int32\n",
    "        \n",
    "        #in_word_indices = tf.placeholder(tf.int32, (None, None), name = \"Q_Word_Indices\")\n",
    "        \n",
    "        #in_lengths = tf.placeholder(tf.int32, (None), name = \"Q_Lengths\")\n",
    "        \n",
    "        in_truth = tf.placeholder(tf.float32, (None, 1), name = \"Truth\")\n",
    "\n",
    "    with tf.variable_scope(\"Embeddings_Layer\"):\n",
    "        in_emb, init_emb, emb = embeddings_initializer((WORD_COUNT, WORD_DIM))\n",
    "        \n",
    "    with tf.variable_scope(\"Encoder_Layer\"):\n",
    "        enc_0, debug_emb = simple_encoder(in_word_indices[0], in_lengths[0], emb, dropout=DROPOUT_RNN)\n",
    "        enc_1, _ = simple_encoder(in_word_indices[1], in_lengths[1], emb, dropout=DROPOUT_RNN, reuse=True)\n",
    "\n",
    "    #IN: (batch, dim) x2\n",
    "    with tf.variable_scope(\"Prediction_Layer\"):\n",
    "        #pred_in = [tf.placeholder(tf.float32, (None, LV2_DIM)) for i in range(2)]\n",
    "        #pred_input = tf.concat([pred_in[0], pred_in[1]], 1)\n",
    "        pred_input = tf.concat([enc_0, enc_1], 1)\n",
    "        \n",
    "        pred_weights = tf.Variable(tf.random_uniform([LV2_DIM * 2, 1], -1, 1), name='weights')\n",
    "        pred_bias = tf.Variable(tf.constant(0.0, shape=[1]), name=\"bias\")\n",
    "        \n",
    "        pred = tf.nn.sigmoid(pred_input @ pred_weights + pred_bias)\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(in_truth, pred))\n",
    "    acc = tf.reduce_mean(tf.contrib.keras.metrics.binary_accuracy(in_truth, pred))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_graph(graph.as_graph_def())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tvars_vals = sess.run(tvars)\n",
    "\n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name, val)  # Prints the name of the variable alongside its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 2000000\n",
    "MODEL = './model/q.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def batchPadding(batch, padding_symbol):\n",
    "    size = max([len(record) for record in batch])\n",
    "    result = np.full((len(batch), size), padding_symbol)\n",
    "    for i in range(len(batch)):\n",
    "        result[i][:len(batch[i])] = batch[i]\n",
    "    return result\n",
    "\n",
    "def proc_batch(batch):\n",
    "    q1 = batchPadding([record[0] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q2 = batchPadding([record[1] for record in batch], glove_emb.getIndex('<PAD>'))\n",
    "    q1_lens = [len(record[0]) for record in batch]\n",
    "    q2_lens = [len(record[1]) for record in batch]\n",
    "    truth = [[record[2]] for record in batch]\n",
    "    \n",
    "    return q1, q2, q1_lens, q2_lens, truth\n",
    "\n",
    "gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    gen[i] = batcher[i].generator(batch_size=BATCH_SIZE, epouch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(graph, model_file, batch_size = 32, force_restart=False, epouch = -1):\n",
    "    \n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        try:\n",
    "            if force_restart:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                print('Started training...')\n",
    "            else:\n",
    "                saver.restore(sess, model_file)\n",
    "                print('Restored training...')\n",
    "        except:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Failed to restore training, restarting training...')\n",
    "\n",
    "        \n",
    "        sess.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "        \n",
    "        epouch_start = time.time()\n",
    "        epouch_count = 0\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            \n",
    "            batcher = Batcher3(data['train'])\n",
    "            total = batcher.size\n",
    "            generator = batcher.generator(batch_size = batch_size, epouch = 1)\n",
    "            count = 0\n",
    "            total_loss = 0\n",
    "            print('Epouch {}...'.format(epouch_count + 1))\n",
    "            for batch in generator:\n",
    "\n",
    "                start = time.time()\n",
    "                train_q1, train_q2, train_q1_lengths, train_q2_lengths, train_label = proc_batch(batch)\n",
    "\n",
    "                feed_dict = {\n",
    "                    in_word_indices[0]: train_q1,\n",
    "                    in_word_indices[1]: train_q2,\n",
    "                    in_lengths[0]: train_q1_lengths,\n",
    "                    in_lengths[1]: train_q2_lengths,\n",
    "                    in_truth: train_label\n",
    "                }\n",
    "\n",
    "                _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                end = time.time()\n",
    "                count += batch_size\n",
    "                total_loss += loss_val\n",
    "                print(\"Progress: {} / {} ({:2.1%}), Avg. Loss: {:.5f}, Batch Loss: {:.5f}, Time / Batch: {:.2f}s, Time Remains: {:.2f}s\".format(\n",
    "                    count, \n",
    "                    total, \n",
    "                    count / total, \n",
    "                    total_loss / count * batch_size, \n",
    "                    loss_val,\n",
    "                    end - start,\n",
    "                    (time.time() - epouch_start) / count * total - (time.time() - epouch_start)\n",
    "                ), end='\\x1b[2K\\r')\n",
    "                #print(\"Progress: {0} / {1}, Loss: {:.2f}, Time / Batch: {:.2f}s, Time Remains: {:.2f}s\".format(count, total, loss_val, end - start, (end - start) * (total - count) / batch), end=\"\\r\")\n",
    "\n",
    "            epouch -= 1\n",
    "            epouch_count += 1\n",
    "            \n",
    "\n",
    "run_graph(graph, MODEL, batch_size=32, force_restart=True)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 1000\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    try:\n",
    "        saver.restore(session, MODEL)\n",
    "        print('Restored training...')\n",
    "        #session.run(tf.global_variables_initializer())\n",
    "        #print('Restarting training...')\n",
    "    except:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Restarting training...')\n",
    "        \n",
    "\n",
    "    \n",
    "    #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    #run_metadata = tf.RunMetadata()\n",
    "    \n",
    "    #tvars_vals = session.run(tvars)\n",
    "    #for var, val in zip(tvars, tvars_vals):\n",
    "    #    print(var.name, val)  # Prints the name of the variable alongside its value.\n",
    "\n",
    "    #for name in session.run( tf.report_uninitialized_variables( tf.global_variables( ) ) ):\n",
    "    #    print(name)\n",
    "    \n",
    "    \n",
    "    average_loss = 0\n",
    "    average_max_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        train_q1, train_q2, train_q1_lengths, train_q2_lengths, train_label = proc_batch(next(gen['train']))\n",
    "        \n",
    "        feed_dict = {\n",
    "            in_word_indices[0]: train_q1,\n",
    "            in_word_indices[1]: train_q2,\n",
    "            in_lengths[0]: train_q1_lengths,\n",
    "            in_lengths[1]: train_q2_lengths,\n",
    "            in_truth: train_label\n",
    "        }\n",
    "        \n",
    "        #_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "                avg_loss_val = 0\n",
    "                avg_acc_val = 0\n",
    "                \n",
    "                gen['test'] = Batcher3(data['test']).generator(epouch = 1, batch_size=1024)\n",
    "                count = 0\n",
    "                for batch in gen['test']:\n",
    "                    test_q1, test_q2, test_q1_lengths, test_q2_lengths, test_label = proc_batch(batch)\n",
    "                    feed_dict = {\n",
    "                        in_word_indices[0]: test_q1,\n",
    "                        in_word_indices[1]: test_q2,\n",
    "                        in_lengths[0]: test_q1_lengths,\n",
    "                        in_lengths[1]: test_q2_lengths,\n",
    "                        in_truth: test_label\n",
    "                    }\n",
    "\n",
    "                    loss_val, acc_val = session.run([loss, acc], feed_dict=feed_dict)\n",
    "                    avg_loss_val += loss_val\n",
    "                    avg_acc_val += acc_val\n",
    "                    count += 1\n",
    "                \n",
    "                print('Testing Set batch loss: {0}, acc {1}:'.format(avg_loss_val/count, avg_acc_val/count))\n",
    "                \n",
    "                start = time.time()\n",
    "                \n",
    "                \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            save_path = saver.save(session, MODEL)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            # Create the Timeline object, and write it to a json\n",
    "            #tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            #ctf = tl.generate_chrome_trace_format()\n",
    "            #with open('timeline.json', 'w') as f:\n",
    "            #    f.write(ctf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_SIZE = 100\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, MODEL)\n",
    "    print('Restored model...')\n",
    "    \n",
    "    average_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "        test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "        test_label = next(label_gen['test'])\n",
    "        \n",
    "        feed_dict = {\n",
    "            inputs[0]: test_q1,\n",
    "            inputs[1]: test_q2,\n",
    "            input_lengths[0]: test_q1_lengths,\n",
    "            input_lengths[1]: test_q2_lengths,\n",
    "            truth: test_label\n",
    "        }\n",
    "        \n",
    "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % DEBUG_SIZE == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= DEBUG_SIZE\n",
    "                print('Total time for {0} steps: {1:.2f}s, each step: {2:.2f}s'.format(DEBUG_SIZE, time.time()-start, (time.time()-start) / DEBUG_SIZE))\n",
    "                print('Average mean loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "q1_gen = {}\n",
    "q2_gen = {}\n",
    "label_gen = {}\n",
    "for i in ['train', 'test']:\n",
    "    q1_gen[i] = q1_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    q2_gen[i] = q2_batcher[i].generator(glove_emb.getIndex('<PAD>'), batch_size=BATCH_SIZE)\n",
    "    label_gen[i] = label_batcher[i].generator(batch_size=BATCH_SIZE)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    \n",
    "    test_q1, test_q1_lengths, _ = next(q1_gen['test'])\n",
    "    test_q2, test_q2_lengths, _ = next(q2_gen['test'])\n",
    "    test_label = next(label_gen['test'])\n",
    "        \n",
    "    feed_dict = {\n",
    "        in_word_indices[0]: test_q1,\n",
    "        in_word_indices[1]: test_q2,\n",
    "        in_lengths[0]: test_q1_lengths,\n",
    "        in_lengths[1]: test_q2_lengths,\n",
    "        in_truth: test_label\n",
    "    }\n",
    "    \n",
    "    e0_val, e1_val = sess.run([e0, e1], feed_dict=feed_dict)\n",
    "    print(e0_val)\n",
    "    print(e1_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug embedding\n",
    "WORD_TO_TEST = 'universe'\n",
    "print(glove[WORD_TO_TEST])\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_emb, feed_dict={in_emb: glove_emb.getNumpyArray()})\n",
    "    debug_val = sess.run(debug_emb, feed_dict={in_word_indices[0]: [[glove_emb.getIndex(WORD_TO_TEST)]], in_lengths[0]: [1], in_truth: [[1.0]] })\n",
    "    print(debug_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
