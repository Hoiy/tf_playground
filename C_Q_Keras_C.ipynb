{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Utils.FS import file\n",
    "from Utils.tensorflow_helper import show_graph\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import TextPreprocess.words2dict as words2dict\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.client import timeline\n",
    "import time\n",
    "from DataLoader import GloVe\n",
    "from TextPreprocess.sequences import Sequences\n",
    "from TextPreprocess.Tokenizer.RegExp import tokenize\n",
    "import Utils.pandas_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "WORD_DIM = 300\n",
    "WORD_COUNT = 400000+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Loading Glove Model\n",
      "End: Loaded 400000 rows.\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe.load2('./data/GloVe/glove.6B.{}d.txt'.format(WORD_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: 300\n",
      "Embedding Symbols: 400003\n",
      "Index to symbol: [(0, '244.5'), (1, 'lahn-dill-kreis'), (2, 'malie'), (3, 'bergler'), (4, 'apsis'), (5, 'in√©s'), (6, 'tobin'), (7, 'sultriness'), (8, 'crissy'), (9, 'marzouk')]\n"
     ]
    }
   ],
   "source": [
    "# emb: Symbol to float32 of fixed DIMENSION\n",
    "# Create an index mapping, index to symbol, symbol to index\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, emb, verbose = False):\n",
    "        # assert emb is dictionary and each entry has same dimension\n",
    "        self.emb = emb\n",
    "        self.dim = len(self.emb[list(self.emb.keys())[0]])\n",
    "        self.emb['<UNK>'] = [0. for i in range(self.dim)]\n",
    "        self.emb['<PAD>'] = [1. for i in range(self.dim)]\n",
    "        self.emb['<GO>'] = [-1. for i in range(self.dim)]\n",
    "        \n",
    "        self.build_dicts()\n",
    "        \n",
    "        if verbose:\n",
    "            self.describe()\n",
    "        \n",
    "    def describe(self):\n",
    "        print('Embedding Dimension: {}'.format(self.dim))\n",
    "        print('Embedding Symbols: {}'.format(len(self.emb)))\n",
    "        print('Index to symbol: {}'.format([(i, self.idx2Sym[i]) for i in range(10)]))\n",
    "        \n",
    "    def getIndex(self, symbol):\n",
    "        if symbol in self.sym2Idx:\n",
    "            return self.sym2Idx[symbol]\n",
    "        else:\n",
    "            return self.sym2Idx['<UNK>']\n",
    "\n",
    "    def getEmb(self, symbol):\n",
    "        return self.emb[self.idx2Sym[self.getIndex(symbol)]]\n",
    "    \n",
    "    def getSymbols(self, indices):\n",
    "        return [self.idx2Sym[idx] for idx in indices]\n",
    "\n",
    "    def getNumpyArray(self):\n",
    "        return np.array([self.emb[self.idx2Sym[idx]] for idx in range(len(self.emb))])\n",
    "    \n",
    "    def build_dicts(self):\n",
    "        self.sym2Idx = {}\n",
    "        index = 0\n",
    "        for key in self.emb.keys():\n",
    "            self.sym2Idx[key] = index\n",
    "            index += 1\n",
    "            \n",
    "        self.idx2Sym = { v:k for k, v in self.sym2Idx.items()}\n",
    "\n",
    "glove_emb = Embedding(glove, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = file.read('data/Quora/train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.question1 = df.question1.astype(str)\n",
    "df.question2 = df.question2.astype(str)\n",
    "df.is_duplicate = df.is_duplicate.astype(float)\n",
    "\n",
    "df = df.as_matrix(['question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "data = {}\n",
    "data['train'], data['test'] = train_test_split(df, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessQuestion(string):\n",
    "    try:\n",
    "        return [glove_emb.getIndex(token.lower()) for token in tokenize(string)]\n",
    "    except:\n",
    "        print(string)\n",
    "\n",
    "\n",
    "def preprocessData(data):\n",
    "    return [[preprocessQuestion(rec[0]), preprocessQuestion(rec[1]), float(rec[2])] for rec in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ['train', 'test']:\n",
    "    data[i] = preprocessData(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Turns iteratable of symbols into padded batch\n",
    "from functools import lru_cache\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.verbose = verbose\n",
    "        self.size = len(self.seqs)\n",
    "        self.seq_lens = [len(seq) for seq in self.seqs]\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.describe()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def max_length(self):\n",
    "        return max(self.seq_lens)\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def longgest_sequence(self):\n",
    "        for seq in self.seqs:\n",
    "            if len(seq) == self.max_length():\n",
    "                return seq\n",
    "    \n",
    "    def describe(self):\n",
    "        print('Size: {}'.format(self.size))\n",
    "        print(\"Longest sequence length: {}\".format(self.max_length()))\n",
    "        bin_width = max(1, self.max_length() // 30)\n",
    "        plt.hist(self.seq_lens, range(0, self.max_length() + bin_width, bin_width))\n",
    "        plt.title('Sequence length distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def batchPadding(self, batch, padding_symbol):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), padding_symbol)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = batch[i]\n",
    "        return result\n",
    "\n",
    "    def batchMask(self, batch):\n",
    "        size = max([len(record) for record in batch])\n",
    "        result = np.full((len(batch), size), 0.0)\n",
    "        for i in range(len(batch)):\n",
    "            result[i][:len(batch[i])] = 1.0\n",
    "        return result\n",
    "        \n",
    "    # Same length within the batch, stuffed with padding symbol\n",
    "    def generator(self, padding_symbol, batch_size=None, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        length = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for seq in self.seqs:\n",
    "                train.append([sym for sym in seq])\n",
    "                length.append(len(seq))\n",
    "                if(len(train) == batch_size):\n",
    "                    yield self.batchPadding(train, padding_symbol), length, self.batchMask(train)\n",
    "                    train = []\n",
    "                    length = []\n",
    "            epouch -= 1\n",
    "            if self.verbose:\n",
    "                print('epouch done...')\n",
    "                \n",
    "class Batcher2:\n",
    "    def __init__(self, sequences, verbose = False):\n",
    "        self.seqs = sequences\n",
    "        self.size = len(self.seqs)\n",
    "\n",
    "    def generator(self, batch_size=32, epouch=-1):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.size\n",
    "        train = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for sym in self.seqs:\n",
    "                train.append([sym])\n",
    "                if(len(train) == batch_size):\n",
    "                    yield train\n",
    "                    train = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')\n",
    "            \n",
    "            \n",
    "# Turn data into batch, where data is iterable over records, record is iterable over fields\n",
    "class Batcher3:\n",
    "    def __init__(self, data):\n",
    "        #assert it is doubly iterable\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        \n",
    "    def generator(self, batch_size = 32, epouch = -1):\n",
    "        batch = []\n",
    "        while(epouch < 0 or epouch > 0):\n",
    "            for record in self.data:\n",
    "                batch.append(record)\n",
    "                if(len(batch) == batch_size):\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            epouch -= 1\n",
    "            print('epouch done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batcher = {}\n",
    "for i in ['train', 'test']:\n",
    "    batcher[i] = Batcher3(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LV1_DIM = 10\n",
    "LV2_STEP = 1\n",
    "LV2_DIM = 150\n",
    "\n",
    "DROPOUT_RNN = 0.1\n",
    "DROP_DENSE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embeddings_initializer(shape):\n",
    "    with tf.variable_scope(\"Embeddings_Initializer\"):\n",
    "        in_emb = tf.placeholder(\n",
    "            dtype = tf.float32, \n",
    "            shape = shape, \n",
    "            name = \"Placeholder\"\n",
    "        )\n",
    "        \n",
    "        emb = tf.Variable(\n",
    "            tf.constant(0.0, shape = shape), \n",
    "            trainable=False, \n",
    "            name = 'Embeddings', \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        init_emb = emb.assign(in_emb)\n",
    "    return in_emb, init_emb, emb\n",
    "\n",
    "def cells_initializer(num_units, reuse):\n",
    "    with tf.variable_scope(\"Cells_Initializer\"):\n",
    "        cells = tf.contrib.rnn.GRUCell(\n",
    "            num_units = num_units,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "    return cells\n",
    "\n",
    "\n",
    "#IN (batch, time, 1)\n",
    "def simple_embedding(inputs, embeddings):\n",
    "    with tf.variable_scope(\"Simple_Embedding\"):\n",
    "        lookup = tf.nn.embedding_lookup(\n",
    "            params = embeddings,\n",
    "            ids = inputs,\n",
    "            partition_strategy='mod',\n",
    "            name='Embedding_Lookup',\n",
    "            validate_indices=True,\n",
    "            max_norm=None\n",
    "        )\n",
    "\n",
    "    return lookup\n",
    "\n",
    "#OUT: (batch, time, dim) float32\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_dynamic_rnn(cell, inputs, lengths):\n",
    "    with tf.variable_scope(\"Simple_Dynamic_RNN\"):        \n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        step_size = tf.shape(inputs)[1]\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(\n",
    "            cell, \n",
    "            inputs, \n",
    "            dtype = tf.float32, \n",
    "            sequence_length = lengths,\n",
    "            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        indices = tf.range(0, batch_size) * step_size + (lengths - 1)\n",
    "        gather = tf.reshape(\n",
    "            tf.gather(\n",
    "                tf.reshape(outputs, [-1, cell.output_size]), indices\n",
    "            ), \n",
    "            [-1, cell.output_size]\n",
    "         )\n",
    "        \n",
    "    return gather\n",
    "#OUT (batch, dim)\n",
    "\n",
    "#IN (batch, time, dim)\n",
    "def simple_encoder(inputs, input_lengths, embeddings, dropout = 0.0, reuse = None):\n",
    "    with tf.variable_scope('Simple_Encoder'):\n",
    "        \n",
    "        emb = simple_embedding(inputs, embeddings)\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size = None,\n",
    "            activation = tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell,\n",
    "            input_keep_prob = 1. - dropout,\n",
    "            output_keep_prob = 1. - dropout,\n",
    "            state_keep_prob = 1. - dropout,\n",
    "            variational_recurrent=False,\n",
    "            input_size=None,\n",
    "            dtype=None,\n",
    "            seed=None\n",
    "        )\n",
    "\n",
    "        rnn = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = emb,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn, emb\n",
    "            \n",
    "        #\n",
    "        # Conv layer does not support dynamic length ;/\n",
    "        #\n",
    "    \"\"\"\n",
    "        filter_2 = tf.Variable(\n",
    "            tf.random_uniform([2, WORD_DIM, LV1_DIM], -1, 1), \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        #IN (batch, time, dim)\n",
    "        conv_2 = tf.nn.conv1d(\n",
    "            value = inputs,\n",
    "            filters = filter_2,\n",
    "            stride = 1,\n",
    "            padding = 'VALID',\n",
    "            use_cudnn_on_gpu=True,\n",
    "            data_format=None,\n",
    "            name='Conv_Witdh_2'\n",
    "        )\n",
    "        #OUT (batch, time-1, dim)\n",
    "\n",
    "    with tf.variable_scope('Level_2_RNN'):\n",
    "        \n",
    "        cell = tf.contrib.rnn.GRUCell(\n",
    "            num_units = LV2_DIM,\n",
    "            input_size=None,\n",
    "            activation=tf.tanh,\n",
    "            reuse = reuse\n",
    "        )\n",
    "        \n",
    "        rnn_output_2 = simple_dynamic_rnn(\n",
    "            cell = cell,\n",
    "            inputs = inputs,\n",
    "            lengths = input_lengths\n",
    "        )\n",
    "        \n",
    "    return rnn_output_2\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#OUT (batch, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Concatenate, Reshape, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "num_dense = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "600\n",
      "1\n",
      "x1:  Tensor(\"flatten_4/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "conv_max_length = 2\n",
    "conv_dim = 300\n",
    "rnn_step = 1\n",
    "\n",
    "print(conv_max_length)\n",
    "print(conv_dim)\n",
    "print(rnn_step)\n",
    "\n",
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(WORD_COUNT,\n",
    "        WORD_DIM,\n",
    "        weights=[glove_emb.getNumpyArray()],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(200, dropout=0.3, recurrent_dropout=0.3)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "\n",
    "# create same conv for both\n",
    "conv = [[Conv1D(conv_dim, j+2, padding='same') for j in range(conv_max_length)] for i in range(rnn_step)]\n",
    "\n",
    "# create conv1d for each step and each length\n",
    "feat_1 = [[conv[i][j](embedded_sequences_1) for j in range(conv_max_length)] for i in range(rnn_step)]\n",
    "#feat_1 = MaxPooling1D(pool_size=2, strides=2, padding='same')(feat_1[0][0])\n",
    "#feat_1 = [Reshape((1, conv_dim * MAX_SEQUENCE_LENGTH * conv_max_length))(Concatenate()(feat_1[i])) for i in range(rnn_step)]\n",
    "feat_1 = Concatenate(1)(feat_1[0])\n",
    "\n",
    "#x1 = lstm_layer(feat_1)\n",
    "x1 = Flatten()(feat_1)\n",
    "\n",
    "print('x1: ', x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "\n",
    "# create conv1d for each step and each length\n",
    "feat_2 = [[conv[i][j](embedded_sequences_2) for j in range(conv_max_length)] for i in range(rnn_step)]\n",
    "#feat_2 = MaxPooling1D(pool_size=2, strides=2, padding='same')(feat_2[0][0])\n",
    "#feat_2 = [Reshape((1, MAX_SEQUENCE_LENGTH * conv_dim * conv_max_length))(Concatenate()(feat_2[i])) for i in range(rnn_step)]\n",
    "feat_2 = Concatenate(1)(feat_2[0])\n",
    "\n",
    "#y1 = lstm_layer(feat_2)\n",
    "y1 = Flatten()(feat_2)\n",
    "\n",
    "merged = Concatenate()([x1, y1])\n",
    "merged = Dropout(0.05)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = Dropout(0.05)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(WORD_COUNT,\n",
    "        WORD_DIM,\n",
    "        weights=[glove_emb.getNumpyArray()],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(150, dropout=0.3, recurrent_dropout=0.3)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[sequence_1_input, sequence_2_input],\n",
    "    outputs=preds\n",
    ")\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "q1 = {}\n",
    "q2 = {}\n",
    "label = {}\n",
    "\n",
    "for i in ['train', 'test']:\n",
    "    q1[i] = [rec[0] for rec in data[i]]\n",
    "    q1[i] += [rec[1] for rec in data[i]]\n",
    "    \n",
    "    q1[i] = pad_sequences(q1[i], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    q2[i] = [rec[1] for rec in data[i]]\n",
    "    q2[i] += [rec[0] for rec in data[i]]\n",
    "    \n",
    "    q2[i] = pad_sequences(q2[i], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    label[i] = [rec[2] for rec in data[i]]\n",
    "    label[i] += [rec[2] for rec in data[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727722\n",
      "727722\n",
      "727722\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[ 1.          1.          1.          1.30902834  1.          1.30902834\n",
      "  1.30902834  1.30902834  1.30902834  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(len(q1['train']))\n",
    "print(len(q2['train']))\n",
    "print(len(label['train']))\n",
    "\n",
    "weight_val = np.ones(len(label['test']))\n",
    "#weight_val *= 0.472001959\n",
    "for i in range(len(label['test'])):\n",
    "    if label['test'][i] == 0:\n",
    "        weight_val[i] = 1.309028344\n",
    "\n",
    "class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "\n",
    "print(label['test'][:10])\n",
    "print(weight_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3696136711546442\n"
     ]
    }
   ],
   "source": [
    "positive_ratio = sum(label['train']) / len(label['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/200\n",
      "  5120/727722 [..............................] - ETA: 2022s - loss: 0.6796 - acc: 0.5975\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-12f0b5d4c1de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoiy927/project/tf_playground/tf_playground/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "hist = model.fit(\n",
    "    [q1['train'], q2['train']], \n",
    "    label['train'],\n",
    "    validation_data=([q1['test'], q2['test']], label['test']),\n",
    "    epochs=200, \n",
    "    batch_size=256, \n",
    "    shuffle=True,\n",
    "    class_weight = {0: 0.5 / (1. - positive_ratio), 1: 0.5 / positive_ratio },\n",
    "    callbacks=[tbCallBack]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
